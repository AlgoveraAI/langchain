{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain.document_loaders.ipfs_file import IPFSFileDataLoader\n",
    "from langchain.document_loaders.ipfs_directory import IPFSDirectoryDataLoader\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"/Users/arshath/play/ipfs/.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local IPFS Node - Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = IPFSFileDataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_ipfs_hash_csv = \"QmSbhdHQ5DN7tYzHTXetUaAQfEp1vqWJzTnxLJAkYFHEdZ\"\n",
    "local_ipfs_hash_json = \"QmaVLPFfNFAxuyn2vmhaTdZ238F7TELKKGNE7hTMYyJadQ\"\n",
    "local_ipfs_hash_html = \"QmQV4k5TK66FZtys8D7T71AdkQt5gh4eqkm29647wyGcnk\"\n",
    "local_ipfs_hash_py = \"QmaBeFHdeL9SLcvPqD8sxgh5VyW9CReo6t4WKpnvDQWHVu\"\n",
    "local_ipfs_hash_pdf = \"QmXbFrfYoUeQp3ho1MEKjtLiC3GGDGe2P9JLPfjngGqyAb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Testing CSV'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='gem_id: viggo-train-0\\nmeaning_representation: inform(name[Dirt: Showdown], release_year[2012], esrb[E 10+ (for Everyone 10 and Older)], genres[driving/racing, sport], platforms[PlayStation, Xbox, PC], available_on_steam[no], has_linux_release[no], has_mac_release[no])\\ntarget: Dirt: Showdown from 2012 is a sport racing game for the PlayStation, Xbox, PC rated E 10+ (for Everyone 10 and Older). It\\'s not available on Steam, Linux, or Mac.\\nreferences: [\"Dirt: Showdown from 2012 is a sport racing game for the PlayStation, Xbox, PC rated E 10+ (for Everyone 10 and Older). It\\'s not available on Steam, Linux, or Mac.\"]\\nsplit: train\\n', metadata={'index': 0, 'file_type': 'text/csv', 'ipfs_hash': 'QmSbhdHQ5DN7tYzHTXetUaAQfEp1vqWJzTnxLJAkYFHEdZ'}),\n",
       " Document(page_content=\"gem_id: viggo-train-1\\nmeaning_representation: inform(name[Dirt: Showdown], release_year[2012], esrb[E 10+ (for Everyone 10 and Older)], genres[driving/racing, sport], platforms[PlayStation, Xbox, PC], available_on_steam[no], has_linux_release[no], has_mac_release[no])\\ntarget: Dirt: Showdown is a sport racing game that was released in 2012. The game is available on PlayStation, Xbox, and PC, and it has an ESRB Rating of E 10+ (for Everyone 10 and Older). However, it is not yet available as a Steam, Linux, or Mac release.\\nreferences: ['Dirt: Showdown is a sport racing game that was released in 2012. The game is available on PlayStation, Xbox, and PC, and it has an ESRB Rating of E 10+ (for Everyone 10 and Older). However, it is not yet available as a Steam, Linux, or Mac release.']\\nsplit: train\\n\", metadata={'index': 1, 'file_type': 'text/csv', 'ipfs_hash': 'QmSbhdHQ5DN7tYzHTXetUaAQfEp1vqWJzTnxLJAkYFHEdZ'}),\n",
       " Document(page_content=\"gem_id: viggo-train-2\\nmeaning_representation: inform(name[Dirt: Showdown], release_year[2012], esrb[E 10+ (for Everyone 10 and Older)], genres[driving/racing, sport], platforms[PlayStation, Xbox, PC], available_on_steam[no], has_linux_release[no], has_mac_release[no])\\ntarget: Dirt: Showdown is a driving/racing sport game released in 2012. It is rated E 10+, and is available on PlayStation, Xbox and PC, but not on Steam, Mac, or Linux.\\nreferences: ['Dirt: Showdown is a driving/racing sport game released in 2012. It is rated E 10+, and is available on PlayStation, Xbox and PC, but not on Steam, Mac, or Linux.']\\nsplit: train\\n\", metadata={'index': 2, 'file_type': 'text/csv', 'ipfs_hash': 'QmSbhdHQ5DN7tYzHTXetUaAQfEp1vqWJzTnxLJAkYFHEdZ'}),\n",
       " Document(page_content=\"gem_id: viggo-train-3\\nmeaning_representation: request(release_year[2014], specifier[terrible])\\ntarget: Were there even any terrible games in 2014?\\nreferences: ['Were there even any terrible games in 2014?']\\nsplit: train\\n\", metadata={'index': 3, 'file_type': 'text/csv', 'ipfs_hash': 'QmSbhdHQ5DN7tYzHTXetUaAQfEp1vqWJzTnxLJAkYFHEdZ'}),\n",
       " Document(page_content='gem_id: viggo-train-4\\nmeaning_representation: request(release_year[2014], specifier[terrible])\\ntarget: What\\'s the most terrible game that you played in the year 2014?\\nreferences: [\"What\\'s the most terrible game that you played in the year 2014?\"]\\nsplit: train\\n', metadata={'index': 4, 'file_type': 'text/csv', 'ipfs_hash': 'QmSbhdHQ5DN7tYzHTXetUaAQfEp1vqWJzTnxLJAkYFHEdZ'}),\n",
       " Document(page_content='gem_id: viggo-train-5\\nmeaning_representation: request(release_year[2014], specifier[terrible])\\ntarget: What\\'s a truly terrible game released in 2014 that you know of?\\nreferences: [\"What\\'s a truly terrible game released in 2014 that you know of?\"]\\nsplit: train\\n', metadata={'index': 5, 'file_type': 'text/csv', 'ipfs_hash': 'QmSbhdHQ5DN7tYzHTXetUaAQfEp1vqWJzTnxLJAkYFHEdZ'}),\n",
       " Document(page_content='gem_id: viggo-train-6\\nmeaning_representation: give_opinion(name[Little Nightmares], rating[good], genres[adventure, platformer, puzzle], player_perspective[side view])\\ntarget: Adventure games that combine platforming and puzzles can be frustrating to play, but the side view perspective is perfect for them. That\\'s why I enjoyed playing Little Nightmares.\\nreferences: [\"Adventure games that combine platforming and puzzles can be frustrating to play, but the side view perspective is perfect for them. That\\'s why I enjoyed playing Little Nightmares.\"]\\nsplit: train\\n', metadata={'index': 6, 'file_type': 'text/csv', 'ipfs_hash': 'QmSbhdHQ5DN7tYzHTXetUaAQfEp1vqWJzTnxLJAkYFHEdZ'}),\n",
       " Document(page_content='gem_id: viggo-train-7\\nmeaning_representation: give_opinion(name[Little Nightmares], rating[good], genres[adventure, platformer, puzzle], player_perspective[side view])\\ntarget: Little Nightmares is a pretty cool game that has kept me entertained. It\\'s an adventure side-scrolling platformer with some puzzle elements to give me a bit of a challenge.\\nreferences: [\"Little Nightmares is a pretty cool game that has kept me entertained. It\\'s an adventure side-scrolling platformer with some puzzle elements to give me a bit of a challenge.\"]\\nsplit: train\\n', metadata={'index': 7, 'file_type': 'text/csv', 'ipfs_hash': 'QmSbhdHQ5DN7tYzHTXetUaAQfEp1vqWJzTnxLJAkYFHEdZ'}),\n",
       " Document(page_content=\"gem_id: viggo-train-8\\nmeaning_representation: give_opinion(name[Little Nightmares], rating[good], genres[adventure, platformer, puzzle], player_perspective[side view])\\ntarget: Little Nightmares is a good little adventure platforming puzzle game, I particularly enjoy what they did with the side view too.\\nreferences: ['Little Nightmares is a good little adventure platforming puzzle game, I particularly enjoy what they did with the side view too.']\\nsplit: train\\n\", metadata={'index': 8, 'file_type': 'text/csv', 'ipfs_hash': 'QmSbhdHQ5DN7tYzHTXetUaAQfEp1vqWJzTnxLJAkYFHEdZ'}),\n",
       " Document(page_content='gem_id: viggo-train-9\\nmeaning_representation: inform(name[Super Bomberman], release_year[1993], genres[action, strategy], platforms[Nintendo, PC], available_on_steam[no], has_linux_release[no], has_mac_release[no])\\ntarget: Super Bomberman is an action and strategy game from 1993 for Nintendo consoles and PC only. It\\'s an older console game that is, unfortunately, not available to play on Steam and does not offer support for Mac or Linux.\\nreferences: [\"Super Bomberman is an action and strategy game from 1993 for Nintendo consoles and PC only. It\\'s an older console game that is, unfortunately, not available to play on Steam and does not offer support for Mac or Linux.\"]\\nsplit: train\\n', metadata={'index': 9, 'file_type': 'text/csv', 'ipfs_hash': 'QmSbhdHQ5DN7tYzHTXetUaAQfEp1vqWJzTnxLJAkYFHEdZ'})]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test CSV\n",
    "display(\"Testing CSV\")\n",
    "res = reader.load(local_ipfs_hash_csv)\n",
    "display(res[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Testing JSON'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Document(page_content='{\"dataset\": \"dataset.csv\", \"training_prompt\": \"Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values.\\\\nThis function should describe the target string accurately and the function must be one of the following [\\'inform\\', \\'request\\', \\'give_opinion\\', \\'confirm\\', \\'verify_attribute\\', \\'suggest\\', \\'request_explanation\\', \\'recommend\\', \\'request_attribute\\'].\\\\nThe attributes must be one of the following: [\\'name\\', \\'exp_release_date\\', \\'release_year\\', \\'developer\\', \\'esrb\\', \\'rating\\', \\'genres\\', \\'player_perspective\\', \\'has_multiplayer\\', \\'platforms\\', \\'available_on_steam\\', \\'has_linux_release\\', \\'has_mac_release\\', \\'specifier\\']\\\\n\\\\n### Target sentence:\\\\n{target}\\\\n\\\\n### Meaning representation:\\\\n{meaning_representation}\\\\n\", \"eval_prompt\": \"Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values.\\\\nThis function should describe the target string accurately and the function must be one of the following [\\'inform\\', \\'request\\', \\'give_opinion\\', \\'confirm\\', \\'verify_attribute\\', \\'suggest\\', \\'request_explanation\\', \\'recommend\\', \\'request_attribute\\'].\\\\nThe attributes must be one of the following: [\\'name\\', \\'exp_release_date\\', \\'release_year\\', \\'developer\\', \\'esrb\\', \\'rating\\', \\'genres\\', \\'player_perspective\\', \\'has_multiplayer\\', \\'platforms\\', \\'available_on_steam\\', \\'has_linux_release\\', \\'has_mac_release\\', \\'specifier\\']\\\\n\\\\n### Target sentence:\\\\nEarlier, you stated that you didn\\'t have strong feelings about PlayStation\\'s Little Big Adventure. Is your opinion true for all games which don\\'t have multiplayer?\\\\n\\\\n### Meaning representation:\\\\n\", \"columns\": [\"target\", \"meaning_representation\"], \"train_split\": \"train\", \"valid_split\": \"valid\"}', metadata={'ipfs_hash': 'QmaVLPFfNFAxuyn2vmhaTdZ238F7TELKKGNE7hTMYyJadQ', 'file_type': 'application/json'})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test JSON\n",
    "display(\"Testing JSON\")\n",
    "res = reader.load(local_ipfs_hash_json)\n",
    "display(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Testing HTML'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Document(page_content='<!-- generate a simple html -->\\n\\n<html>\\n<head>\\n<title>My first PHP page</title>\\n</head>\\n<body>\\n    <h1>My first PHP page</h1>\\n    <p>\\n        <?php\\n            echo \"Hello World!\";\\n        ?>\\n    </p>\\n</body>\\n</html>', metadata={'ipfs_hash': 'QmQV4k5TK66FZtys8D7T71AdkQt5gh4eqkm29647wyGcnk', 'file_type': 'text/html'})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test HTML\n",
    "display(\"Testing HTML\")\n",
    "res = reader.load(local_ipfs_hash_html)\n",
    "display(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Testing Python'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Document(page_content='import json\\nimport time\\nimport magic\\nimport logging\\nimport requests\\nimport tempfile\\nimport pandas as pd\\nfrom PIL import Image\\nimport PyPDF2 as pypdf2\\n\\ndef get_logger():\\n    logger = logging.getLogger(__name__)\\n    logger.setLevel(logging.DEBUG)\\n    handler = logging.StreamHandler()\\n    handler.setLevel(logging.DEBUG)\\n    logger.addHandler(handler)\\n    return logger\\n\\nlogger = get_logger()\\n\\n\\ndef identify_file_type_from_bytes(byte_data):\\n    mime = magic.Magic(mime=True)\\n    file_type = mime.from_buffer(byte_data)\\n    return file_type\\n\\ndef pdf_reader(response):\\n    with tempfile.TemporaryFile() as temp:\\n        temp.write(response.content)\\n        pdf = pypdf2.PdfFileReader(temp)\\n        page = pdf.getPage(0)\\n        text = page.extractText()\\n        return text\\n    \\ndef txt_reader(response):\\n    with tempfile.TemporaryFile() as temp:\\n        temp.write(response.content)\\n        temp.seek(0)\\n        text = temp.read().decode(\"utf-8\")\\n        return text\\n\\ndef csv_reader(response):\\n    with tempfile.TemporaryFile() as temp:\\n        temp.write(response.content)\\n        temp.seek(0)\\n        df = pd.read_csv(temp)\\n        return df\\n    \\ndef json_reader(response):\\n    with tempfile.TemporaryFile() as temp:\\n        temp.write(response.content)\\n        temp.seek(0)\\n        data = json.load(temp)\\n        return data\\n    \\ndef image_reader(response):\\n    with tempfile.TemporaryFile() as temp:\\n        temp.write(response.content)\\n        temp.seek(0)\\n        img = Image.open(temp)\\n        return img\\n    \\ndef script_reader(response):\\n    with tempfile.TemporaryFile() as temp:\\n        temp.write(response.content)\\n        temp.seek(0)\\n        script = temp.read().decode(\"utf-8\")\\n        return script\\n\\ndef select_reader(file_type):\\n    reader_map = {\\n        \"text/csv\": csv_reader,\\n        \"application/json\": json_reader,\\n        \"image/png\": image_reader,\\n        \"text/html\": txt_reader,\\n        \"text/x-shellscript\": script_reader,\\n        \"text/x-python\": script_reader,\\n        \"application/pdf\": pdf_reader,\\n        \"text/plain\": txt_reader\\n    }\\n    return reader_map[file_type]\\n\\n\\nclass IPFSFileloader:\\n    base_url = \"http://127.0.0.1:5001\"\\n    base_api_path = \"api/v0\"\\n    read_suffix = \"cat\"\\n    version_suffix = \"version\"\\n    max_retries = 5\\n    retry_delay = 1\\n\\n    def __init__(self, debug=False):\\n        self.debug = debug\\n        self.check_daemon_running()\\n\\n    def check_daemon_running(self):\\n        full_url = \"/\".join([self.base_url, self.base_api_path, self.version_suffix])\\n        if self.debug:\\n            logger.debug(f\"Checking if IPFS daemon is running at {full_url}\")\\n\\n        for i in range(self.max_retries):\\n            try:\\n                response = requests.post(full_url)\\n                if self.debug:\\n                    logger.debug(f\"IPFS daemon is running at {full_url}\")\\n                return response.status_code\\n            except requests.exceptions.ConnectionError:\\n                time.sleep(self.retry_delay)\\n\\n        raise Exception(\"IPFS daemon not running\")\\n    \\n    def load(self, ipfs_hash):\\n        full_url = \"/\".join([self.base_url, self.base_api_path, self.read_suffix])\\n        full_url += f\"?arg={ipfs_hash}\"\\n\\n        for i in range(self.max_retries):\\n            try:\\n                response = requests.post(full_url)\\n                file_type = identify_file_type_from_bytes(response.content)\\n                reader = select_reader(file_type)\\n                return reader(response)\\n            except requests.exceptions.ConnectionError:\\n                time.sleep(self.retry_delay)\\n\\n        raise Exception(\"IPFS daemon not running\")\\n', metadata={'ipfs_hash': 'QmaBeFHdeL9SLcvPqD8sxgh5VyW9CReo6t4WKpnvDQWHVu', 'file_type': 'text/x-script.python'})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test Python\n",
    "display(\"Testing Python\")\n",
    "res = reader.load(local_ipfs_hash_py)\n",
    "display(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Testing PDF'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Document(page_content='Journal of Restorative Medicine 2017; 6: page 19    Neurological Activity of Lion’s Mane (Hericium erinaceus)\\nKevin Spelman, PhD, MCPPa\\nElizabeth Sutherland, NDb\\nAravind Bagade, MDc©2017, Kevin Spelman, PhD, MCPP\\nJournal Compilation ©2017, AARM\\nDOI 10.14200/jrm.2017.6.0108\\nABSTRACT\\nHericium erinaceus, most commonly known as lion’s mane, is an edible fungus, with \\na long history of use in Traditional Chinese Medicine. The mushroom is abundant in bioactive compounds including β-glucan polysaccharides; hericenones and erinacine terpenoids; isoindolinones; sterols; and myconutrients, which potentially have neuroprotective and neuroregenerative properties. Because of its anti-inflammatory properties and promotion of nerve growth factor gene expression and neurite (axon or dendrite) outgrowth, H. erinaceus mycelium shows great promise for the treatment of Alzheimer’s and Parkinson’s diseases. The fungus was well tolerated in two clinical studies, with few adverse events reported.\\nKeywords: Lion’s mane; Neuroregeneration; Neurodegeneration; Neuroprotection; \\nNeurotropins; Neurotrophic; Alzheimer’s disease; Parkinson’s disease; Multiple Sclerosis; Nerve growth factor\\naCorresponding author: Health, Education & Research, POB 599, Ashland, OR 97520, USA,  \\nTel.: +1-541-708-3002; E-mail: phytochemks@gmail.com\\nbAdjunct faculty National University of Natural Medicine, Portland, OR, USA\\ncExecutive Secretary and Researcher, Ayurveda Interdisciplinary Research Minds Association, Mysore, India\\n\\nJournal of Restorative Medicine 2017; 6: page 20\\nLion’s Mane Neurological ActivityINTRODUCTION\\nAncient, traditional, and modern cultures around \\nthe world have known about the nutritive and medicinal properties of mushrooms for centu-ries. As early as 450 BCE, the Greek physician Hippocrates identified mushrooms as potent anti-inflammatory agents, useful for cauterizing wounds. In the East, reverence for fungi is evident in the Chinese description of ling zhi (Ganoderma lucidum), as the “spirit plant,” believed to provide longevity and spiritual potency.\\nModern medicine has been slower to catch on to \\nthe immense potential of fungi. Despite Fleming’s 1929 discovery of penicillin,\\n1 and the subsequent \\nimplementation of the fungi-chemical as a block-buster pharmaceutical in the 1940s,\\n2 it is only \\nin the last few decades that medical science has looked beyond the antimicrobial and cholesterol-lowering properties of fungi for other potential applications.\\nClinicians now have greater access to myce-\\nlium extracts, which are used clinically for their cytotoxic, antineoplastic, cardiovascular, anti-inflammatory, and immune-modulating activities.\\n3–5 \\nFunctional studies and chemical assays also support their potential to act as analgesic, antibacterial, antioxidant, and neuroprotective agents. A number of mushrooms, including Sarcodon scabrosus, \\nGanoderma lucidum, Grifola frondosa, and Hericium erinaceus are reported to have activi-ties related to nerve and brain health.\\n6 Hericium \\nerinaceus, a member of the Herinaceae family, is a culinary and medicinal mushroom. Both the myce-lium and fruiting bodies of H. erinaceus have been shown to have therapeutic potential for brain and nerve health.\\n7 The unique neurological activities of \\nthis fungus are the subject of this review.\\nTRADITIONAL USE OF LION’S \\nMANE (H. ERINACEUS)\\nHericium erinaceus (lion’s mane, yamabushi-\\ntake, or bearded tooth carpophore) grows on old or dead broadleaf trees, and is used as both food and medicine in parts of Asia. The fruiting body is called hóu tóu gū (“monkey head mushroom”) in Chinese\\n8 and yamabushitake (“mountain monk \\nmushroom”) in Japanese. In Chinese and Japanese medical systems, it has traditionally been used to fortify the spleen, nourish the gut, and also as an anticancer drug.\\n9 Lion’s mane is said to be nutri-\\ntive to the five internal organs (liver, lung, spleen, heart, and kidney), and promotes good digestion, general vigor, and strength. It is also recommended for gastric and duodenal ulcers, as well as chronic gastritis (in prepared tablet form).\\n10 The mushroom \\nis also known for its effects on the central nervous system, and is used for insomnia, vacuity (weak-ness), and hypodynamia, which are characteristic symptoms of Qi deficiency in Traditional Chinese medicine (TCM).\\nCHEMISTRY\\nThe bioactive metabolites of H. erinaceus can be classified into high molecular weight compounds, such as polysaccharides, and low molecular weight compounds, such as polyketides and terpenoids.\\n10,11\\nPOLYSACCHARIDES\\nFungal polysaccharides are found mainly in cell walls, and are present in large quantities in both fruiting bodies and cultured mycelium. Hericium erinaceus fruiting bodies (HEFB) contain immu-noactive β-glucan polysaccharides, as well as α-glucans and glucan-protein complexes.\\n12 A total \\nof more than 35 H. erinaceus polysaccharides (HEP) have been extracted to date from cultured, wild-growing, or fermentative mycelia and fresh/dried fruiting bodies. Of these β-glucans represent the main polysaccharides. HEP are composed of xylose (7.8%), ribose (2.7%), glucose (68.4%), arabinose (11.3%), galactose (2.5%), and mannose (5.2%).\\n4 Four different polysaccharides isolated \\nfrom the H. erinaceus sporocarp show antitumor activity: xylans, glucoxylans, heteroxyloglucans, and galactoxyloglucans.\\n5 Chemical analysis shows \\nthat the total content of HEP found in fruiting bod-ies is higher than that in mycelium. Table 1 lists the \\n\\nJournal of Restorative Medicine 2017; 6: page 21    \\nLion’s Mane Neurological Activitypolysaccharides along with their source and chemi-\\ncal composition.\\nStudies of the polysaccharides found in  \\nH. erinaceus reveal a number of activities. For \\nexample, extracellular and intracellular polysac-charides showed a protective effect on oxidative hepatotoxicity in mice.\\n11 Neuroprotective effects of \\nHEPs were observed in an in vitro model of cells that were toxic from amyloid β  plaque formation. \\nIn this model, HEPs decreased the production of reactive oxygen species from 80% to 58% in a dose-dependent manner, and increased the efficacy of free radical scavenging. HEPs also promoted cell viabil-ity and protected cells against apoptosis induced by amyloid β  plaque formation.\\n13 HEPs decreased \\nblood lactic acid, serum urea nitrogen, tissue glyco-gen, and malondialdehyde, further supporting the beneficial role of HEPs on oxidative stress.\\n14\\nTERPENOIDS: SESTERPENES, AND \\nDITERPENOIDS\\nTerpenoids are a class of naturally occurring \\nhydrocarbons that consist of terpenes attached to an oxygen containing group. Terpenoids make up over 60% of products in the natural world.\\n15,16\\nA variety of diterpenes and sesterpenes are found in the fruiting body and fermenting mycelium of H. erinaceus.\\n17 Of particular pharmacological inter -\\nest are two classes of terpenoid compounds thus far known to occur only in Hericium spp.: hericenones \\n(C–H), a group of aromatic compounds isolated from the fruiting body; and erinacines (A–I), a \\ngroup of cyathane-type diterpenoids found in the mycelium.\\n18 Both groups of substances easily cross \\nthe blood-brain barrier, and have been found to have neurotrophic and in some cases neuroprotective effects.\\n19 Erinacines (A–I) have demonstrated induc-\\ntion of nerve growth factor (NGF) synthesis.20 Table \\n2 lists the terpenoids, sesterpenes, and diterpenoids along with their source and chemical composition.\\nSTEROLS\\nTen erinarols, described as erinarol A–J, five ergostane-type sterol fatty acid esters, and ten ergostane-type sterols have been identified in the fruiting body of H. erinaceus.\\n21 Sterols, such as Table 1: Polysaccharides: source and composition.\\nPolysaccharides No. Isolated from Composition\\n(FI0-a, FI0-a-α, FI0-a-β, FI0-b, \\nFII-1, FIII-2b)6 Fresh fruiting bodies of H. erinaceusXylans, glucoxylans, heteroxyloglucans, and galactoxyloglucans\\nAF2S-2, BF2S-2 2 Fresh fruiting bodies Backbone of β-(l→6)-linked D-glucopyranosyl residues, and had β-(1→3) and β-(l→6) glucosidic linkages\\nHeteropolysaccharides (HEPA1, HEPA4, HEPB2)3 Mycelium Glucose\\nWater extractable polysaccharides (HPA and HPB)2 Aqueous extract Glucose and galactose\\nWater soluble polysaccharide (HPI)1 H. caput-medusae Glucose and galactose\\nNeutral heteropolysaccharides (HEP-1 and HEP-4)2 Fruiting bodies Glucose\\nGlucans HEP-3 (β-glucan) and HEP-5 (α glucan)2 Fruiting bodies Glucose\\nAcidic polysaccharide (HEP-2) 1 Fruiting bodies Uronic acid\\nHeteropolysaccharide (HPB-3) 1 The maturating-stage IV , V , and VI fruiting bodyI-fucose, d-galactose and d-glucose\\nHomopolysaccharides, a neutral glucan (HPP)1 Fermentative mycelia Glucose\\n\\nJournal of Restorative Medicine 2017; 6: page 22\\nLion’s Mane Neurological Activityergosterol confer antioxidative properties.21,22  \\nHericium erinaceus has been found to be the most \\npotent in vitro inhibitor of both low-density lipo-protein (LDL) oxidation and HMG Co-A reductase activity, suggesting therapeutic potential for the prevention of oxidative stress-mediated vascular diseases.\\n23\\nNEUROLOGICAL ACTIVITY\\nNEUROPROTECTION\\nHericenones and erinacines isolated from H. erinaceus have demonstrated neuroprotective properties.\\n24 Hericium erinaceus mycelia (HEM), \\nand its isolated diterpenoid derivative, erinacine A, reduced infarction by 22% at 50 mg/kg and 44% at 300 mg/kg in an animal model of global ischemic stroke. This effect was thought to be partially medi-ated by its ability to reduce cytokine levels.\\n25\\nA purified polysaccharide from the liquid culture broth of HEM was also found to possess neuro-protective activity in an in vitro model through a dramatic delay of apoptosis, which was 20%–50% greater than that seen in the control sample. The same study showed HEM to be more effective than control, NGF, or brain-derived neurotrophic fac-tor (BDNF) alone in enhancing the growth of rat adrenal nerve cells and neurite (axon or dendrite) extension.\\n26 However, in a model of NG108-15 \\nneuroblastoma cells subjected to H2O2 oxidative \\nstress in pre-treatment and co-treatment, the aque-ous extract of H. erinaceus (as opposed to a purified polysaccharide), failed to show a protective effect.\\n27 \\nAlthough it is challenging to draw clinically rele-vant conclusions from in vitro studies, this suggests that water extracts would not have a neuroprotec-tive effect without one particular polysaccharide being highly concentrated.\\nNEUROTROPHIC ACTIVITY AND \\nMYELINATION\\nThe addition of an ethanol extract of HEFB resulted \\nin NGF gene expression in human astrocytoma cells, in a concentration-dependent manner. Neurite outgrowth was also improved. The same investi-gators also observed that mice fed 5% HEFB dry powder for 7 days, showed an increase in the level of NGF mRNA expression in the hippocampus.\\n28 \\nAnother study showed that an aqueous extract of HEFB increased secretion of extracellular NGF and neurite outgrowth activity. These\\n researchers \\nalso observed a synergistic interaction between H. erinaceus aqueous extract and exogenous NGF on neurite outgrowth stimulation of neuro-blastoma-glioma cells at physiologically relevant concentrations (1 μg/mL HEFB extract +10 ng/mL NGF).\\n21 Myelin sheath formation in the presence \\nof H. erinaceus extract proceeded at a higher rate and was completed by day 26, as compared to day 31 in controls. No toxic effects of the extracts were observed in this model.\\n30\\nCOGNITIVE FUNCTION\\nIn a behavior test on wild-type mice, oral supplementation with H. erinaceus induced a Table 2: Sesterpenes and diterpenoids: source and composition.\\nTerpenoids Isolated from Composition\\nHericenones\\nErinacinesFresh fruiting bodies of H. erinaceusMyceliaErinacerins C–L together with (E)-5- (3,7- methylocta-2,6-dien-1-yl)-4-hydroxy-6-methoxy-2-phenethylisoindolin-1-one \\nDiterpenoids Fresh fruiting bodies of H. erinaceus Erinacines A–I\\nIsoindolinones Fresh fruiting bodies of H. erinaceus Erinaceolactams A–E, hericenone A, hericenone J, N-De phenylethylisohericerin, erinacerin A, and hericerin\\n\\nJournal of Restorative Medicine 2017; 6: page 23    \\nLion’s Mane Neurological Activitystatistically significant improvement in spatial \\nshort-term and visual recognition memory.31 In a \\ndouble-blind placebo-controlled clinical trial of 50–80-year-old Japanese adults (n =30) diagnosed \\nwith mild cognitive impairment, oral intake of  \\nH. erinaceus 250 mg tablets (96% dry powder) three times a day for 16 weeks was associ-ated with marked improvement in the revised Hasegawa Dementia Scale (HDS-R) as compared to controls. Scores on the HDS-R decreased, however, by 4 weeks after cessation of the intervention.\\n28\\nALZHEIMER’S DISEASE\\nIn a mouse model of Alzheimer’s disease, oral administration of HEFB increased expression of NGF mRNA in the hippocampus, and prevented impairments of spatial, short-term, and visual recognition memory induced by amyloid β plaque that were observed in non-treated mice.\\n28 In another \\nstudy using an Alzheimer’s model of mice that develop amyloid plaque deposits by 6 months of age, a 30-day oral administration of HEM resulted in fewer plaque deposits in microglia and astro-cytes in the cerebral cortex and hippocampus.\\n32 \\nIn an aluminum chloride induced animal model of Alzheimer’s disease, HEM increased serum and hypothalamic concentrations of acetylcholine and choline acetyltransferase in a dose-dependent manner.\\n29 Figure 1 illustrates the apparent mecha-\\nnisms of action for the effects that H. erinaceus may have in Alzheimer’s disease.\\nPARKINSON’S DISEASE\\nOral administration of low-dose HEM (10.76 or 21.52 mg/day) used in an animal model of Parkinson’s disease led to significant improve-ment in oxidative stress and dopaminergic lesions in the striatum and substantia nigra after 25 days.\\n33\\nPERIPHERAL NERVE INJURY\\nAn aqueous extract of HEFB that was admin-istered to animals at a dose of 10 mL/kg for 14 days following crush injury improved nerve regeneration and increased the rate of motor functional recovery. The animals treated with HEFB recovered 4–7 days earlier than animals in the control group, as assessed by walking track analysis. Normal toe spreading, a measure of reinnervation, was achieved 5–10 days earlier in the aqueous extract group than in the control group. Based on functional evaluation and the morphological examination of regenerated nerves, ipsilateral dorsal root ganglia, and target extensor digitorum longus muscles, researchers concluded that HEFB aqueous extract promoted peripheral \\nFigure 1: Mechanism of action of Hericium erinaceus in Alzheimer’s disease.\\n\\nJournal of Restorative Medicine 2017; 6: page 24\\nLion’s Mane Neurological Activitynerve regeneration with significant functional \\nrecovery.33\\nCLINICAL TRIALS\\nAs previously described under Cognitive Function, a double-blind placebo-controlled study of 50–80-year-old Japanese men and women (n=30) diagnosed with mild cognitive impair -\\nment showed marked improvement in cognitive function, as measured by the revised Hasegawa Dementia Scale (HDS-R), when compared to controls, following oral intake of H. erinaceus 250 mg tablets (96% dry powder) three times a day for 16 weeks. Scores on the HDS-R decreased, however, by 4 weeks after cessation of the intervention.\\n28\\nIn another clinical trial, administration of HEFB at 2.0 g/day (in cookies) over 4 weeks showed a reduction in some symptoms of anxiety and depres-sion in menopausal women (n=30). The Indefinite Complaints Index categories for Palpitation and Incentive showed a statistically significant improvement in women taking HEFB compared to those taking placebo. The categories of Irritating, Anxious, and Concentration indicated a trend in the direction of improvement with HEFB as compared to placebo.\\n29 Table 3 summarizes the two clinical \\ntrials reported on in this paper.POSOLOGY\\nCOGNITION AND NGF PRODUCTION\\nThe recommended dose of H. erinaceus dried fruiting body for increasing NGF production is 3–5 g per day.\\n34 Hericium erinaceus dosed at  \\n250 mg tablets (96% dry powder) three times a day for 16 weeks was associated with signifi-cant improvement on a dementia rating scale in subjects with mild cognitive impairment.\\n28 The \\ndose utilized in the study of menopausal women that showed reduction in symptoms of depression and anxiety was 2.0 g/day of HEFB (in cookies) for 4 weeks.\\n29\\nTOXICOLOGY\\nIn an in vitro model, HEFB aqueous extract demonstrated a remarkable lack of cytotoxicity.\\n31 \\nToxicology studies of H. erinaceus in rats suggest that mycelia enriched with 5 mg/g erinacine A at doses of up to 5 g/kg bodyweight/day are safe. No toxicity was found in the two clinical trials reported on here.\\n28,29\\nREPORTED ADVERSE EVENTS\\nNo adverse clinical or biochemical events were reported in the clinical trial of subjects with mild cognitive impairment.\\n28 In the study of Table 3: Outcomes of clinical trials of H. erinaceus.\\nTrial Parameter \\nassessed/ScaleResults Adverse events Dose Citation\\nDouble-blind, parallel-group, placebo-controlled trialMild cognitive impairment/Revised Hasegawa Dementia Scale (HDS-R)Significant improvement in cognitive function scaleNone 250 mg tid×16 weeks Mori et al., 2009\\n28\\nRandomized placebo-controlled trialAnxiety and depression/Center for Epidemiologic Studies Depression Scale (CES-D) and Indefinite Complaints Index (ICI)Significant improvement in some anxiety and depression scoresNone 2 g/day×4 weeks Nagano et al., 2010\\n29\\n\\nJournal of Restorative Medicine 2017; 6: page 25    \\nLion’s Mane Neurological ActivityREFERENCES\\n1. Hobbs C. Medicinal Mushrooms, 3rd edn. Santa Cruz: \\nBotanica Press; 1995.\\n2. Zhou J, Xie G, Yan X. Encyclopedia of Molecular Structures, Pharmacological Activities, Natural Sources and Applications Traditional Chinese Medicines Vol. 5: Isolated Compounds T–Z. Berlin Heidelberg: Springer-Verlag; 2011.\\n3. Kwagishi H, Shimada A, Shirai R, et al. Erinacines A, B and C strong stimulators of nerve growth factor (NGF)-synthesis from the mycelia of Hericium erinaceum. Tetrahedron Lett. 1994;35:1569–72.\\n4. Shen JW, Yu HY , Ruan Y , et al. Hericenones and erinacines: stimulators of nerve growth factor (NGF) biosynthesis in Hericium erinaceus. Mycol Int J Fungal Biol. 210;1:92–8.\\n5. Mizuno T, Wasa T, Ito H, et al. Antitumor-active poly-\\nsaccharides isolated from the fruiting body of Hericium erinaceum, an edible and medicinal mushroom called Yamabushitake or Houtou. Biosci Biotechnol Biochem. 1992;56:347–8.\\n6. He X, Wang X, Fang J, et al. Structures, biological activities, and industrial applications of the polysaccha-rides from Hericium erinaceus (Lion’s Mane) mushroom: a review. Int J Biol Macromol. 2017;97:228–37.7. Cheng JH, Tsai CL, Lien YY , et al. High molecular weight of polysaccharides from Hericium erinaceus against amyloid beta-induced neurotoxicity. BMC Complement Altern Med. 2016;16:170.\\n8. Cui F, Gao X, Zhang J, et al. Protective effects of extracellular and intracellular polysaccharides on hepa-totoxicity by Hericium erinaceus SG-02. Curr Microbiol. 2016;73(3):379–85.\\n9. Liu J, Du C, Wang Y , Yu Z. Anti-fatigue activities of polysaccharides extracted from Hericium erinaceus. Exp Ther Med. 2015;9(2):483–7.\\n10. Thongbai B, Rapior S, Hyde KD, et al. Hericium erinaceus, an amazing medicinal mushroom. Mycol Progress. 2015;14:91.\\n11. Wang K, Bao L, Qi Q, et al. Erinacerins C-L, isoindolin-1-ones with alpha-glucosidase inhibitory activity from cultures of the medicinal mushroom Hericium erinaceus. J Nat Prod. 2015;78(1);146–54.\\n12. Moldavan MG, Gryganski AP, Kolotushkina OV ,  \\net al. Neurotropic and trophic action of lion’s  \\nmane mushroom Hericium erinaceus (Bull.: Fr.)  \\nPers. (Aphyllophoromycetideae) extracts on  \\nnerve cells in vitro. Int J Med Mushrooms.  \\n2007;9:15–28.menopausal women, one subject reported epi-\\nmenorrhea (18 days menorrhea/month). However, whether or not supplementation with H. eri-naceus was the cause of the epimenorrhea is inconclusive.\\n29\\nAllergies and sensitivities to mushrooms are not unusual. One case report describes a 63-year-old male who suffered acute respiratory failure and lymphocytosis in his lungs. The report suggests he had used an extract of dry H. erinaceus (with no further description given) daily for 4 months in commonly available doses, and the connection between the two was considered to be probable. In another case report, a 53-year-old male exposed to HEFB occupationally, developed chronic dermatitis on his hands, with painful fissures within 1 month of exposure. The dermatitis spread to his forearms, face, and legs, at which point he ceased exposure to the HEFB and his symptoms resolved. His patch tests were negative for the European stan-dard series, and positive for HEFB. Sensitization was confirmed by a highly positive repeated open application test (ROAT) with an aqueous emulsion of HEFB. Interestingly, patch and prick tests were negative for other culinary mushrooms suggesting a lack of cross-sensitivity.\\nCONCLUSION\\nTo the best of this author’s knowledge, no toxicity was established for H. erinaceus in the experimen-tal, animal, or two clinical trials reported here. The adverse event (epimenorrhea) reported in one of the clinical trials could not be conclusively attributed to the intervention. The substantial historical record for the traditional use of lion’s mane for chronic ailments, together with the results of studies so far, suggest H. erinaceus is safe and has important potential as a \\nneuroprotective and neurotrophic therapeutic agent in neurological conditions.\\n35 Its rich myconutrient \\ncomposition suggest that using the whole fungus may be most advantageous clinically. More clinical studies are needed to corroborate these conclusions.\\nCOMPETING INTERESTS\\nThe authors declare they have no competing interests.\\n\\nJournal of Restorative Medicine 2017; 6: page 26\\nLion’s Mane Neurological Activity13. Li JL, Lu L, Dai CC, et al. A comparative study on ste-\\nrols of ethanol extract and water extract from Hericium erinaceus. Zhongguo Zhong Yao Za Zhi. 2001;26:831−4.\\n14. Abdullah N, Ismail SM, Aminudin N, et al. Evaluation of selected culinary-medicinal mushrooms for antioxidant and ACE inhibitory activities. Evid Based Complement Alternat Med. 2012 (2012), Article ID 464238, 12 pages. http://dx.doi.org/10.1155/2012/464238.\\n15. Gershenzon J, Dudareva N. The function of terpene natural products in the natural world. Nat Chem Biol. 2007;3(7):408–14.\\n16. Chappell J. The genetics and molecular genetics of terpene and sterol origami. Curr Opin Plant Biol. 2002;5(2):151–7.\\n17. Guillamón E, García-Lafuente A, Lozano M, et al. Edible mushrooms: role in the prevention of cardiovas-cular diseases. Fitoterapia. 2010;81(7):715–23.\\n18. Rahman MA, Abdullah N, Aminudin N. Inhibitory effect on in vitro LDL oxidation and HMG Co-A reductase activity of the liquid-liquid partitioned frac-tions of Hericium erinaceus (Bull.) Persoon (lion’s mane mushroom). BioMed Res Int. 2014;2014:828149. doi:10.1155/2014/828149.\\n19. Lee KF, Chen JH, Teng CC, et al. Protective effects of Hericium erinaceus mycelium and its isolated erinacine A against ischemia-injury-induced neuronal cell death via the inhibition of iNOS/p38 MAPK and nitrotyrosine. Int J Mol Sci. 2014;15(9):15073–89.\\n20. Yaoita Y , Kakuda R, Machida K, et al. Ceramide con-stituents from five mushrooms. Chem Pharmaceut Bull. 2002;50(5):551–3.\\n21. Mori K, Obara Y , Hirota M, et al. Nerve growth factor-inducing activity of Hericium erinaceus in 1321N1 human astrocytoma cells. Biol Pharm Bull. 2008;31:1727–32.\\n22. Mori K, Obara Y , Moriya T, et al. Effects of Hericium erinaceus on amyloid beta(25–35) peptide-induced learning and memory deficits in mice. Biomed Res. 2011;32(1):67–72.\\n23. Tzeng TT, Chen CC, Lee LY , et al. Erinacine A-enriched Hericium erinaceus mycelium ameliorates Alzheimer’s disease-related pathologies in APPswe/PS1dE9 trans-genic mice. J Biomed Sci. 2016;23:49.\\n24. Trovato A, Siracusa R, Di Paola R, et al. Redox modulation of cellular stress response and lipoxin A4 expression by Hericium erinaceus in rat brain: relevance to Alzheimer’s disease pathogenesis. Immun Ageing. 2016;13:23.\\n25. Zhang J, An S, Hu W, et al. The neuroprotective properties of Hericium erinaceus in glutamate-damaged differentiated PC12 cells and an Alzheimer’s disease mouse model. Int J Mol Sci. 2016;17(11);1810.\\n26. Kuo HC, Lu CC, Shen CH, et al. Hericium eri-naceus mycelium and its isolated erinacine A protection from MPTP-induced neurotoxicity through the ER stress, triggering an apoptosis cascade. J Translat Med. 2016;14:78.\\n27. Wong KH, Naidu M, David RP,  et al. Neuroregenerative potential of lion’s mane mushroom, Hericium erinaceus (Bull.: Fr.) Pers. (higher Basidiomycetes), in the treat-ment of peripheral nerve injury (review). Int J Med Mushrooms. 2012;14(5):427–46.\\n28. Mori K, Inatomi S, Ouchi K, et al. Improving effects of the mushroom Yamabushitake (Hericium erinaceus) on mild cognitive impairment: a double-blind placebo-controlled clinical trial. Phytother Res.  \\n2009;23:367–72.\\n29. Nagano M, Shimizu K, Kondo R, et al. Reduction of depression and anxiety by 4 weeks Hericium erinaceus intake. Biomed Res. 2010;31:231–7.\\n30. Operational guidance: Information needed to support clinical trials of herbal products. http://www.who.int/tdr/publications/documents/operational-guidance-eng.pdf.\\n31. Lai PL, Naidu M, Sabaratnam V , et al. Neurotrophic properties of the Lion’s mane medicinal mushroom, Hericium erinaceus (Higher Basidiomycetes) from Malaysia. Int J Med Mushrooms. 2013;15(6);539–54.\\n32. Chinese pharmacopoeia. Beijing: Chinese Medicine Science and Technology Publishing House; 2010.\\n33. Wong KH, Naidu M, David P, et al. Peripheral nerve regeneration following crush injury to rat peroneal nerve by aqueous extract of medicinal mushroom Hericium erinaceus (Bull.: Fr) Pers. (Aphyllophoromycetideae). Evid Based Complement \\nAlternat Med. 2011;2011:580752.\\n34. Tanaka A, Matsuda H. Expression of nerve growth factor in itchy skins of atopic NC/NgaTnd mice. J Vet Med Sci. 2005;67:915–9.\\n35. Nakatsugawa M, Takahashi H, Takezawa C, et al. Hericium erinaceum (yamabushitake) extract-induced acute respiratory distress syndrome monitored by serum surfactant proteins. Intern Med. 2003;42:1219–22.\\n\\n', metadata={'ipfs_hash': 'QmXbFrfYoUeQp3ho1MEKjtLiC3GGDGe2P9JLPfjngGqyAb', 'file_type': 'application/pdf'})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test PDF\n",
    "display(\"Testing PDF\")\n",
    "res = reader.load(local_ipfs_hash_pdf)\n",
    "display(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infura - Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "infura_ipfs_hash_csv = \"QmQyktKF3TWrAuZAnXWGJfm3b6CdV4oNJfidcV7yrwjsjG\"\n",
    "infura_ipfs_hash_html = \"QmQV4k5TK66FZtys8D7T71AdkQt5gh4eqkm29647wyGcnk\"\n",
    "infura_ipfs_hash_md = \"QmSCETKEvQvrUQmbYZYDfeVBYYGyuAASTz3FC2iU2JQeMN\"\n",
    "infura_ipfs_hash_txt =\"QmbEgLh6jFLbdk78CPoKMyXdwmNqxPpPAjp32nbm6Yu4oB\"\n",
    "infura_ipfs_hash_py =\"QmZCTVui7HsNAgn6YQUBBBbuxJbvFUEMvHiguQRiiWbv2T\"\n",
    "infura_ipfs_hash_pdf = \"QmeYwLHRRkBFCDF6jvRJbvvvSJPqy4zFPM5BE6JYMdjygc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = IPFSFileDataLoader(use_infura=True, infura_api_key=os.getenv(\"INFURA_API_KEY\"), infura_api_secret=os.getenv(\"INFURA_API_SECRET\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Testing CSV'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='PassengerId: 1\\nSurvived: 0\\nPclass: 3\\nName: Braund, Mr. Owen Harris\\nSex: male\\nAge: 22.0\\nSibSp: 1\\nParch: 0\\nTicket: A/5 21171\\nFare: 7.25\\nCabin: nan\\nEmbarked: S\\n', metadata={'index': 0, 'file_type': 'text/csv', 'ipfs_hash': 'QmQyktKF3TWrAuZAnXWGJfm3b6CdV4oNJfidcV7yrwjsjG'}),\n",
       " Document(page_content='PassengerId: 2\\nSurvived: 1\\nPclass: 1\\nName: Cumings, Mrs. John Bradley (Florence Briggs Thayer)\\nSex: female\\nAge: 38.0\\nSibSp: 1\\nParch: 0\\nTicket: PC 17599\\nFare: 71.2833\\nCabin: C85\\nEmbarked: C\\n', metadata={'index': 1, 'file_type': 'text/csv', 'ipfs_hash': 'QmQyktKF3TWrAuZAnXWGJfm3b6CdV4oNJfidcV7yrwjsjG'}),\n",
       " Document(page_content='PassengerId: 3\\nSurvived: 1\\nPclass: 3\\nName: Heikkinen, Miss. Laina\\nSex: female\\nAge: 26.0\\nSibSp: 0\\nParch: 0\\nTicket: STON/O2. 3101282\\nFare: 7.925\\nCabin: nan\\nEmbarked: S\\n', metadata={'index': 2, 'file_type': 'text/csv', 'ipfs_hash': 'QmQyktKF3TWrAuZAnXWGJfm3b6CdV4oNJfidcV7yrwjsjG'}),\n",
       " Document(page_content='PassengerId: 4\\nSurvived: 1\\nPclass: 1\\nName: Futrelle, Mrs. Jacques Heath (Lily May Peel)\\nSex: female\\nAge: 35.0\\nSibSp: 1\\nParch: 0\\nTicket: 113803\\nFare: 53.1\\nCabin: C123\\nEmbarked: S\\n', metadata={'index': 3, 'file_type': 'text/csv', 'ipfs_hash': 'QmQyktKF3TWrAuZAnXWGJfm3b6CdV4oNJfidcV7yrwjsjG'}),\n",
       " Document(page_content='PassengerId: 5\\nSurvived: 0\\nPclass: 3\\nName: Allen, Mr. William Henry\\nSex: male\\nAge: 35.0\\nSibSp: 0\\nParch: 0\\nTicket: 373450\\nFare: 8.05\\nCabin: nan\\nEmbarked: S\\n', metadata={'index': 4, 'file_type': 'text/csv', 'ipfs_hash': 'QmQyktKF3TWrAuZAnXWGJfm3b6CdV4oNJfidcV7yrwjsjG'}),\n",
       " Document(page_content='PassengerId: 6\\nSurvived: 0\\nPclass: 3\\nName: Moran, Mr. James\\nSex: male\\nAge: nan\\nSibSp: 0\\nParch: 0\\nTicket: 330877\\nFare: 8.4583\\nCabin: nan\\nEmbarked: Q\\n', metadata={'index': 5, 'file_type': 'text/csv', 'ipfs_hash': 'QmQyktKF3TWrAuZAnXWGJfm3b6CdV4oNJfidcV7yrwjsjG'}),\n",
       " Document(page_content='PassengerId: 7\\nSurvived: 0\\nPclass: 1\\nName: McCarthy, Mr. Timothy J\\nSex: male\\nAge: 54.0\\nSibSp: 0\\nParch: 0\\nTicket: 17463\\nFare: 51.8625\\nCabin: E46\\nEmbarked: S\\n', metadata={'index': 6, 'file_type': 'text/csv', 'ipfs_hash': 'QmQyktKF3TWrAuZAnXWGJfm3b6CdV4oNJfidcV7yrwjsjG'}),\n",
       " Document(page_content='PassengerId: 8\\nSurvived: 0\\nPclass: 3\\nName: Palsson, Master. Gosta Leonard\\nSex: male\\nAge: 2.0\\nSibSp: 3\\nParch: 1\\nTicket: 349909\\nFare: 21.075\\nCabin: nan\\nEmbarked: S\\n', metadata={'index': 7, 'file_type': 'text/csv', 'ipfs_hash': 'QmQyktKF3TWrAuZAnXWGJfm3b6CdV4oNJfidcV7yrwjsjG'}),\n",
       " Document(page_content='PassengerId: 9\\nSurvived: 1\\nPclass: 3\\nName: Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\\nSex: female\\nAge: 27.0\\nSibSp: 0\\nParch: 2\\nTicket: 347742\\nFare: 11.1333\\nCabin: nan\\nEmbarked: S\\n', metadata={'index': 8, 'file_type': 'text/csv', 'ipfs_hash': 'QmQyktKF3TWrAuZAnXWGJfm3b6CdV4oNJfidcV7yrwjsjG'}),\n",
       " Document(page_content='PassengerId: 10\\nSurvived: 1\\nPclass: 2\\nName: Nasser, Mrs. Nicholas (Adele Achem)\\nSex: female\\nAge: 14.0\\nSibSp: 1\\nParch: 0\\nTicket: 237736\\nFare: 30.0708\\nCabin: nan\\nEmbarked: C\\n', metadata={'index': 9, 'file_type': 'text/csv', 'ipfs_hash': 'QmQyktKF3TWrAuZAnXWGJfm3b6CdV4oNJfidcV7yrwjsjG'})]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test CSV\n",
    "display(\"Testing CSV\")\n",
    "res = reader.load(infura_ipfs_hash_csv)\n",
    "display(res[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Testing HTML'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Document(page_content='<!-- generate a simple html -->\\n\\n<html>\\n<head>\\n<title>My first PHP page</title>\\n</head>\\n<body>\\n    <h1>My first PHP page</h1>\\n    <p>\\n        <?php\\n            echo \"Hello World!\";\\n        ?>\\n    </p>\\n</body>\\n</html>', metadata={'ipfs_hash': 'QmQV4k5TK66FZtys8D7T71AdkQt5gh4eqkm29647wyGcnk', 'file_type': 'text/html'})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test HTML\n",
    "display(\"Testing HTML\")\n",
    "res = reader.load(infura_ipfs_hash_html)\n",
    "display(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Testing Markdown'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Document(page_content='# This is a readme\\n\\n## This too is a readme\\n\\n```python\\nx = 1\\ny = 2\\n\\nprint(x+y)\\n```', metadata={'ipfs_hash': 'QmSCETKEvQvrUQmbYZYDfeVBYYGyuAASTz3FC2iU2JQeMN', 'file_type': 'text/plain'})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test Markdown\n",
    "display(\"Testing Markdown\")\n",
    "res = reader.load(infura_ipfs_hash_md)\n",
    "display(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Testing Text'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Document(page_content='abcd asdasd asd sdf asd adf ev evvrer\\nsdcadcasd  wdcsdcsd sdfsdfsdf\\nsdfdf sdfsdfsdf sdfsdf sddff \\nsdfad sdve dscd', metadata={'ipfs_hash': 'QmbEgLh6jFLbdk78CPoKMyXdwmNqxPpPAjp32nbm6Yu4oB', 'file_type': 'text/plain'})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test Text\n",
    "display(\"Testing Text\")\n",
    "res = reader.load(infura_ipfs_hash_txt)\n",
    "display(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Testing Python'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Document(page_content='import json\\nimport time\\nimport magic\\nimport PyPDF2\\nimport logging\\nimport requests\\nimport tempfile\\nimport pandas as pd\\nfrom PIL import Image\\nfrom langchain.docstore.document import Document\\n\\nlogger = logging.getLogger(__name__)\\n\\ndef identify_file_type_from_bytes(byte_data):\\n    \"\"\"\\n    Identify the file type of a given byte stream.\\n\\n    Args:\\n        byte_data (bytes): Byte stream to identify the file type of.\\n\\n    Returns:\\n        file_type (str): Identified file type of the byte stream.\\n    \"\"\"\\n    mime = magic.Magic(mime=True)\\n    file_type = mime.from_buffer(byte_data)\\n    return file_type\\n\\n# Reader functions for different file types.\\n# Each function takes a response object and returns\\n# the appropriate data format (text, DataFrame, JSON, image).\\n\\ndef pdf_reader(response):\\n    \"\"\"\\n    Read and extract text from a PDF file contained in a response object.\\n\\n    Args:\\n        response (requests.Response): Response object containing PDF content.\\n\\n    Returns:\\n        text (str): Extracted text from the PDF.\\n    \"\"\"\\n    with tempfile.TemporaryFile() as temp:\\n        temp.write(response.content)\\n        pdf = PyPDF2.PdfReader(temp)\\n        text = \"\"\\n        for page in pdf.pages:\\n            text += page.extract_text() + \"\\\\n\\\\n\"\\n\\n        return text\\n    \\ndef txt_reader(response):\\n    \"\"\"\\n    Read text content from a response object.\\n\\n    Args:\\n        response (requests.Response): Response object containing text content.\\n\\n    Returns:\\n        text (str): Decoded text content.\\n    \"\"\"\\n    with tempfile.TemporaryFile() as temp:\\n        temp.write(response.content)\\n        temp.seek(0)\\n        text = temp.read().decode(\"utf-8\")\\n        return text\\n\\ndef lazy_csv_loader(df):\\n    for index, row in df.iterrows():\\n        content = \"\"\\n        for k, v in row.items():\\n            # only string values which are not null and string\\n            if isinstance(v, str) and v.strip() != \"\":\\n                content += f\"{k.strip()}: {v.strip()}\\\\n\"\\n            else:\\n                content += f\"{k}: {v}\\\\n\"\\n        yield Document(\\n            page_content=content,\\n            metadata={\\n                \"index\": index,\\n                \"file_type\": \"text/csv\",\\n            },\\n        )\\n\\ndef csv_reader(response):\\n    \"\"\"\\n    Read CSV content from a response object and return it as a pandas DataFrame.\\n\\n    Args:\\n        response (requests.Response): Response object containing CSV content.\\n\\n    Returns:\\n        df (pandas.DataFrame): DataFrame containing the CSV data.\\n    \"\"\"\\n    with tempfile.TemporaryFile() as temp:\\n        temp.write(response.content)\\n        temp.seek(0)\\n        df = pd.read_csv(temp)\\n    \\n    df = [doc for doc in lazy_csv_loader(df)]\\n\\n    return df\\n\\n    \\ndef json_reader(response):\\n    \"\"\"\\n    Read JSON content from a response object and return it as a dictionary.\\n\\n    Args:\\n        response (requests.Response): Response object containing JSON content.\\n\\n    Returns:\\n        data (dict): Dictionary containing the JSON data.\\n    \"\"\"\\n    with tempfile.TemporaryFile() as temp:\\n        temp.write(response.content)\\n        temp.seek(0)\\n        data = json.load(temp)\\n        # JSON to string\\n        data = json.dumps(data)\\n        return data\\n    \\ndef image_reader(response):\\n    \"\"\"\\n    Read image content from a response object and return it as a PIL image.\\n\\n    Args:\\n        response (requests.Response): Response object containing image content.\\n\\n    Returns:\\n        img (PIL.Image.Image): Image object.\\n    \"\"\"\\n    with tempfile.NamedTemporaryFile() as temp:\\n        temp.write(response.content)\\n        temp.seek(0)\\n        img = Image.open(temp.name)\\n        return img\\n\\ndef select_reader(file_type):\\n    \"\"\"\\n    Select the appropriate reader function based on the file type.\\n\\n    Args:\\n        file_type (str): File type to select the reader for.\\n\\n    Returns:\\n        reader (function): Appropriate reader function for the file type.\\n\\n    Raises:\\n        Exception: If the file type is unsupported.\\n    \"\"\"\\n    if \"pdf\" in file_type:\\n        return pdf_reader\\n    elif \"csv\" in file_type:\\n        return csv_reader\\n    elif \"json\" in file_type:\\n        return json_reader\\n    # elif \"image\" in file_type:\\n        # return image_reader\\n    elif \"text\" in file_type:\\n        return txt_reader\\n    else:\\n        raise Exception(f\"Unsupported file type: {file_type}\")\\n\\n\\nclass IPFSFileloader:\\n    base_url = \"http://127.0.0.1:5001\"\\n    infura_base_url = \"https://ipfs.infura.io:5001\"\\n    base_api_path = \"api/v0\"\\n    read_suffix = \"cat\"\\n    version_suffix = \"version\"\\n    max_retries = 5\\n    retry_delay = 1\\n    def __init__(self, use_infura=False, infura_api_key=None, infura_api_secret=None, debug=False):\\n        \"\"\"\\n        Constructor for IPFSFileloader.\\n\\n        Args:\\n            use_infura (bool, optional): Flag to use Infura instead of local IPFS. Defaults to False.\\n            infura_api_key (str, optional): Infura API key. Required if use_infura is True.\\n            infura_api_secret (str, optional): Infura API secret. Required if use_infura is True.\\n            debug (bool, optional): Enable or disable debug logging. Defaults to False.\\n        \"\"\"\\n        self.use_infura = use_infura\\n        self.infura_api_key = infura_api_key\\n        self.infura_api_secret = infura_api_secret\\n        self.debug = debug\\n\\n        if self.use_infura and (not self.infura_api_key or not self.infura_api_secret):\\n            raise ValueError(\"Infura API key and secret are required when using Infura\")\\n\\n        self.check_daemon_running()\\n\\n    def check_daemon_running(self):\\n        \"\"\"\\n        Checks if the IPFS daemon is running and accessible, or if the Infura API is reachable.\\n        \"\"\"\\n        if self.use_infura:\\n            full_url = f\"{self.infura_base_url}/{self.base_api_path}/{self.version_suffix}\"\\n            auth = requests.auth.HTTPBasicAuth(self.infura_api_key, self.infura_api_secret)\\n        else:\\n            full_url = f\"{self.base_url}/{self.base_api_path}/{self.version_suffix}\"\\n\\n        for i in range(self.max_retries):\\n            try:\\n                response = requests.post(full_url, auth=auth if self.use_infura else None)\\n                if response.ok:\\n                    if self.debug:\\n                        logger.debug(f\"IPFS daemon is running at {full_url}\")\\n                    return\\n                else:\\n                    raise Exception(\"IPFS daemon not running or not accessible\")\\n            except requests.exceptions.ConnectionError:\\n                time.sleep(self.retry_delay)\\n\\n        raise Exception(\"IPFS daemon not running or not accessible\")\\n\\n    def load(self, ipfs_hash):\\n        \"\"\"\\n        Loads a file from IPFS using its hash, either from local IPFS or Infura.\\n        \"\"\"\\n        if self.use_infura:\\n            full_url = f\"{self.infura_base_url}/{self.base_api_path}/{self.read_suffix}\"\\n            auth = requests.auth.HTTPBasicAuth(self.infura_api_key, self.infura_api_secret)\\n        else:\\n            full_url = f\"{self.base_url}/{self.base_api_path}/{self.read_suffix}\"\\n\\n        full_url += f\"?arg={ipfs_hash}\"\\n\\n        for i in range(self.max_retries):\\n            try:\\n                response = requests.post(full_url, auth=auth if self.use_infura else None)\\n                if response.ok:\\n                    file_type = identify_file_type_from_bytes(response.content)\\n                    reader = select_reader(file_type)\\n                    text = reader(response)\\n                    # if isinstance() is string\\n                    if isinstance(text, str):\\n                        return Document(page_content=text, metadata={\"ipfs_hash\": ipfs_hash, \"file_type\": file_type})\\n                    if isinstance(text, list):\\n                        # add ipfs_hash to metadata\\n                        for doc in text:\\n                            doc.metadata[\"ipfs_hash\"] = ipfs_hash\\n                        return text\\n                else:\\n                    logger.error(f\"Failed to load file from IPFS with hash {ipfs_hash}\")\\n            except requests.exceptions.ConnectionError:\\n                time.sleep(self.retry_delay)\\n\\n        raise Exception(f\"Failed to connect to IPFS daemon for hash {ipfs_hash}\")\\n', metadata={'ipfs_hash': 'QmZCTVui7HsNAgn6YQUBBBbuxJbvFUEMvHiguQRiiWbv2T', 'file_type': 'text/x-script.python'})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test Python\n",
    "display(\"Testing Python\")\n",
    "res = reader.load(infura_ipfs_hash_py)\n",
    "display(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Testing PDF'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Document(page_content='1CS3244 : Machine Learning \\nXavier Bresson1Department of Computer ScienceNational University of Singapore (NUS)Lecture 3 : kNN, k-d Tree, Decision Tree, Random Forest, Gradient BoostingSemester 1 2023/24\\nXavier Bressonhttps://twitter.com/xbresson  \\n\\n\\n2Material used for preparation\\nXavier Bresson2\\nProf Kilian Weinberger, CS4780 Cornell, Machine Learning, 2018\\nhttps://www.cs.cornell.edu/courses/cs4780/2018fa \\nProf Min-Yen Kan, CS3244 NUS, Machine Learning, 2022\\nhttps://knmnyn.github.io/cs3244-2210 \\n\\n\\n3Outline\\nXavier Bresson3\\nkNN\\nk-d tree\\nDecision tree\\nBagging\\nBoosting\\nConclusion\\n\\n\\n4Outline\\nXavier Bresson4\\nkNN\\nk-d tree\\nDecision tree\\nBagging\\nBoosting\\nConclusion\\n\\n\\n5\\n<latexit sha1_base64=\"3XUwxeo15JJl0/kJnb65md5BdzQ=\">AAADdXicbVJbaxNBFN4kXmq8tNUXQYRjq20W2jRbipdCoCCCL0JFe4FOsszOnk2GzMyuO7O66XZ/gf/ON/+GL7462cTYph4Y+OZ85/pxgkRwbTqdn7V648bNW7eX7jTv3rv/YHll9eGxjrOU4RGLRZyeBlSj4AqPDDcCT5MUqQwEngSjtxP+5Cummsfqsxkn2JN0oHjEGTXW5a/Wvkf+qJW7sNEFYjA3qSxkHGJJAj5okWK838q3xi7hCj71R35OKsIlpLkxj4dv3AyhnAZAF0bbfxmFNEVtQCEfDIM4hTiCMp/nhRjZsUOgGspJRTLUCWVYdNpvmCxn5UiRb+4TSXPfgmoOP++Pynn3sLQT5psuEfhl0bnpbgEBEsUpFQLsv8oHotFIrjI9rQWkvLoPVSEsNDh37WJEZ9LnlTIXuc+3z31+0U+gUqRfeDtJOWmXdL1/pVofqBpSY6hyZ+TuJfJdxgQP0ZJQNptNf2W90+5UBteBNwPrzswO/ZUfJIxZJlEZJqjWZ14nMb2CpoYzgWWTZBqtoiM6wDMLFZWoe0V1NSW8sJ4QrDb2KQOV93JGQaXWYxnYSEnNUC9yE+f/uLPMRK97BVdJZlCxaaMoE2BimJwghDxFZsTYAspSbmcFNqQpZcYe6kQEb3Hl6+B4t+29bO993Fs/2J3JseQ8cdacluM5r5wD571z6Bw5rPar/rj+rL5W/9142nje2JiG1muznEfOFWvs/AEckhVG</latexit>fk(x)=m o d e\\x00{y:(x, y)2Skx}\\x00withSkx=k\\x00nearest neighbor ofxdeﬁned asSkx={x0: maxx02Skxd(x, x0)\\uf8ffd(x, x00),8x002S\\\\Skx}and d(x, z)=Xi\\x00|xi\\x00zi|p\\x001/p,p= 1 (Manhattan),p= 2 (Euclidean)kNN\\nXavier Bresson5\\nkNN algorithm\\nAssumption : Close data points have similar labels, i.e. class or regression value.\\nAlgorithm : For a test data point x, assign the most common labels in its k nearest neighbors in the training set S.\\nFormalization\\nkNNclassification : \\nkNNregression : \\n<latexit sha1_base64=\"pLcZQhPXPg2YEB4N/bT/tBseJIM=\">AAACIXicbVBNa9tAEF2laeO6H3GaYy9LTIsNxUghpCZQMOTSo0PiJOBVxWo9chbvrsTuqFgI/5Ve8ldyySGl5FbyZyIrPqRJHww83pthZl6cKenQ9/96ay/WX77aaLxuvnn77v1ma+vDqUtzK2AkUpXa85g7UNLACCUqOM8scB0rOItnh0v/7CdYJ1NzgkUGoeZTIxMpOFZS1Oon0awz79LP3yhDmKPVpQZuFiyW0w4ri4PO/EvRZdLQ4x+zaM5qo8tYM2q1/Z5fgz4nwYq0yQrDqHXLJqnINRgUijs3DvwMw5JblELBoslyBxkXMz6FcUUN1+DCsv5wQT9VyoQmqa3KIK3VxxMl184VOq46NccL99Rbiv/zxjkm/bCUJssRjHhYlOSKYkqXcdGJtCBQFRXhwsrqViouuOUCq1CXIQRPX35OTnd7wX5v72ivPdhdxdEgH8kO6ZCAfCUD8p0MyYgI8otckRvy27v0rr0/3u1D65q3mtkm/8C7uwdEbaJP</latexit>fk(x) = mean\\x00{y:(x, y)2Skx}\\x00\\nkNN binary classification with k=3x belongs to blue class\\n\\n6Curse of dimensionality\\nXavier Bresson6\\nEuclidean/Lp distances between points : \\nk-NN assumption “close data points have similar labels” works if we can define a meaningful distance between two data points.\\nUnfortunately, in high-dim spaces, data points sampled from a random probability distribution, are far from each other with (almost) the same distance value. \\nLet us sample points uniformly at random within the unit cube and let us the compute the distance between all pair of points when the dimensionality increases.\\n\\n\\n7Blessing of structure\\nXavier Bresson7\\nReal-world data does not follow a random probability distribution!\\nData has structure, s.a. edges, textures for natural images.\\nThis means that data lie in a much lower dimensional sub-space than Rd. \\nFor example, images of human face can be accurately described with e.g. 50 features s.a. male/female, blond/dark hair, etc, although the original image lie in a space of 1M dimensions (1,000 x 1,000 pixels).\\nRd\\n\\n8kNN summary\\nXavier Bresson8\\nkNN algorithm is the simplest machine learning technique for classification (binary and multiple classes) and regression.\\nIt is expressive as it can produce non-linear boundary decision.\\nAs n→∞, kNNbecomes provably very accurate, but also intractable.\\nReal-world applications have a limited n number of training data.\\nAs d→∞, curse of dimensionality kicks in and kNNbreaks for Euclidean distances.\\nkNN works if distances are semantically meaningful.\\nCombining kNNand deep learning representation is today a strong baseline.\\n\\n\\n9Outline\\nXavier Bresson9\\nkNN\\nk-d tree\\nDecision tree\\nBagging\\nBoosting\\nConclusion\\n\\n\\n10kNN complexity\\nXavier Bresson10\\nkNN time complexity is O(n.d.k), where n the number of training data, d the number of data dimensions and k is the number of k nearest neighbors.\\nThis complexity means that kNN becomes very slow and memory consuming when n is large but we want to have n as large as possible to get the best possible accuracy.\\nCan we improve the speed? Yes, by leveraging data structure. \\n\\n\\n11k-d tree\\nXavier Bresson11\\nGeneral idea : When we search for the closest point(s), most data points are actually far away and hence there is no need to compute the distances for these far away points.\\n How to achieve this goal?\\nSolution is to partition the d-dim feature space with a binary tree structure.\\n\\n\\n12\\nk-d tree speeds up kNN\\nXavier Bresson12\\nFor example, let us consider the full dataset and one partition as follows:\\nMake a cut along one feature dimension that divides the data into two sets, i.e. Left and Right, with approximatively the same number of data in each half.\\nConsider a new data point x, or which we want to find the closest neighbor.\\nIdentify which set the data x lies, here the right set R.\\nFind the nearest neighborxNNRin R, it requires O(n/2). \\nCompute the distance d(x,C) between x and the cut C.\\nIf d(x,C) > d(x, xNNR) then all x in L can be discarded (by triangular inequality) and kNNgets a 2x speed-up! \\nCut/split the space S into 2 sets R and L with ≈ the same number of points\\nAll points in L cannot be NN, we can discard/prune the space by a factor 2.\\n<latexit sha1_base64=\"w1Oy9Bvc2SFpjHPIifHqw4QEoZ8=\">AAACOnicbVBNSwMxEM36WetX1aOXYLFUkLIroh48FOrBU6liVejWkk1TG8wmSzJrW5b9XV78Fd48ePGgiFd/gNvagrY+CLx5b4bJPC8Q3IBtP1tT0zOzc/OphfTi0vLKamZt/dKoUFNWpUoofe0RwwSXrAocBLsONCO+J9iVd1fq+1f3TBuu5AX0Alb3ya3kLU4JJFIjc+YC64L2o2ac7+52G6OyXI5vRvw83skd/+kbFaV4B7suTufcE9WRRGvVSTcyWbtgD4AniTMkWTREpZF5cpuKhj6TQAUxpubYAdQjooFTweK0GxoWEHpHblktoZL4zNSjwekx3k6UJm4pnTwJeKD+noiIb0zP95JOn0DbjHt98T+vFkLrqB5xGYTAJP1Z1AoFBoX7OeIm14yC6CWEUM2Tv2LaJppQSNLuh+CMnzxJLvcKzkFh/2w/W7SHcaTQJtpCeeSgQ1REp6iCqoiiB/SC3tC79Wi9Wh/W50/rlDWc2UB/YH19A73urm8=</latexit>d(x, xRNN)<d(x,C)+\\n\\n13\\nk-d tree speeds up kNN (on average)\\nXavier Bresson13\\nQ: What happens if d(x,C) < d(x, xNNR)? \\nIt is possible that the NN lies in L –so we need to compute all distances d(x,xL).\\nSpeed complexity is then O(n), same as kNN.\\nWorst case complexity of k-d tree is kNNcomplexity, but it is actually much better in practice (average complexity).\\nWe need to compute the distance to all points in L.<latexit sha1_base64=\"G9ICP1SDtT4GnrRnBuMujiCiWuQ=\">AAACO3icbVBNSwMxEM36WetX1aOXYLFUkLIrop6kUA+eShWrQreWbJraYDZZklnbsuz/8uKf8ObFiwdFvHp3W1vQ1geBN+/NMJnnBYIbsO1na2p6ZnZuPrWQXlxaXlnNrK1fGhVqyqpUCaWvPWKY4JJVgYNg14FmxPcEu/LuSn3/6p5pw5W8gF7A6j65lbzFKYFEamTOXWBd0H7UjPPd3W5jVJbL8c2In8c7OXz8p3FUlOId7Lo4nXNPVEcSrVUn3chk7YI9AJ4kzpBk0RCVRubJbSoa+kwCFcSYmmMHUI+IBk4Fi9NuaFhA6B25ZbWESuIzU48Gt8d4O1GauKV08iTggfp7IiK+MT3fSzp9Am0z7vXF/7xaCK2jesRlEAKT9GdRKxQYFO4HiZtcMwqilxBCNU/+immbaEIhibsfgjN+8iS53Cs4B4X9s/1s0R7GkUKbaAvlkYMOURGdogqqIooe0At6Q+/Wo/VqfVifP61T1nBmA/2B9fUNLESumw==</latexit>d(x, xRNN)>d(x,C)+\\n<latexit sha1_base64=\"ylSVsS16ZqJFC7+IjNC5bfFEU80=\">AAACqnicjVFNT9tAEF27LR9poWk5clkRoYIURXaFgEORkLhwoAgiklDFIVqvx2TFem3tjiGR5R/HX+DGv2ETkqqBSjDSSm/evDe7OxNmUhj0vEfH/fDx08Li0nLl85eV1a/Vb9/bJs01hxZPZaovQ2ZACgUtFCjhMtPAklBCJ7w5Gtc7t6CNSNUFjjLoJexaiVhwhpbqV+8DhCHqpBAxLeksicqtYX3Yn6Wnp+XVDDfL7V/v0Z2U2zNIcQDKdp8THszbmnUaBJW/jtRa9J0w8IbtpFHpV2tew5sEfQ38KaiRaZz1qw9BlPI8AYVcMmO6vpdhr2AaBZdQVoLcQMb4DbuGroWKJWB6xWTUJd20TETjVNujkE7Yfx0FS4wZJaFVJgwH5mVtTP6v1s0x3u8VQmU5guLPF8W5pJjS8d5oJDRwlCMLGNfCvpXyAdOMo93ueAj+yy+/Bu2fDX+3sXO+Uzv0puNYIutkg2wRn+yRQ3JMzkiLcOeH89tpOx237jbdP273Weo6U88amQs3egIuOdZZ</latexit>if d(x, xRNN)<d(x, xLNN)t h e nxNN=xRNN,otherwisexNN=xLNN.Complexity is O(n).\\n\\n14k-d tree structure\\nXavier Bresson14\\nTree construction / training stage \\nSplit recursively in half along each feature dimension.\\nIterate over all feature dimensions.\\nTree depth is quite small depth =O(log2n), e.g. log2103≈10, log2106≈20, log2109≈30\\nHow to select which next feature dimension?\\nA good heuristic is to select the feature dimension that captures the largest variation of data (similar to PCA). \\n\\n\\n15k-d tree search\\nXavier Bresson15\\nExample of NN search process in 2D \\nWhich order of search?\\nWhich partitions to prune?\\n\\n\\n16\\nk-d tree search #1\\nXavier Bresson16\\nExample of NN search process in 2D \\nWhich order of search?\\nWhich partitions to prune?\\nBest case scenario O(n/4)\\n\\n\\n17\\nk-d tree search #2\\nXavier Bresson17\\nExample of NN search process in 2D \\nWhich order of search?\\nWhich partitions to prune?\\nWorst case scenario O(n)\\n\\n\\n18\\nk-d tree search\\nXavier Bresson18\\nExample of NN search process in 2D \\nWhich order of search?\\nWhich partitions to prune?\\nWorst case scenario O(n)\\n\\n\\n19\\nk-d tree search\\nXavier Bresson19\\nExample of NN search process in 2D \\nWhich order of search?\\nWhich partitions to prune?\\nWorst case scenario O(n)\\n\\n\\n20\\n<latexit sha1_base64=\"YvsD3pUQn2gS/BeuVAM3k9iTV30=\">AAACP3icbVBBSxtBGJ21tabR1liPXj4ahEgh7AZRL4LgpbcoNFHIxjA7mU0GZ2eGmW/VsOw/8+Jf8Narlx5aSq+9dTYG2qoPBt689z5mvpcYKRyG4ddg6dXr5Tcrtbf11bV379cbGx/6TueW8R7TUtvzhDouheI9FCj5ubGcZonkZ8nlceWfXXHrhFZfcGb4MKMTJVLBKHpp1Oh3WwY+wThOLWWFKovOhSl3IIbYiskUqbX62t+g24qlnow6oKq0DyC/QZsVcC1wCiWYw7+JnXp91GiG7XAOeE6iBWmSBU5Gjft4rFmecYVMUucGUWhwWFCLgkle1uPccUPZJZ3wgaeKZtwNi/n+JWx7ZQyptv4ohLn670RBM+dmWeKTGcWpe+pV4kveIMf0YFgIZXLkij0+lOYSUENVJoyF5QzlzBPKrPB/BTalvkn0lVclRE9Xfk76nXa019493W0edRZ11MgW+UhaJCL75Ih8JiekRxi5JQ/kO/kR3AXfgp/Br8foUrCY2ST/Ifj9B8BFq7A=</latexit>O(p+dn2p)!O(log2n+d)w i t hp=O(log2n)\\n<latexit sha1_base64=\"Q6+CjJBvO6ywHSv3aOpml5SSwyA=\">AAACPnicbVAxTxsxGPVR2tJAS2jHLp8aVQpLdBch6IKExNINkAggxenJ5/guFj7bsr9rFZ3yy1j4DWwdWRiKECtjfSFDCzzJ0tN775O/72VWSY9x/DtaerX8+s3blXet1bX3H9bbGx9PvKkcFwNulHFnGfNCSS0GKFGJM+sEKzMlTrPz/cY//Smcl0Yf49SKUckKLXPJGQYpbQ+or8rUAoUDmsmiCzR3jNd6Vvd/2BlQlKXwEDg09ibswkHX6s2Qp04WE2TOmV/NdFdTZYq0D8Fspe1O3IvngOckWZAOWeAwbV/RseFVKTRyxbwfJrHFUc0cSq7ErEUrLyzj56wQw0A1C1uN6vn5M/galDHkxoWnEebqvxM1K72flllIlgwn/qnXiC95wwrzb6Naaluh0Pzxo7xSgAaaLmEsneCopoEw7mTYFfiEhf4wNN6UkDw9+Tk56feS7d7W0VZnr7+oY4V8Jl9IlyRkh+yR7+SQDAgnF+Sa/CG30WV0E91F94/RpWgx84n8h+jhLxaDq+8=</latexit>XpO\\x00n2p⇥2p\\x00=O(pn)!O(nlog2n)\\n<latexit sha1_base64=\"38ZF7Ude2tU40KRTGQ17XFicKII=\">AAACb3icbVFNb9QwEHXCV1m+lnLgUIRGrJCyl1USVcClUiUu3LZIbFtps0SO42St2o5lTyirKFd+IDf+Axf+Ac52haBlJEtP782zZ54LI4XDOP4RhLdu37l7b+/+6MHDR4+fjJ/un7qmtYwvWCMbe15Qx6XQfIECJT83llNVSH5WXLwf9LMv3DrR6E+4MXylaK1FJRhFT+Xjb5lrVW4ggzlkhagjKLPKUtbpvks/mx4yFIo78HirT+EI5lFp9NRbMivqNVJrm8vhgqjM1s5Qxrt4FqdM9aAz2dR5CkM38q9oVQeXAtfQgzmaR3/UUT6eeM+24CZIdmBCdnWSj79nZcNaxTUySZ1bJrHBVUctCiZ5P8pax/0oF7TmSw819Uusum1ePbz2TAlVY/3RCFv2b0dHlXMbVfhORXHtrmsD+T9t2WL1btUJbVrkml09VLUSsIEhfCiF5QzlxgPKrPCzAltTHzf6LxpCSK6vfBOcprPkzezw4+HkON3FsUcOyCsSkYS8JcfkAzkhC8LIz2A/OAheBL/C5+HLEK5aw2DneUb+qXD6Gwqit8I=</latexit>XpO\\x00dn2p⇥2p\\x00=O(dpn)!O(dnlog2n)w i t hp=O(log2n)k-d tree complexity\\nXavier Bresson20\\nSuppose k=1 (i.e. nearest neighbor)\\nTraining / building k-d tree\\nSpace/memory complexity (worst case) : \\nTime/speed complexity (worst case) :\\nInference / NN search\\nTime/speed complexity (best case) : \\nTime/speed complexity (worst case/NN) : \\nTime/speed complexity (average case/tricky) : \\n<latexit sha1_base64=\"4edVQzyRIUhBDHzXYf1R7aPpsqk=\">AAAB7XicbVBNSwMxEJ2tX7V+VT16CRahXsquFPVY8OLNCvYD2qVks9k2NpssSVYopf/BiwdFvPp/vPlvzLZ70NYHA4/3ZpiZFyScaeO6305hbX1jc6u4XdrZ3ds/KB8etbVMFaEtIrlU3QBrypmgLcMMp91EURwHnHaC8U3md56o0kyKBzNJqB/joWARI9hYqX1XDcV5aVCuuDV3DrRKvJxUIEdzUP7qh5KkMRWGcKx1z3MT40+xMoxwOiv1U00TTMZ4SHuWChxT7U/n187QmVVCFEllSxg0V39PTHGs9SQObGeMzUgve5n4n9dLTXTtT5lIUkMFWSyKUo6MRNnrKGSKEsMnlmCimL0VkRFWmBgbUBaCt/zyKmlf1LzLWv2+Xmlc5HEU4QROoQoeXEEDbqEJLSDwCM/wCm+OdF6cd+dj0Vpw8plj+APn8wcreI4r</latexit>O(dn)\\n<latexit sha1_base64=\"9SUMYVBzvad+UTXcvcPBcKhk+vg=\">AAAB+HicbVDLSsNAFJ3UV62PRl26GSxC3ZSkFHVZcOPOCvYBbQiTyaQdOpkJMxOhhn6JGxeKuPVT3Pk3TtostPXAhcM593LvPUHCqNKO822VNja3tnfKu5W9/YPDqn103FMilZh0sWBCDgKkCKOcdDXVjAwSSVAcMNIPpje5338kUlHBH/QsIV6MxpxGFCNtJN+u3tXDERNjvwn5BaxUfLvmNJwF4DpxC1IDBTq+/TUKBU5jwjVmSKmh6yTay5DUFDMyr4xSRRKEp2hMhoZyFBPlZYvD5/DcKCGMhDTFNVyovycyFCs1iwPTGSM9UateLv7nDVMdXXsZ5UmqCcfLRVHKoBYwTwGGVBKs2cwQhCU1t0I8QRJhbbLKQ3BXX14nvWbDvWy07lu1drOIowxOwRmoAxdcgTa4BR3QBRik4Bm8gjfryXqx3q2PZWvJKmZOwB9Ynz/MXZEv</latexit>O(dlog2n)#hierarchy levels#data pts in each region#regions\\nsplitting time per feature dimension\\n\\n\\n21k-d tree with k nearest neighbors\\nXavier Bresson21\\nNote that the “k” in the name “k”-d tree means the number of data features.\\nNote that the “d” in the name k-”d” tree means “dimension”.\\nIn our lecture, we call the number of data features d and the number of nearest neighborsk.\\n\\n\\n22Summary\\nXavier Bresson22\\nkNN is slow because it does a lot of unnecessary pairwise distance computations.\\nk-d tree partitions the feature space so we can discard space partitions that are further away than our closest k neighbors.\\nPros : \\nExact kNN, but approximation can be used e.g. no backtracking in parent nodes.\\nEasy to implement.\\nAverage inference complexity is O(d.log2n), compared to O(d.n) with kNN.\\nCons : \\nCuts are axis-aligned which does not generalize well to higher dimensions.\\n[Not included] Ball tree partitions the manifold of data points (assumption),                      as opposed to the whole space. This performs much better in higher dimensions.\\n\\n\\n23Outline\\nXavier Bresson23\\nkNN\\nk-d tree\\nDecision tree\\nBagging\\nBoosting\\nConclusion\\n\\n\\n24Motivation\\nXavier Bresson24\\nkNN requires to store the full training set to make a prediction.\\nWhen n becomes large, it becomes intractable.  \\nReal-world assumption : Most data are not random and usually concentrate in regions with the same predicted target e.g. class or regression value. \\nThis enables faster nearest neighbor search with k-d tree data structure.\\nHowever, the ultimate goal is not to find the closest data points, but to solve a classification or a regression problem.\\nThe data identity, i.e. data features, is irrelevant for the classification/regression task. \\nWhat is critical is to identify areas where all points have the same class label.\\nFor example, if a test point falls into a cluster of 1,000 points with all positive class label, then we know that its kNN will all be positive before computing the distances to the 1,000 points. \\n\\n\\n25Decision tree\\nXavier Bresson25\\nDecision tree leverages the idea that a data point has the same class label or same regression value when it falls into a cluster of same label or same regression value.\\nMajor advantage : There is no need to load the full training set for inference. \\nInstead, we can build and load a tree structure that recursively splits the feature space into regions with similar label/value. \\n\\n\\n26Construction\\nXavier Bresson26\\nDecision tree construction / training stage\\nStart from the root node of the tree that represents the entire dataset.\\nSplit this set into two halves with approximatively the same size by cutting along a feature dimension e.g. x1 with a threshold value t1. This produces two sets of data points R and L.\\nThreshold t1 and dimension x1 are chosen such that the resulting children nodes R and L are purer than their parent node S w.r.t. class label or regression value.\\nIf all points in R have the same e.g. positive class label and all points in L have also the same negative class label, then the decision tree is done.\\nIf not, the current leaf nodes are split again until all leaves are pure, i.e. all data points in the node have the same label. \\n\\n\\n27Inference\\nXavier Bresson27\\nInference with decision tree\\nOnce the tree is constructed, there is no need to keep the training set in memory.\\nWhat we need to store\\nThe tree structure which has a depth of log2n ≤ 30.\\nClass probability/regression value in the final leaf nodes.\\nIf pure classes, only class label is stored.\\nDecision tree does not require any distance computation.\\nThe cut is based on feature value.\\nHence, inference is very fast with O(log2n), independent of feature dimension d.\\n\\n\\n28Inference\\nXavier Bresson28\\nInference with decision tree\\nclass probability class label \\n\\n\\n29Optimal decision tree\\nXavier Bresson29\\nQ: Can we build a decision tree that is \\nMaximally compact, i.e. small depth.\\nOnly has pure leaves.\\nYes in theory, if no two data points have same features but different labels.\\nNo in practice, as finding a minimum size tree is NP-hard.\\nBut there exists a greedy algorithm that can approximate effectively small decision trees.\\nWe split the data recursively by minimizing a function that measures label purity in the children’s nodes.\\n\\n\\n30Purity function\\nXavier Bresson30\\n<latexit sha1_base64=\"Iy2UUiiTjzVOtspCLYbZGCc++Co=\">AAAC5nicbVLLbhMxFPUMrxIeDbBkc0VC1UrRKFNVgCpFqgQLloXQhxSnI49zp7HG85DtgY4m8wFsWIAQW76JHR+DhJ1ECFquZPn43MfxvXZcSqHNcPjT869dv3Hz1sbtzp279+5vdh88PNZFpTge8UIW6jRmGqXI8cgII/G0VMiyWOJJnL50/pP3qLQo8nemLnGasfNcJIIzY6mo+2uLGrwwKmteYWJrtJR2tsb7tNm+iMSgjsRO1IhR2J7ltB0ABctSkdOMmXkcN2/bs5ljbdyINuEgCIIBXwaui0Kf90FoMHOEvMpiVFAkwCXTGjWsxKJ05OQG9c5+PUpXMjpKqa5ijQbG7jwejaOQ8qq0Cm6DccRdcumSE8V4s7B1Fq3dFu0fcedwbTrNGTMMPggzB8lilNBP+20n6vaGwXBpcBWEa9AjazuMuj/orOBVhrlZNjEJh6WZNkwZwSW2HVppLBlP2TlOLMxZhnraLJ+phaeWmUFSKLtyA0v274yGZVrXWWwj3Xz1ZZ8j/+ebVCZ5MW1EXlYGc74SSioJpgD35jATCrmRtQWMK2HvCnzO3HDsz3BDCC+3fBUc7wbhs2DvzV7vYHc9jg3ymDwh2yQkz8kBeU0OyRHhHvc+ep+9L/7c/+R/9b+tQn1vnfOI/GP+99/e9OTO</latexit>DeﬁneS:{(xi,yi)ni=1},xi2Rd,yi={1,. . . ,c},cis the number of classesSk={(x, y):y=k},sk⇢S, S=S1[...[Scpk=|Sk||S|fraction of data with labelkWe want pure leaf nodes, i.e. The worst case is when all leaves are random prediction, i.e. To avoid the worst case, we will maximize the KL distance between the random prediction and the best candidate p obtained by splitting\\n<latexit sha1_base64=\"nO9lPCmT/4Lrb+q2k35jfqxhvsw=\">AAACLXicbVBNaxRBEO2JH4nr16pHL4UbiQdZZkJQLwsBPeSYQDYJ7CxDTW9N0kxPd9NdE1yG/UO55K+I4CEiufo3nNnsQRPf6fHeK6rq5U6rwHF8Fa3du//g4frGo97jJ0+fPe+/eHkUbO0ljaXV1p/kGEgrQ2NWrOnEecIq13Scl587//icfFDWHPLc0bTCU6MKJZFbKet/cVk5SiBl+sq+aqCwHhCCI9mFYLPcBDQzWIDLmnJrMYrfQ9pmUGsot1JjeQRlL+sP4mG8BNwlyYoMxAr7Wf97OrOyrsiw1BjCJIkdTxv0rKSmRS+tAzmUJZ7SpKUGKwrTZvntAt62ymx5aGENw1L9e6LBKoR5lbfJCvks3PY68X/epObi07RRxtVMRt4sKmoNbKGrDmbKk2Q9bwlKr9pbQZ6hR8ltwV0Jye2X75Kj7WHyYbhzsDPY3V7VsSFeizfinUjER7Er9sS+GAspLsQ3cSV+RpfRj+hXdH0TXYtWM6/EP4h+/wGqoKXf</latexit>pk= 1 for a speciﬁckandpk0=0,8k06=k\\n<latexit sha1_base64=\"ZK3/o9VEi3EthsbfwWD9XQSTvLw=\">AAACBHicbVDLSsNAFJ3UV62vqMtuBovgQkpSiroRCm5cVrAPaEKYTCftkMkkzEyEErJw46+4caGIWz/CnX/jpM1CWw9cOJxzL/fe4yeMSmVZ30ZlbX1jc6u6XdvZ3ds/MA+P+jJOBSY9HLNYDH0kCaOc9BRVjAwTQVDkMzLww5vCHzwQIWnM79UsIW6EJpwGFCOlJc+sJ1547QQC4czOM5yfQyeIBWIMhjXPbFhNaw64SuySNECJrmd+OeMYpxHhCjMk5ci2EuVmSCiKGclrTipJgnCIJmSkKUcRkW42fyKHp1oZQ71cF1dwrv6eyFAk5SzydWeE1FQue4X4nzdKVXDlZpQnqSIcLxYFKYMqhkUicEwFwYrNNEFYUH0rxFOkE1E6tyIEe/nlVdJvNe2LZvuu3ei0yjiqoA5OwBmwwSXogFvQBT2AwSN4Bq/gzXgyXox342PRWjHKmWPwB8bnD52kl2I=</latexit>pk=1c,8k\\n<latexit sha1_base64=\"HkJ71qJoYXKgJ7fFmfgWc4hUym0=\">AAACVXicbVFdSyMxFM3Mun7U3bW7PvoSLIKClBkR9aUg+CLog4JVoVOHTJppwyQzMbmzWsL8SV/Ef+KLYKYt4teFhJNzz+EmJ4kS3EAQPHn+j7mf8wuLS43lX7//rDT//rs0Rakp69JCFPo6IYYJnrMucBDsWmlGZCLYVZId1f2r/0wbXuQXMFasL8kw5ymnBBwVN0UkyX2scIQjYPegpT05rTbV9u1WJzKljG3WCasbilWc4UgUwyjVhFp3quyt27bfbPiOwwhX2LGdqSisLK0FaaGJEDhrNOJmK2gHk8JfQTgDLTSrs7j5EA0KWkqWAxXEmF4YKOhbooFTwapGVBqmCM3IkPUczIlkpm8nqVR4wzED7Ka7lQOesO8dlkhjxjJxSklgZD73avK7Xq+E9KBvea5KYDmdDkpLgaHAdcR4wDWjIMYOEKq5uyumI+IiAfcRdQjh5yd/BZc77XCvvXu+2zrcmcWxiNbQOtpEIdpHh+gYnaEuougBPXue53uP3os/589Ppb4386yiD+WvvALKCLNK</latexit>maxpKL(p, q)=cXk=1pklogpkqk,withqk=1c,8k\\n\\n31Entropy\\nXavier Bresson31\\nMaximizing the KL distance reduces to minimizing the entropy :\\n<latexit sha1_base64=\"HuIgqeP1PoIpM/bEbneQGQ2UqYI=\">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lKUY8FLx6r2A9oQ9lsN+3SzSbsToRS+g+8eFDEq//Im//GTZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNMbjO/88S1EbF6xGnC/YiOlAgFo2ilh6Q0KFfcqrsAWSdeTiqQozkof/WHMUsjrpBJakzPcxP0Z1SjYJLPS/3U8ISyCR3xnqWKRtz4s8Wlc3JhlSEJY21LIVmovydmNDJmGgW2M6I4NqteJv7n9VIMb/yZUEmKXLHlojCVBGOSvU2GQnOGcmoJZVrYWwkbU00Z2nCyELzVl9dJu1b1rqr1+3qlUcvjKMIZnMMleHANDbiDJrSAQQjP8ApvzsR5cd6dj2VrwclnTuEPnM8fDMKNAQ==</latexit>p\\n<latexit sha1_base64=\"gH9x9gJvmPQ8SJmO4b5DE2vnjLc=\">AAAB7HicbVBNS8NAEJ3Ur1q/oh69LBahXkpSinoseOmxgmkLbSib7aZdutmE3Y1QQn+DFw+KePUHefPfuGlz0NYHA4/3ZpiZFyScKe0431Zpa3tnd6+8Xzk4PDo+sU/PuipOJaEeiXks+wFWlDNBPc00p/1EUhwFnPaC2X3u956oVCwWj3qeUD/CE8FCRrA2kteuJdeVkV116s4SaJO4BalCgc7I/hqOY5JGVGjCsVID10m0n2GpGeF0URmmiiaYzPCEDgwVOKLKz5bHLtCVUcYojKUpodFS/T2R4UipeRSYzgjrqVr3cvE/b5Dq8M7PmEhSTQVZLQpTjnSM8s/RmElKNJ8bgolk5lZEplhiok0+eQju+subpNuouzf15kOz2moUcZThAi6hBi7cQgva0AEPCDB4hld4s4T1Yr1bH6vWklXMnMMfWJ8/ZZ6NuA==</latexit>H(p)\\n<latexit sha1_base64=\"8ZkcBi5RhgC6lhK3nA4F1kTFZ/g=\">AAADn3ichVJdb9MwFHUTPkb4WDce4cFQgTqxVck0MV4qTSDEpE2oSHQbqkvkuE5r1U4824FVUX4Wf4Q3/g1O0m5sXYUlW8f3nHt8c3MjyZk2vv+n4bh37t67v/bAe/jo8ZP15sbmiU4zRWifpDxVZxHWlLOE9g0znJ5JRbGIOD2Nph9K/vQHVZqlyVczk3Qo8DhhMSPY2FC40fiFDL0wSuRHx0Vbbp9vve4inYkwn3aD4juBMpxCxNMxihUmub0V+bk9tuEiD/5kZgILaKPdWhQUOSkFcaow59DmI692vXSrwE55Vjeb+1/DFSZvLk3ISkVNwwW1/NSC6Qarar0yQR4S+CKUCN7SuQWzMr+2FywpZTvLumv8YVtuXdX6MTEqlbOyEV7YbPkdv1pwGQRz0ALz1Qubv9EoJZmgiSEcaz0IfGmGOVaGEU4LD2WaSkymeEwHFiZYUD3Mq/kq4CsbGUH7O+1ODKyi/2bkWGg9E5FVCmwm+iZXBm/jBpmJ3w1zlsjM0ITUD8UZhyaF5bDCEVOUGD6zABPFbK2QTLAdCWNHumxCcPOTl8HJbid429n7stc62J23Yw08Ay9BGwRgHxyAQ9ADfUCc585758g5dl+4n9zPbq+WOo15zlNwbbnf/gKQIiDP</latexit>KL(p, q)=cXk=1pklogpkqk,withqk=1c,8k=Xpklogpk\\x00pklogqk,withqk=1c=Xpklogpk+pklogc=Xpklogpk+ logcXpk,withXpk=1=Xpklogpk+ logcmaxpKL(p, q) = maxpXpklogpk+ logc=m i np\\x00Xpklogpk=m i npH(p) Entropy\\n\\n\\n32Entropy of binary tree\\nXavier Bresson32\\nEntropy of binary tree :\\n<latexit sha1_base64=\"GbeaAL52oOz3HTk15G1sS1QdhO8=\">AAACMXicbZDLSsNAFIYn9VbrrerSTbBYEpSSlKJuCgU3XXRRq71AU8pkOmmHTi7MTISS9pXc+CbipgtF3PoSTtMgWj0w8PGf/3Dm/HZACReGMVdSa+sbm1vp7czO7t7+QfbwqMX9kCHcRD71WceGHFPi4aYgguJOwDB0bYrb9vhm0W8/YMaJ792LSYB7Lhx6xCEICin1s9WqVrto6PlyoNV0yfp5oDUkNHTLyuTLlsMgiqa16Sya3k1nsSHRGt9a7O1nc0bBiEv9C2YCOZBUvZ99tgY+Cl3sCUQh513TCEQvgkwQRPEsY4UcBxCN4RB3JXrQxbwXxRfP1DOpDFTHZ/J5Qo3VnxMRdDmfuLZ0ulCM+GpvIf7X64bCue5FxAtCgT20XOSEVBW+uohPHRCGkaATCRAxIv+qohGUgQgZ8iIEc/Xkv9AqFszLQum2lKsUkzjS4AScAg2Y4ApUQBXUQRMg8AhewCt4U56UufKufCytKSWZOQa/Svn8Akxupis=</latexit>H(L, R)=p(L)H(L)+p(R)H(R)=|L||S|H(L)+|R||S|H(R)\\n<latexit sha1_base64=\"Q8aDKqH64RSKW/Cad6UVEFPcRsI=\">AAAB7HicbVBNT8JAEJ3iF+IX6tHLRmKCF9ISgh5JvHDEaIEEGrJdtrBhu212tyak4Td48aAxXv1B3vw3bqEHBV8yyct7M5mZ58ecKW3b31Zha3tnd6+4Xzo4PDo+KZ+edVWUSEJdEvFI9n2sKGeCupppTvuxpDj0Oe35s7vM7z1RqVgkHvU8pl6IJ4IFjGBtJLddfbgujcoVu2YvgTaJk5MK5OiMyl/DcUSSkApNOFZq4Nix9lIsNSOcLkrDRNEYkxme0IGhAodUeeny2AW6MsoYBZE0JTRaqr8nUhwqNQ990xliPVXrXib+5w0SHdx6KRNxoqkgq0VBwpGOUPY5GjNJieZzQzCRzNyKyBRLTLTJJwvBWX95k3TrNadZa9w3Kq16HkcRLuASquDADbSgDR1wgQCDZ3iFN0tYL9a79bFqLVj5zDn8gfX5AzlwjZs=</latexit>H(S)\\n\\n33Information gain\\nXavier Bresson33\\nDefinition : The information that is gained by splitting a set of data points.\\nIn the case of decision tree, the splitting is controlled by a specific feature value, i.e. xi< ti.\\nThe entropy of the subsets S1,…,Sc is defined as\\nFinally, the information gain (IG) is the difference between the entropy of the original set S and the weighted sum of the entropy of the subset Sk. \\n<latexit sha1_base64=\"tSwnXzWuirttquI/+B+xXNtB6c8=\">AAACP3icbVDLSgMxFM3UV62vqks3waK0UIaZUtRNoeCmy0rtAzp1yKSZNjTzIMkIZTp/5sZfcOfWjQtF3LozfSDaeiG5h3POJbnHCRkV0jCetdTa+sbmVno7s7O7t3+QPTxqiSDimDRxwALecZAgjPqkKalkpBNygjyHkbYzup7q7XvCBQ38WzkOSc9DA5+6FCOpKDvbquUbtlnUdb3YsHHhvGKJyLPjUcVM7jAMlTgq1Ga3ZWWWVMvlCMcTJU4S1SbJjzFjZ3OGbswKrgJzAXJgUXU7+2T1Axx5xJeYISG6phHKXoy4pJiRJGNFgoQIj9CAdBX0kUdEL57tn8AzxfShG3B1fAln7O+JGHlCjD1HOT0kh2JZm5L/ad1Iule9mPphJImP5w+5EYMygNMwYZ9ygiUbK4Awp+qvEA+RSkWqyKchmMsrr4JWSTcv9PJNOVctLeJIgxNwCvLABJegCmqgDpoAgwfwAt7Au/aovWof2ufcmtIWM8fgT2lf37PVq90=</latexit>H(S1,. . . ,Sc)=cXk=1p(Sk)H(Sk)=cXk=1|Sk||S|H(Sk)\\n<latexit sha1_base64=\"TrY1pvjhvch4gRTfDBIug9dSGa8=\">AAACPnicbVBNSwMxFMz6bf2qevQSLEoLddkVUS+C4MF6U2ptoVuXbJpqaLK7JG/FsvSXefE3ePPoxYMiXj2a1gW1OpCXYd48kjdBLLgGx3m0xsYnJqemZ2Zzc/MLi0v55ZULHSWKshqNRKQaAdFM8JDVgINgjVgxIgPB6kH3aNCv3zCleRSeQy9mLUmuQt7hlICR/HzNA3YLSqYnx/1itVz13bJt2+amuLR5UClWS1umfKslz8tluqcT6afdA7d/SXFsTN1SZVg9z88XHNsZAv8lbkYKKMOpn3/w2hFNJAuBCqJ103ViaKVEAaeC9XNeollMaJdcsaahIZFMt9Lh+n28YZQ27kTKnBDwUP05kRKpdU8GxikJXOvR3kD8r9dMoLPfSnkYJ8BC+vVQJxEYIjzIEre5YhREzxBCFTd/xfSaKELBJJ4zIbijK/8lF9u2u2vvnO0UDrezOGbQGlpHReSiPXSIKugU1RBFd+gJvaBX6956tt6s9y/rmJXNrKJfsD4+ASbNqIM=</latexit>IG(S, S1,. . . ,Sc)=H(S)\\x00H(S1,. . . ,Sc)=H(S)\\x00cXk=1p(Sk)H(Sk)\\n\\n34Feature and threshold selection\\nXavier Bresson34\\nGoal is to find subsets that maximizes the information gain, achieving the purest possible subsets. \\nIdentifying the purest subsets is to find a feature xi  and a threshold value ti. \\nDecision tree construction (pseudo-code)\\nWhile leaf nodes are not pure (or ≥ threshold)\\nLoop over (remaining) feature dimensions, i.e. x1,x2,…,xd\\nLoop over n thresholds (e.g. middle points between two consecutive points,            such as ti=(xi+1-xi)/2)\\nCompute information gain for R and L\\nSave (dimension, threshold value) with maximum information gain.\\nSplit space with best (dimension, threshold value) and remove dimension xi from loop.\\nExact complexity is O(n.d). Approximations are used in practice for speed-up.\\n\\n\\n35Regression tree\\nXavier Bresson35\\nIt is straightforward to extend decision tree to other task s.a. regression as long as a purity function can be defined for the new task :\\n<latexit sha1_base64=\"rGtW0UTPSe1NHJ/bvYDx+QOZ1Zs=\">AAADI3icbVLLbhMxFPUMrxIeTWHJ5oqIKpFoNBNVPBZIlbrpgkUhpK0Uh5HHcRIrY89ge9qOpvMvbPgVNixAFRsW/Aue6TSEhCtZOj73nnvsa4dJxLXxvF+Oe+Pmrdt3Nu427t1/8HCzufXoSMepomxA4yhWJyHRLOKSDQw3ETtJFCMijNhxON8v88enTGkeyw8mS9hIkKnkE06JsVSw5bzGhp0bJfKDot3vvNnBOhWQBHPAUTytwDb+lJIxYMWnM0OUis+gZmrl20oJeKIIzf0iv+hfFGWbIG+fP886mEvoF9DOdnBIVJ4VQb/zsbdQQ/uUKE4kZZ0C48Y2numEUJb7VBSLojNuZmC31w1gxQ7W/bIlB8GIrLovqP2IaL2YAxii57a/vStc+/eW/eE9myqm9d/aRiNotryuVwWsA78GLVTHYdC8xOOYpoJJQ0v7oe8lZpQTZTiNWNHAqWbWek6mbGihJILpUV69cQHPLDOGSazskgYqdlmRE6F1JkJbKYiZ6dVcSf4vN0zN5NUo5zJJDZP0ymiSRmBiKD8MjLli1ESZBYQqbs8KdEbs7I39VuUQ/NUrr4OjXtd/0d19t9va69Xj2EBP0FPURj56ifbQATpEA0Sdz85X57vzw/3ifnMv3Z9Xpa5Tax6jf8L9/Qf0xwCb</latexit>H(S)=\\x00Xpklogpk!L(S)=1|S|X(x,y)2S(y\\x00¯yS)2(variance)with ¯yS=1|S|X(x,y)2Sy(mean)Classiﬁcation task Regression task\\nRegression function (blue) learned with regression treeTrue regression function (black)\\n\\n\\n36\\n<latexit sha1_base64=\"Q6+CjJBvO6ywHSv3aOpml5SSwyA=\">AAACPnicbVAxTxsxGPVR2tJAS2jHLp8aVQpLdBch6IKExNINkAggxenJ5/guFj7bsr9rFZ3yy1j4DWwdWRiKECtjfSFDCzzJ0tN775O/72VWSY9x/DtaerX8+s3blXet1bX3H9bbGx9PvKkcFwNulHFnGfNCSS0GKFGJM+sEKzMlTrPz/cY//Smcl0Yf49SKUckKLXPJGQYpbQ+or8rUAoUDmsmiCzR3jNd6Vvd/2BlQlKXwEDg09ibswkHX6s2Qp04WE2TOmV/NdFdTZYq0D8Fspe1O3IvngOckWZAOWeAwbV/RseFVKTRyxbwfJrHFUc0cSq7ErEUrLyzj56wQw0A1C1uN6vn5M/galDHkxoWnEebqvxM1K72flllIlgwn/qnXiC95wwrzb6Naaluh0Pzxo7xSgAaaLmEsneCopoEw7mTYFfiEhf4wNN6UkDw9+Tk56feS7d7W0VZnr7+oY4V8Jl9IlyRkh+yR7+SQDAgnF+Sa/CG30WV0E91F94/RpWgx84n8h+jhLxaDq+8=</latexit>XpO\\x00n2p⇥2p\\x00=O(pn)!O(nlog2n)\\n<latexit sha1_base64=\"TcbnvN/dWjvqNBt8HNUeigQ8yyQ=\">AAACJXicbVDLSgMxFM34rONr1KWbYBHaTZkpRV0oCG7ctYKthU4tmTRtg5lkSO6oZejPuPFX3LiwiODKXzGtXfg6cOHknHvJvSdKBDfg++/O3PzC4tJybsVdXVvf2PS2thtGpZqyOlVC6WZEDBNcsjpwEKyZaEbiSLCr6OZs4l/dMm24kpcwTFg7Jn3Je5wSsFLHO64WytdJEYc41Lw/AKK1urOvakFaEdg96DjDdxwGeISTk2ohFKrfKWNZdF234+X9kj8F/kuCGcmjGWodbxx2FU1jJoEKYkwr8BNoZ0QDp4KN3DA1LCH0hvRZy1JJYmba2fTKEd63Shf3lLYlAU/V7xMZiY0ZxpHtjAkMzG9vIv7ntVLoHbUzLpMUmKRfH/VSgUHhSWS4yzWjIIaWEKq53RXTAdGEgg12EkLw++S/pFEuBQelykUlf1qexZFDu2gPFVCADtEpOkc1VEcUPaAn9ILGzqPz7Lw6b1+tc85sZgf9gPPxCSV3oeM=</latexit>O(2p)!O(n)w i t hp=O(log2n)Complexity\\nXavier Bresson36\\nTraining / building decision tree\\nSpace/memory complexity (worst case) : \\nTime/speed complexity (worst case) :\\nInference / NN search\\nTime/speed complexity :\\n#nodes in the tree\\npurity time\\n<latexit sha1_base64=\"wuYBHcq5+53z+6H78PGk0mkQ1Qc=\">AAACCXicbVC7TsMwFHV4lvIKMLJYVEhlqZKqAsZKLGwUiT6kJooc12mtOnZkO6Aq6srCr7AwgBArf8DG3+C0GaDlSFc6Pude+d4TJowq7Tjf1srq2vrGZmmrvL2zu7dvHxx2lEglJm0smJC9ECnCKCdtTTUjvUQSFIeMdMPxVe5374lUVPA7PUmIH6MhpxHFSBspsGECPehJOhxpJKV4MC94U/WYGAZ1yM9gObArTs2ZAS4TtyAVUKAV2F/eQOA0JlxjhpTqu06i/QxJTTEj07KXKpIgPEZD0jeUo5goP5tdMoWnRhnASEhTXMOZ+nsiQ7FSkzg0nTHSI7Xo5eJ/Xj/V0aWfUZ6kmnA8/yhKGdQC5rHAAZUEazYxBGFJza4Qj5BEWJvw8hDcxZOXSadec89rjdtGpVkv4iiBY3ACqsAFF6AJrkELtAEGj+AZvII368l6sd6tj3nrilXMHIE/sD5/AOKHl9g=</latexit>p!O(log2n)\\n\\n37Outline\\nXavier Bresson37\\nkNN\\nk-d tree\\nDecision tree\\nBagging\\nBoosting\\nConclusion\\n\\n\\n38Bagging\\nXavier Bresson38\\nDecision trees have great advantages at inference\\nSpeed complexity is very fast, i.e. O(log2n)≤ 30\\nMemory complexity is low, i.e. O(n) independent of d\\nHowever, these techniques have high variance performance.\\nThis means that the quality of the classification/regression solutions vary significantly (see next slide).\\nThey are known as weak learners (classifiers or regressors).\\n\\n\\n39Bias and variance (week 5)\\nXavier Bresson39\\nQuality of predictive models are evaluated by their bias-variance properties.\\nFor example, let assume that the task of the model is to predict the red center of the target below \\nLow variance and low biasThe perfect model!High variance and low biasThe model is able to find the correct solution is average.Low variance and high biasThe model favors more some solutions, far from the true ones.Low variance and low biasThe worst model!The model has not only bad bias but also large variance results.\\n\\n40\\n<latexit sha1_base64=\"ZjmUFJx1tSXIA4YEAQqx6rWGEjM=\">AAADG3icnVJNa9RAGJ7Ej9b4ta1HL4OLNmvrkiyleikURPBYWbct7KRhZnayOzRfzLypG0L+h5f+lV48KOJJ8OC/cZKuWtuK4AsDD8/zPvN+zLA8lho877tlX7t+4+bS8i3n9p279+53Vlb3dFYoLkY8izN1wKgWsUzFCCTE4iBXgiYsFvvs6GWj7x8LpWWWvoUyF0FCp6mMJKdgqHDF8p0nJKEwY6x6VYeVO98oezUmTE7H2I3CoTvvPSt7h4OWCkgbzjYm+N82wqiqotrA9d/w3GUbmICYg0oq/E7CDDf+n2nbRKYQVsM1ogumBeDh4dMaRw3TyLk7XOtNjPp//fxqYf3vpitbNrWcsNP1+l4b+DLwF6CLFrEbdr6SScaLRKTAY6r12PdyCCqqQPJY1A4ptMgpP6JTMTYwpYnQQdW+bY0fG2aCo0yZkwJu2fOOiiZalwkzmc0o+qLWkFdp4wKiF0El07wAkfKzQlERY8hw81HwRCrBIS4NoFxJ0yvmM6ooB/OdmiX4F0e+DPYGfX+rv/lms7szWKxjGT1Ej5CLfPQc7aDXaBeNELfeW6fWR+uTfWJ/sD/bX85SbWvheYD+CPvbD1+C85w=</latexit>E(x,y)⇥(fS(x)\\x00y)2⇤=E(x,y)⇥(fS(x)\\x00¯f(x)+¯f(x)\\x00y)2⇤,with¯f(x)=ZS0⇢S⇤fS0(x)p(S0)dS0=E(x,y)⇥(fS(x)\\x00¯f(x))2⇤+E(x,y)⇥(¯f(x)\\x00y)2⇤Bias and variance\\nXavier Bresson40\\nLong history of analysis of the bias-variance trade-off (but recently questioned by deep learning).\\nIt is highly challenging to design the perfect model (i.e. low bias and low variance).\\nFormalization\\ndatatargetAssumption: no noise\\nPredictive model\\nAverage predictor\\nError between prediction model and average over all predictorsVarianceError between average predictor and targetBias\\nTrue data distribution\\nMean over data points\\n\\n\\n41Reducing variance\\nXavier Bresson41\\nDecision trees have low bias but high variance, i.e. solutions vary around the true solution.\\nGoal : Design a technique that reduces variance, i.e. \\nMost common idea is to take the average of multiple solutions, a.k.a. ensemble technique :\\n<latexit sha1_base64=\"FfQKEmQqwMCZD6ucfpbGumeefJw=\">AAACJHicbVDLSgMxFM3Ud32NunQTLEILWmZKUcGNIIJLRWuFzjgkaaaGZjJDkhHLMB/jxl9x48IHLtz4LWZqF9p6IHA4515uzsEJZ0o7zqdVmpqemZ2bXygvLi2vrNpr61cqTiWhLRLzWF5jpChngrY005xeJ5KiCHPaxv3jwm/fUalYLC71IKF+hHqChYwgbaTAPvQiJjzoRUjfYpyd5EFWvd8Z1HLoYdbrwGoYXFTva7seRjILc0NrN42h55cDu+LUnSHgJHFHpAJGOAvsN68bkzSiQhOOlOq4TqL9DEnNCKd52UsVTRDpox7tGCpQRJWfDUPmcNsoXRjG0jyh4VD9vZGhSKlBhM1kEUaNe4X4n9dJdXjgZ0wkqaaC/BwKUw51DIvGYJdJSjQfGIKIZOavkNwiiYg2vRYluOORJ8lVo+7u1ZvnzcpRY1THPNgEW6AKXLAPjsApOAMtQMADeAIv4NV6tJ6td+vjZ7RkjXY2wB9YX99zZKK/</latexit>minE(x,y)⇥(fS(x)\\x00¯f(x))2⇤\\n<latexit sha1_base64=\"Y+a+L55tvrlRX6AmuOTC7IITubQ=\">AAACunicbVFNa9tAEF2pX6n75bbHXoaaBqcEYYXQ5NBAoJceU1wnAa8jRuuVvbFWErujxEboR7a3/puuFJU2SQcWHm/ezOy8iYtUWRqNfnn+g4ePHj/Zetp79vzFy1f9129ObV4aISciT3NzHqOVqcrkhBSl8rwwEnWcyrN49aXJn11JY1WefadNIWcaF5lKlEByVNT/kUTj4XoHOBaFydfAl0hVUjfU9hHwxKCowrrSNXBb6qi6PArrCw1JVI2jy1bGjVosCY3Jr4HHaLpqDpzkmoyuAC24ctC3lCpLaMN5b/uP7HopjWyErnO4GwTB7jjSvJ0bW0kwvvj4t+WQTClhjoQwdyYZFZfNPjtQ96L+YBSM2oD7IOzAgHVxEvV/8nkuSi0zEilaOw1HBc0qNKREKuseL60sUKxwIacOZqilnVWt9TV8cMwckty4lxG07L8VFWprNzp2So20tHdzDfm/3LSk5HBWqawoSWbiZlBSpkA5NHd0axspKN04gMIo91cQS3TnInftxoTw7sr3weleEH4K9r/tD473Oju22Dv2ng1ZyA7YMfvKTtiECe/Am3mJt/A/+7Gv/NWN1Pe6mrfsVvj0Gzkj1kY=</latexit>fS(x)⇡ˆf(x)=1mmXj=1fSj(x)!¯f(x) asm!1where{S1,. . . ,Sm}⇢S⇤(true data distribution)\\n\\n42Reducing variance\\nXavier Bresson42\\nWhy averaging classifiers reduces variance ? \\nBecause of the law of large numbers :The law of large numbers states for i.i.d.(independent and identically distributed)       random variable xiwith mean    :\\nApply to learners : Assume we have m training datasets S1,…,Smsampled from S*, the true data distribution.Train a learner on each training set and average the result:\\n<latexit sha1_base64=\"nOdTILUR2vDYGAohPMuHCzPoYs0=\">AAAB73icbVBNS8NAEJ34WetX1aOXxSJ4Kkkp6rHgxWMF+wFtKJvttl262cTdiVhC/4QXD4p49e9489+4aXPQ1gcDj/dmmJkXxFIYdN1vZ219Y3Nru7BT3N3bPzgsHR23TJRoxpsskpHuBNRwKRRvokDJO7HmNAwkbweTm8xvP3JtRKTucRpzP6QjJYaCUbRSpxdQnT7Niv1S2a24c5BV4uWkDDka/dJXbxCxJOQKmaTGdD03Rj+lGgWTfFbsJYbHlE3oiHctVTTkxk/n987IuVUGZBhpWwrJXP09kdLQmGkY2M6Q4tgse5n4n9dNcHjtp0LFCXLFFouGiSQYkex5MhCaM5RTSyjTwt5K2JhqytBGlIXgLb+8SlrVindZqd3VyvVqHkcBTuEMLsCDK6jDLTSgCQwkPMMrvDkPzovz7nwsWtecfOYE/sD5/AHZL4/O</latexit>¯x\\n<latexit sha1_base64=\"k7Ce2MqYdmFNS435bINH7vYM5JE=\">AAACPHicbVA9TyMxFPQCx0GOgwAljUV0ElW0ixDQnIREQwmCAFI2rN463sTC9q7st3eJrP1hNPwIOioaChCipcYJKfgaydJoZp6e36SFFBbD8DaYmp75Mftzbr72a+H34lJ9eeXU5qVhvMVymZvzFCyXQvMWCpT8vDAcVCr5WXq5P/LP/nFjRa5PcFjwjoKeFplggF5K6sdxZoC5qHKqorEtVeLE36i6UHSQCBob0esjGJP/p3EKxg18CPkAjXIULK2o+hAROsNhLak3wmY4Bv1KoglpkAkOk/pN3M1ZqbhGJsHadhQW2HFgUDDJq1pcWl4Au4Qeb3uqQXHbcePjK/rHK12a5cY/jXSsvp9woKwdqtQnFWDffvZG4ndeu8Rst+OELkrkmr0tykpJMaejJmlXGM5QDj0BZoT/K2V98G2i73tUQvT55K/kdLMZbTe3jrYae5uTOubIGlknGyQiO2SPHJBD0iKMXJE78kAeg+vgPngKnt+iU8FkZpV8QPDyCqpxr4Y=</latexit>1mmXi=1xi!¯xasm!1\\n<latexit sha1_base64=\"GJpOyExZki/dqvaa+U/yOLuToS8=\">AAACSnicbVBNSyMxGM5UXbXqbnWPewkWwVOZEdndiyB48ajsVoVOHd5JM200yQzJO2oJ8/u8ePLmj/DiQVn2Yqb24NcDgYfngyRPWkhhMQzvgsbM7NyX+YXF5tLyytdvrdW1I5uXhvEuy2VuTlKwXArNuyhQ8pPCcFCp5Mfp+V7tH19wY0Wu/+K44H0FQy0ywQC9lLQgHgG6rKI7NM4MMBdVTlU0tqVK3NlOVJ0qmiXuT3LmRSOGIwRj8ksap2DqWoz8Co1yFCytqHoTETrDcTNptcNOOAH9SKIpaZMpDpLWbTzIWam4RibB2l4UFth3YFAwyatmXFpeADuHIe95qkFx23eTKSq64ZUBzXLjj0Y6UV83HChrxyr1SQU4su+9WvzM65WY/e47oYsSuWYvF2WlpJjTelc6EIYzlGNPgBnh30rZCPyi6NevR4jef/kjOdrqRD8724fb7d2t6RwL5AdZJ5skIr/ILtknB6RLGLkm9+SRPAU3wUPwL/j/Em0E08538gaN2WcIgbQP</latexit>ˆf=1mmXj=1fSj!¯fasm!1\\n\\n\\n43Bagging\\nXavier Bresson43\\nAveraging several classifiers/regressors will decrease the variance and make the ensemble classifier/regressor more accurate.\\nBut we do not have access to more training sets {S1,..,Sm} than the original set S.\\nBecause we do not know the true distribution S*.\\nHow do we create new training sets? \\nSolution is bagging.\\nBagging algorithm\\nSample m datasets S1,..,Sm from original S with replacement.\\nFor each training set Sj, train a classifier fSj.\\nFinal/ensemble classifier is \\n<latexit sha1_base64=\"FpXhZZpXdyZE54fgmsoijfNj8LE=\">AAACGXicbVDLSgMxFM3UV62vqks3wSLUTZkpRd0UCm5cVrQP6NQhk2batMnMkGTEEuY33Pgrblwo4lJX/o3pY6GtBwKHc87l5h4/ZlQq2/62Miura+sb2c3c1vbO7l5+/6Apo0Rg0sARi0TbR5IwGpKGooqRdiwI4j4jLX90OfFb90RIGoW3ahyTLkf9kAYUI2UkL2+7A6R0kBYfTmEVuoFAWDup5il0ZcI9Paw66R2HgadvvOEk5eULdsmeAi4TZ04KYI66l/90exFOOAkVZkjKjmPHqquRUBQzkubcRJIY4RHqk46hIeJEdvX0shSeGKUHg0iYFyo4VX9PaMSlHHPfJDlSA7noTcT/vE6igouupmGcKBLi2aIgYVBFcFIT7FFBsGJjQxAW1PwV4gEy7ShTZs6U4CyevEya5ZJzVqpcVwq18ryOLDgCx6AIHHAOauAK1EEDYPAInsEreLOerBfr3fqYRTPWfOYQ/IH19QNPup/U</latexit>ˆf(x)=1mmXj=1fSj(x)\\n\\n44Sampling with replacement\\nXavier Bresson44\\nWhat is sampling with replacement?\\nWhen a data is selected, it continues to be part of the set and can be sampled again (unlike sampling without replacement, once a data is selected then it is removed from the set and cannot be sampled again). \\nSampling with replacementSampling without replacement\\n\\n\\n45Bagging\\nXavier Bresson45\\nSampling with replacement breaks the assumption of the law of large numbers as training sets Sj have data not i.i.d. and therefore, there is no theoretical guarantee that the following convergence is true anymore :\\nHowever, in practice, bagging reduces the variance quite effectively.\\nBut after a large number m, there will be a diminishing return.\\nImportantly, bagging can reduce the variance without increasing the error of an unbiased classifier.\\nUnbiased classifier produces the correct solution is average.\\n<latexit sha1_base64=\"mPxIKr052/HW4uVSE6SXhe2f0J0=\">AAACKXicbVBNa9tAFFw5beO6H3GSYy9LTaEnIxnT5mIw9JJjSurYYLniab2y196VxO5Tiln0d3LJX8klhZak1/6RrmwfWrsDC8PMG96+iXMpDPr+o1c7ePL02WH9eePFy1evj5rHJ1cmKzTjA5bJTI9iMFyKlA9QoOSjXHNQseTDePmp8ofXXBuRpV9wlfOJglkqEsEAnRQ1++Ec0CYl7dEw0cBsUFpV0tAUKrKLXlB+VTSJ7GW0cKIWszmC1tk3Gsagq1gjarb8tr8G3SfBlrTIFhdR83s4zViheIpMgjHjwM9xYkGjYJKXjbAwPAe2hBkfO5qC4mZi15eW9J1TpjTJtHsp0rX6d8KCMmalYjepAOdm16vE/3njApOziRVpXiBP2WZRUkiKGa1qo1OhOUO5cgSYFu6vlM3BFYau3KqEYPfkfXLVaQcf2t3P3Va/s62jTt6Qt+Q9CchH0ifn5IIMCCM35I78ID+9W+/ee/B+bUZr3jZzSv6B9/sPvQim4A==</latexit>ˆf=1mmXj=1fSj!¯f\\n<latexit sha1_base64=\"mPxIKr052/HW4uVSE6SXhe2f0J0=\">AAACKXicbVBNa9tAFFw5beO6H3GSYy9LTaEnIxnT5mIw9JJjSurYYLniab2y196VxO5Tiln0d3LJX8klhZak1/6RrmwfWrsDC8PMG96+iXMpDPr+o1c7ePL02WH9eePFy1evj5rHJ1cmKzTjA5bJTI9iMFyKlA9QoOSjXHNQseTDePmp8ofXXBuRpV9wlfOJglkqEsEAnRQ1++Ec0CYl7dEw0cBsUFpV0tAUKrKLXlB+VTSJ7GW0cKIWszmC1tk3Gsagq1gjarb8tr8G3SfBlrTIFhdR83s4zViheIpMgjHjwM9xYkGjYJKXjbAwPAe2hBkfO5qC4mZi15eW9J1TpjTJtHsp0rX6d8KCMmalYjepAOdm16vE/3njApOziRVpXiBP2WZRUkiKGa1qo1OhOUO5cgSYFu6vlM3BFYau3KqEYPfkfXLVaQcf2t3P3Va/s62jTt6Qt+Q9CchH0ifn5IIMCCM35I78ID+9W+/ee/B+bUZr3jZzSv6B9/sPvQim4A==</latexit>ˆf=1mmXj=1fSj!¯f?\\n\\n46Bagging\\nXavier Bresson46\\nAdvantages\\nEasy to implement\\nEasy to reduces variance for high variance classifiers/regressors.\\nBagging also provides an error estimate of the test error (for free).\\nDuring sampling Sj, some training data xk will not be selected and hence can act as a test data for the ensemble of classifiers. \\n\\n\\n47Random forest\\nXavier Bresson47\\nOne of the most popular and useful bagging algorithms is random forest.\\nRandom forest is an ensemble of decision trees.\\nAlgorithm\\nSample m training datasets S1,…,Sm from S with replacement.\\nFor each Sj, train a decision tree fSj with one important modification :\\nOnly consider a randomly small number of k of splits with k≤ d features. \\nGoal is to make sure that all classifiers fSj are all very different. As such, they will make different errors at test time, but averaging will correct most of the errors.\\nFinal classifier is \\nHyper-parameters \\n           is a good heuristic\\nm is as large as computational resource permits\\n<latexit sha1_base64=\"FpXhZZpXdyZE54fgmsoijfNj8LE=\">AAACGXicbVDLSgMxFM3UV62vqks3wSLUTZkpRd0UCm5cVrQP6NQhk2batMnMkGTEEuY33Pgrblwo4lJX/o3pY6GtBwKHc87l5h4/ZlQq2/62Miura+sb2c3c1vbO7l5+/6Apo0Rg0sARi0TbR5IwGpKGooqRdiwI4j4jLX90OfFb90RIGoW3ahyTLkf9kAYUI2UkL2+7A6R0kBYfTmEVuoFAWDup5il0ZcI9Paw66R2HgadvvOEk5eULdsmeAi4TZ04KYI66l/90exFOOAkVZkjKjmPHqquRUBQzkubcRJIY4RHqk46hIeJEdvX0shSeGKUHg0iYFyo4VX9PaMSlHHPfJDlSA7noTcT/vE6igouupmGcKBLi2aIgYVBFcFIT7FFBsGJjQxAW1PwV4gEy7ShTZs6U4CyevEya5ZJzVqpcVwq18ryOLDgCx6AIHHAOauAK1EEDYPAInsEreLOerBfr3fqYRTPWfOYQ/IH19QNPup/U</latexit>ˆf(x)=1mmXj=1fSj(x)\\n<latexit sha1_base64=\"5P5QHk8gm3/RnTz6j8rrw7cRcKQ=\">AAAB8nicbVBNS8NAEN3Ur1q/qh69LBbBU0lKUS9CwYvHCrYV2lA2m027dLMbdydCCf0ZXjwo4tVf481/46bNQVsfDDzem2FmXpAIbsB1v53S2vrG5lZ5u7Kzu7d/UD086hqVaso6VAmlHwJimOCSdYCDYA+JZiQOBOsFk5vc7z0xbbiS9zBNmB+TkeQRpwSs1J9cD8yjhiycVYbVmlt358CrxCtIDRVoD6tfg1DRNGYSqCDG9D03AT8jGjgVbFYZpIYlhE7IiPUtlSRmxs/mJ8/wmVVCHCltSwKeq78nMhIbM40D2xkTGJtlLxf/8/opRFd+xmWSApN0sShKBQaF8/9xyDWjIKaWEKq5vRXTMdGEgk0pD8FbfnmVdBt176LevGvWWo0ijjI6QafoHHnoErXQLWqjDqJIoWf0it4ccF6cd+dj0Vpyiplj9AfO5w8Uv5EV</latexit>k=pd\\n\\n48Random forest\\nXavier Bresson48\\nExample with two-moon binary classification \\n\\n\\n49Outline\\nXavier Bresson49\\nkNN\\nk-d tree\\nDecision tree\\nBagging\\nBoosting\\nConclusion\\n\\n\\n50Boosting\\nXavier Bresson50\\nLet us consider the case where classifiers/regressors have high bias, i.e. these prediction models have large errors on the training set. \\nAn example of such high-bias models are decision trees with limited depth, e.g. value 4.\\nQ: Can we design an ensemble method that combines a large number of weak learners to generate a strong learner with low bias? \\nYes, this class of algorithms is called boosting.\\nBoosting reduces bias.\\n\\n\\n51Boosting\\nXavier Bresson51\\nVanilla boosting algorithm\\nAssume we have an ensemble classifier at step t= T, i.e.\\nPrediction error is defined with a loss function as L(f) as\\nWe would like to add a new (weak) learner f to FTto decrease the prediction error as much as possible. The weak learner f is selected by minimizing the following loss :\\nAfter we found the new learner ft+1, we simply add it to FT :\\n<latexit sha1_base64=\"zVjkc7MfIsRScBg16DlpXMV2KAQ=\">AAACG3icbVDLSsNAFJ3UV42vqks3g0VoQUpSiropFNy4cFHBPqCJYTKdtEMnkzAzEUvIf7jxV9y4UMSV4MK/cfpYaOuBezmccy8z9/gxo1JZ1reRW1ldW9/Ib5pb2zu7e4X9g7aMEoFJC0csEl0fScIoJy1FFSPdWBAU+ox0/NHlxO/cEyFpxG/VOCZuiAacBhQjpSWvUL0uBWVYh04gEE7tLOUZdGQSeimt29kdhw5hrBSUHjxaPh3rZkKvULQq1hRwmdhzUgRzNL3Cp9OPcBISrjBDUvZsK1ZuioSimJHMdBJJYoRHaEB6mnIUEumm09syeKKVPgwioYsrOFV/b6QolHIc+noyRGooF72J+J/XS1Rw4aaUx4kiHM8eChIGVQQnQcE+FQQrNtYEYUH1XyEeIp2S0nGaOgR78eRl0q5W7LNK7aZWbFTnceTBETgGJWCDc9AAV6AJWgCDR/AMXsGb8WS8GO/Gx2w0Z8x3DsEfGF8/P2WfAg==</latexit>L(f)=1nnXi=1`(f(xi),yi)\\n<latexit sha1_base64=\"SQDm+1fzBD5uL+imfQYr0toqYZE=\">AAACPXicbVBNSxxBEO1Z86GTr9UcvTQugQ2GZUYkyUURAsGDB4VdXdgZhprent3G7p6huya4DPPHvPgfvHnLJYeIePVq70cgWfOg4fWrelTVSwspLAbBjddYefb8xcvVNf/V6zdv3zXXN05tXhrGeyyXuemnYLkUmvdQoOT9wnBQqeRn6fm3af3sBzdW5LqLk4LHCkZaZIIBOilpdrOkwu2w3ouQX6BRFZiRErpOqiwSmh7WNKJH7e9JdzsCWYyBZh8/OWn+2Q/oHxsFPaRWgZS1T5NmK+gEM9CnJFyQFlngOGleR8OclYprZBKsHYRBgbHbBQWTvPaj0vIC2DmM+MBRDYrbuJpdX9MPThnSLDfuaaQz9W9HBcraiUpdpwIc2+XaVPxfbVBi9jWuhC5K5JrNB2WlpJjTaZR0KAxnKCeOADPC7UrZGAwwdIH7LoRw+eSn5HSnE37u7J7stg52FnGskk2yRdokJF/IATkkx6RHGLkkP8lvcutdeb+8O+9+3trwFp735B94D48Baa0v</latexit>ft+1= argminf2HL(FT+↵f),↵>0 and small\\n<latexit sha1_base64=\"n4pEdr0iJ3ql8g3Zg6Brvw68434=\">AAACDHicbVDLSsNAFL2prxpfVZduBosgFEpSiroRCoK4rNAXtCFMppN26OTBzEQooR/gxl9x40IRt36AO//GSZuFth4YOJxzLnfu8WLOpLKsb6Owtr6xuVXcNnd29/YPSodHHRklgtA2iXgkeh6WlLOQthVTnPZiQXHgcdr1JjeZ332gQrIobKlpTJ0Aj0LmM4KVltxS+dZNWxV7hq5RxmaoggaYx2OMfDdV2jBNpFNW1ZoDrRI7J2XI0XRLX4NhRJKAhopwLGXftmLlpFgoRjidmYNE0hiTCR7RvqYhDqh00vkxM3SmlSHyI6FfqNBc/T2R4kDKaeDpZIDVWC57mfif10+Uf+WkLIwTRUOyWOQnHKkIZc2gIROUKD7VBBPB9F8RGWOBidL9mboEe/nkVdKpVe2Lav2+Xm7U8jqKcAKncA42XEID7qAJbSDwCM/wCm/Gk/FivBsfi2jByGeO4Q+Mzx8Suphr</latexit>FT+1=FT+↵ft+1H: hypothesis space<latexit sha1_base64=\"gp0VPBY6JWbRehzv/z3hwpuYjHM=\">AAACHnicbVDJSgNBEO1xjXGLevTSGIQIEmZCXC6RgCAeI2SDTBx6Oj1Jk56F7hoxDPkSL/6KFw+KCJ70b+wsoCY+KHi8V0VVPTcSXIFpfhkLi0vLK6uptfT6xubWdmZnt67CWFJWo6EIZdMligkesBpwEKwZSUZ8V7CG278c+Y07JhUPgyoMItb2STfgHqcEtORkTq6cau7+CJewrWLfSaBkDW+r2CYi6hEHsOeAto/tH+XCTKedTNbMm2PgeWJNSRZNUXEyH3YnpLHPAqCCKNWyzAjaCZHAqWDDtB0rFhHaJ13W0jQgPlPtZPzeEB9qpYO9UOoKAI/V3xMJ8ZUa+K7u9An01Kw3Ev/zWjF45+2EB1EMLKCTRV4sMIR4lBXucMkoiIEmhEqub8W0RyShoBMdhWDNvjxP6oW8dZov3hSz5cI0jhTaRwcohyx0hsroGlVQDVH0gJ7QC3o1Ho1n4814n7QuGNOZPfQHxuc3dJWfhw==</latexit>FT(x)=TXt=1↵tft(x),↵t>0\\n\\n52Gradient boosting\\nXavier Bresson52\\nHow do we solve the optimization problem?\\nWe use a first-order Taylor approximation of L : \\n<latexit sha1_base64=\"IxDnNzEZ+dVlhVaCefiNl+5X91w=\">AAACG3icbVBNSwMxEM36bf2qevQSLIIilN1S1IsgCNKDhwptFbplyabZNphkl2RWLMv+Dy/+FS8eFPEkePDfmH4ctPpg4PHeDDPzwkRwA6775czMzs0vLC4tF1ZW19Y3iptbLROnmrImjUWsb0JimOCKNYGDYDeJZkSGgl2Ht+dD//qOacNj1YBBwjqS9BSPOCVgpaBYiYIMDr381Ad2D1pmRPckV3mQRT5XuJZjH1/uXwSNQ5+IpE9wdBAUS27ZHQH/Jd6ElNAE9aD44XdjmkqmgApiTNtzE+jYTcCpYHnBTw1LCL0lPda2VBHJTCcb/ZbjPat0cRRrWwrwSP05kRFpzECGtlMS6Jtpbyj+57VTiE46GVdJCkzR8aIoFRhiPAwKd7lmFMTAEkI1t7di2ieaULBxFmwI3vTLf0mrUvaOytWraumsMoljCe2gXbSPPHSMzlAN1VETUfSAntALenUenWfnzXkft844k5lt9AvO5zenK6B1</latexit>ft+1= argminf2HL(FT+↵f)\\n<latexit sha1_base64=\"W7m6Bcmnu45n2/7+pR0fDJ157uc=\">AAACfXicdVFda9RAFJ3Erxq/Vn305eJS2bUlJKVYX4SCUHzoQwW3Leys4WZ2sjt0MhlmJtIl5F/4y3zzr/iikzSgtnphmMM598zHubmWwrok+R6Et27fuXtv63704OGjx09GT5+d2qo2jM9YJStznqPlUig+c8JJfq4NxzKX/Cy/eN/pZ1+4saJSn9xG80WJKyUKwdB5Kht9PZ4c7VCUeo1QTOEVRa1NdQmensIODAqVqFaSA1WYS+zF3QKo6VlKo//ZbF1mjXiXtp8V0MIga6hG4wRKOG5/46PJZSambVz0exRlo3ESJ33BTZAOYEyGOslG3+iyYnXJlWMSrZ2niXaLpjueSd5GtLZcI7vAFZ97qLDkdtH06bWw7ZklFJXxSzno2T8dDZbWbsrcd5bo1va61pH/0ua1K94uGqF07bhiVxcVtQRXQTcKWArDmZMbD5AZ4d8KbI0+JecH1oWQXv/yTXC6F6dv4v2P++PDvSGOLfKCvCQTkpIDckg+kBMyI4z8CCCYBq+Dn+F2uBvGV61hMHiek78qPPgFuxG8Fw==</latexit>L(F+↵f)⇡L(F)+↵hrL(F),fi⇡L(F)+↵nXi=1@L@F(xi).f(xi)gradient\\n<latexit sha1_base64=\"NWIP2zsXOKvrdLfnLIlKzCYcZgw=\">AAACtHicbVFba9swFJa9W+ddmm6PezksrDgPC3YoXRkECqNjDx10bGkKUebJipyISrKR5C3B+BfubW/7N5PjQLukBwQf3+Uc6SgtBDc2iv56/r37Dx4+2nscPHn67Pl+5+DFpclLTdmI5iLXVykxTHDFRpZbwa4KzYhMBRun1x8affyTacNz9c2uCjaVZK54ximxjko6v7FlS6tldbYkshAMfnG7gM9fz+B9DRgOz8OP4TLhvd4QZ5rQKq6rQY1TPg+hFd6uEg4N0fs+AIyDQ2iNuCDaciLgvL7BbaSGIdzdor+VDW8Mvd02w9v5IAiSTjfqR+uCXRBvQBdt6iLp/MGznJaSKUsFMWYSR4WdVs0UKlgd4NKwgtBrMmcTBxWRzEyr9dJreOOYGWS5dkdZWLO3ExWRxqxk6pyS2IXZ1hryLm1S2uxkWnFVlJYp2g7KSgE2h+YHYcY1o1asHCBUc3dXoAvi9mbdPzdLiLefvAsuB/34uH/05ah7OtisYw+9Qq9RiGL0Dp2iT+gCjRD1Ym/s/fCIf+xjn/qstfreJvMS/Ve++geKas6D</latexit>Example with MSE :L(F(xi)) =12\\x00F(xi)\\x00yi\\x002@L@F(xi)=\\x00F(xi)\\x00yi\\x00.@(F(xi)\\x00yi)@F(xi)=F(xi)\\x00yi\\n\\n53Gradient boosting\\nXavier Bresson53\\nOptimization problem :\\nAt last, we need an algorithm that computes f ∈H, H is known as the hypothesis space, i.e. a space of solutions for the task at hand.\\nAs the goal of boosting is to reduce the bias of predictors, the space H is supposed to contain high-bias models s.a. decision trees with limited depth.\\nAnother major advantage of gradient boosting is that solution f does not have to be exact, i.e. any approximation f can used as long as the dot product is negative (see next slides for justification) :\\n<latexit sha1_base64=\"mtJwSQ6AwLTmYB8nAEe8BFp0zqA=\">AAACyXicjVFdaxQxFM2MX3X96KqPvlxcLLtUlplS1BehICwF+1Ch2xY263Anm9kNzWRikim7DvPkP/TNN3+KmemA2op4IeRwzj33JvemWgrrouh7EN66fefuva37vQcPHz3e7j95emqL0jA+ZYUszHmKlkuh+NQJJ/m5NhzzVPKz9OJ9o59dcmNFoU7cRvN5jkslMsHQeSrp/6COr53JKzTLXKg6qTIqFBzWQOFoOElOdilKvULIRrBDUWtTrOHfnhHsQmeiEtVScqAKU4mt+CoDajqW9nbgP2pSW+ZJJd7F9ScFNDPIKqrROIESjupfeDJcJ2JUj7P27vnyPpL+IBpHbcBNEHdgQLo4Tvrf6KJgZc6VYxKtncWRdvOq6cEkr3u0tFwju8Aln3moMOd2XrWbqOGlZxaQFcYf5aBlf3dUmFu7yVOfmaNb2etaQ/5Nm5UuezuvhNKl44pdNcpKCa6AZq2wEIYzJzceIDPCvxXYCv2onF9+M4T4+pdvgtO9cfx6vP9xf3Cw141jizwnL8iQxOQNOSCH5JhMCQsmgQzK4DL8EH4O1+GXq9Qw6DzPyB8Rfv0JBs/aSA==</latexit>argminf2HL(FT+↵f)⇡argminf2HL(F)+↵hrL(F),fi⇡argminf2HnXi=1@L@F(xi).f(xi)\\n<latexit sha1_base64=\"zwx3GGYPYPMDvJxrEepnV3qH33o=\">AAACKXicbVDLSsNAFJ3UV42vqks3g0Wom5KUoi4UCoK4cFHBPqCpYTKdtEMnkzAzEUvI77jxV9woKOrWH3HSFtTWA8MczrmXe+/xIkalsqwPI7ewuLS8kl8119Y3NrcK2ztNGcYCkwYOWSjaHpKEUU4aiipG2pEgKPAYaXnD88xv3REhachv1Cgi3QD1OfUpRkpLbqHmyDhwE3pmp7ccOr5AOHEiJBRFDF6lP/yidO/Sw7Tsj394apmOY2q4haJVtsaA88SekiKYou4WXpxeiOOAcIUZkrJjW5HqJtkYzEhqOrEkEcJD1CcdTTkKiOwm40tTeKCVHvRDoR9XcKz+7khQIOUo8HRlgNRAznqZ+J/XiZV/0k0oj2JFOJ4M8mMGVQiz2GCPCoIVG2mCsKB6V4gHSKeldLhZCPbsyfOkWSnbR+XqdbVYq0zjyIM9sA9KwAbHoAYuQR00AAYP4Am8gjfj0Xg23o3PSWnOmPbsgj8wvr4B+Z+kZw==</latexit>nXi=1@L@F(xi).f(xi)<0\\n\\n\\n54Gradient boosting\\nXavier Bresson54\\nStep-by-step optimization :\\n<latexit sha1_base64=\"dDXEvQPOYWct8HCJSZlNh98Icic=\">AAADCHicjVLLahsxFNVMX6n7iNMuC0XUtDiQGI8JTTeBQMFk4UVK4yRguUYj3xmLaDSDdCeNGWbZTX+lmy5aSrf9hO76N9XY00celF4QHJ17z7nSlcJMSYvd7g/Pv3b9xs1bK7cbd+7eu7/aXHtwaNPcCBiKVKXmOOQWlNQwRIkKjjMDPAkVHIUnL6v80SkYK1N9gPMMxgmPtYyk4OioyZr3+BlDOEOTFK/zLEst0JIO2v31HRYZLoqgLHolC2Xcpv3NOa3Q+pveBv0lojgD7STLapZxg5IrOij/4H6546SMNX53Oqg19ZabOJG6nBQRk5ruuYTiOlbwb9ONiDJT1w0gQiPjGXJj0rfnjPnZFcab/+vcmDRb3U53EfQyCGrQInXsT5rf2TQVeQIaheLWjoJuhuOiMhcKygbLLWRcnPAYRg5qnoAdF4uHLOlTx0xplBq3NNIF+7ei4Im18yR0lQnHmb2Yq8ircqMcoxfjQuosR9Bi2SjKFcWUVr+CTqUBgWruABdGurNSMeNuRuj+TjWE4OKVL4PDXid43tl6tdXa7dXjWCGPyBPSJgHZJrtkj+yTIRHeO++D98n77L/3P/pf/K/LUt+rNQ/JufC//QSCNfdl</latexit>SupposeL(F)=12\\x00F\\x00y\\x002,then@L@F=F\\x00yThen argminf2Hh@L@F,fi,argmaxf2Hh\\x00@L@F,fiFind candidate f that best aligns to –gradient, which always points to the solution y.\\n\\n\\n55Gradient boosting\\nXavier Bresson55\\nStep-by-step optimization :\\n\\n\\n56Gradient boosting (AnyBoost)\\nXavier Bresson56\\nPseudo-code\\n<latexit sha1_base64=\"Mxr8fHj040ae0BH435xmbuOz348=\">AAAD4HicbVNNb9NAEN3YfJTw0RSOXEZEVIlaRXZVAaqIVIQUFamHIjVtpThYm806XnW9NrtraGT5xIUDCHHlZ3Hjj3Bm7Thy2jAHezwz773Z2fEk4Uxpx/nTsOxbt+/c3bjXvP/g4aPN1tbjMxWnktAhiXksLyZYUc4EHWqmOb1IJMXRhNPzyeXbIn/+iUrFYnGq5wkdR3gmWMAI1ibkbzX+bnuaXmkZZe9EkuoDyOF4FzzMkxCbd9a58tnu3GddLzefEdYhwTx7k8MSBh0jOGVEx7Kbe15ze+A7fadwlhWDWBpW3XcOTsvwxxRPYeYz6IMXSEwyL8FSM8zhuDPwdaHYLSXzOjMoo6uqM4mnjArdhbxmDfxM77h5f1mG5SxiIvezwGMCjgwePJVGfsb6bv5BFF30woVgTf2Z6dA0HBT91QfuXJtFt9Zc4lhgQOvsi45KjddOLaJDKlY6rx8AgwXEyJtpwE51GVAxwboy5YrCSmKVbFkjqU6lUSw5m02/1XZ6Tmmw7riV00aVnfit3940JmlkJk44VmrkOokeZ8X1EE7zppcqmmByiWd0ZFyBI6rGWbmgOTw3EXM5Zg+CWGgoo6uIDEdKzaOJqSwGrm7miuD/cqNUB6/GGSs2lwqyEApSDjqGYtthyiQlms+Ng4lkplcgITY7p80/UQzBvXnkdedsr+e+6O2/328f7lXj2EBP0TPUQS56iQ7RETpBQ0SssfXF+mZ9tyf2V/uH/XNRajUqzBN0zexf/wDkzzwD</latexit>Input:L,↵,{(xi,yi)},A(predictor)F0=0Fort=0:Tgi=@L(Ft(xi),yi)@F(xi)(gradient)ft+1= argminf2HnXi=1gi.h(xi),withf=A({(xi,yi)})ifnXi=1gi.ht+1(xi)<0 thenFt+1=Ft+↵ht+1elsereturnFt\\n\\n57Summary\\nXavier Bresson57\\nBoosting is a powerful technique to turn weak learners into a strong learner.\\nClass of boosting algorithms \\nGradient boosting \\nClassification and regression tasks \\nWeak learners are regression trees of depth 4\\nAdaptative boosting (AdaBoost)\\nSpecific case of binary classification (cannot be applied directly to multi-class and regression)\\nSpecific case of exponential loss, i.e. \\nStep size α can be computed optimally (closed-form solution)\\nTraining error decreases exponentially, O(log  n) convergence (can be proved)\\nA hybrid algorithm that combines advantages of bagging and boosting can be designed :\\nStochastic gradient boosting : sub-sample with replacement + low-depth trees\\n<latexit sha1_base64=\"E7zHR0k3ty4XcO6i2+uYwhOu1Sc=\">AAACDHicbVDLSgMxFM34rOOr6tJNsAjtwmGmFHVTKAjFhYsK9gF9DJk004ZmMkOSEcswH+DGX3HjQhG3foA7/8b0sdDWA4HDOedyc48XMSqVbX8bK6tr6xubmS1ze2d3bz97cNiQYSwwqeOQhaLlIUkY5aSuqGKkFQmCAo+Rpje6mvjNeyIkDfmdGkekG6ABpz7FSGnJzeZu8tVCuSPjwE1o2Ul7HJJecjZ2qVXNP7i0kJqmTtmWPQVcJs6c5MAcNTf71emHOA4IV5ghKduOHalugoSimJHU7MSSRAiP0IC0NeUoILKbTI9J4alW+tAPhX5cwan6eyJBgZTjwNPJAKmhXPQm4n9eO1b+ZTehPIoV4Xi2yI8ZVCGcNAP7VBCs2FgThAXVf4V4iATCSvc3KcFZPHmZNIqWc26Vbku5SnFeRwYcgxOQBw64ABVwDWqgDjB4BM/gFbwZT8aL8W58zKIrxnzmCPyB8fkDSNSZLw==</latexit>L(F)=nXi=1e\\x00yi.F(xi)\\n\\n58Outline\\nXavier Bresson58\\nkNN\\nk-d tree\\nDecision tree\\nBagging\\nBoosting\\nConclusion\\n\\n\\n59Conclusion\\nXavier Bresson59\\nkNN is a simple and expressive learning technique but time and memory consuming.  \\nk-d tree speeds up kNN by discarding far away data points.\\nDecision tree improves the memory complexity (no loading of data points required) and speeds up inference with tree structure.\\nBagging is an ensemble method that combines a large number of weak learners with high variance to generate a strong learner with low variance. \\nBoosting is an ensemble method that combines a large number of weak learners with high bias to generate a strong learner with low bias. \\nBagging/boosting are universal, i.e. agnostic of the algorithm used.\\nUse these ensemble techniques to boost your algorithm accuracy by a few percentage,         e.g. to win Kaggle competitions J.\\n\\n\\n60\\nQuestions?\\nXavier Bresson60\\n\\n', metadata={'ipfs_hash': 'QmeYwLHRRkBFCDF6jvRJbvvvSJPqy4zFPM5BE6JYMdjygc', 'file_type': 'application/pdf'})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test PDF\n",
    "display(\"Testing PDF\")\n",
    "res = reader.load(infura_ipfs_hash_pdf)\n",
    "display(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local IPFS Node - Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = IPFSDirectoryDataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to load file alvan-nee-9M0tSjb-cpA-unsplash.jpeg with hash QmaRFiHfJMDVmXNmS9FnvsDF9LUmaUUWs9jior9URVEdt2: Unsupported file type: image/jpeg\n",
      "Skipping file alvan-nee-9M0tSjb-cpA-unsplash.jpeg with hash QmaRFiHfJMDVmXNmS9FnvsDF9LUmaUUWs9jior9URVEdt2\n",
      "Failed to load file alvan-nee-Id1DBHv4fbg-unsplash.jpeg with hash QmYqCcsZ6AURu8siTNiTnXyBUBgb6bxGvtj7nchcqNKAM3: Unsupported file type: image/jpeg\n",
      "Skipping file alvan-nee-Id1DBHv4fbg-unsplash.jpeg with hash QmYqCcsZ6AURu8siTNiTnXyBUBgb6bxGvtj7nchcqNKAM3\n",
      "Failed to load file alvan-nee-bQaAJCbNq3g-unsplash.jpeg with hash QmWUtwgcGTKm3kz741GbN6kK9AUbPXiyH36st95rwQ48n9: Unsupported file type: image/jpeg\n",
      "Skipping file alvan-nee-bQaAJCbNq3g-unsplash.jpeg with hash QmWUtwgcGTKm3kz741GbN6kK9AUbPXiyH36st95rwQ48n9\n",
      "Failed to load file alvan-nee-brFsZ7qszSY-unsplash.jpeg with hash QmRMDTS3TaecLFZBTqDfRNgY8KnNnGca6wRdDzEMBrfYBR: Unsupported file type: image/jpeg\n",
      "Skipping file alvan-nee-brFsZ7qszSY-unsplash.jpeg with hash QmRMDTS3TaecLFZBTqDfRNgY8KnNnGca6wRdDzEMBrfYBR\n",
      "Failed to load file alvan-nee-eoqnr8ikwFE-unsplash.jpeg with hash QmRaKmgBTGbm1PWtwc3Ds9snYohstdGbtzY8dBXhGd61ga: Unsupported file type: image/jpeg\n",
      "Skipping file alvan-nee-eoqnr8ikwFE-unsplash.jpeg with hash QmRaKmgBTGbm1PWtwc3Ds9snYohstdGbtzY8dBXhGd61ga\n",
      "Failed to load file page_0.png with hash QmRRLspWSxvp1xjbgGjpP8rfwPqfqqs3Svv4A3hUdioDeB: Unsupported file type: image/png\n",
      "Skipping file page_0.png with hash QmRRLspWSxvp1xjbgGjpP8rfwPqfqqs3Svv4A3hUdioDeB\n"
     ]
    }
   ],
   "source": [
    "# The following hash is a directory containing a CSV file\n",
    "# It skips files that doesnt have a supported extension such as images\n",
    "r = reader.load(\"QmNzHAn1dRoNesgJPNn7QYkU3MN1S6FhHdpDxBuvvTDuZd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Pumperla, Oakes & Liaw Learning RayLearning Ray\\nMax Pumperla,  \\nEdward Oakes  \\n& Richard Liaw\\nForeword by Ion StoicaLearning Ray\\nFlexible Distributed Python for Machine Learning\\nCompliments of\\n\\nDATA“A fantastic introduction \\nto the Ray distributed \\ncomputing framework \\nfrom the perspective \\nof three of its key \\ncontributors. The authors \\ngracefully decompose \\nthe daunting topics of \\ndistributed computing \\nand machine learning \\ninto a series of easy-\\nto-follow examples.”\\n—Patrick Ames\\nPrincipal Engineer, Amazon\\n“The definitive book on \\ndistributed systems \\napplied to Machine \\nLearning. This is an \\naccessible introduction \\nto building massively \\ndistributed data \\napplications from \\nthe comfort of your \\nJupyter notebook.”\\n—Mark Saroufim\\nStaff Applied AI Engineer, PyTorch, MetaLearning Ray\\nTwitter: @oreillymedia\\nlinkedin.com/company/oreilly-media\\nyoutube.com/oreillymedia Get started with Ray, the open source distributed \\ncomputing framework that simplifies the process of \\nscaling compute-intensive Python workloads. With this \\npractical book, Python programmers, data engineers, \\nand data scientists will learn how to leverage Ray locally \\nand spin up compute clusters. You’ll be able to use Ray to \\nstructure and run machine learning programs at scale.\\nAuthors Max Pumperla, Edward Oakes, and Richard Liaw \\nshow you how to build machine learning applications \\nwith Ray. You’ll understand how Ray fits into the current \\nlandscape of machine learning tools and discover \\nhow Ray continues to integrate ever more tightly \\nwith these tools. Distributed computation is hard, \\nbut by using Ray you’ll find it easy to get started.\\n• Learn how to build your first distributed \\napplications with Ray Core\\n• Conduct hyperparameter optimization with Ray Tune\\n• Use the Ray RLlib library for reinforcement learning\\n• Manage distributed training with the Ray Train library \\n• Use Ray to perform data processing with Ray Datasets\\n• Learn how to work with Ray Clusters and \\nserve models with Ray Serve\\n• Build end-to-end machine learning applications with Ray AIR\\nMax Pumperla  is a data science professor, and a software engineer \\nat Anyscale.\\nEdward Oakes  is a software engineer and team lead at Anyscale .\\nRichard Liaw  is a software engineer at Anyscale.\\nUS $65.99  CAN $82.99\\nISBN: 978-1-098-11722-1Pumperla, Oakes & Liaw\\nISBN: 978-1-098-135164\\n\\n\\n\\n\\n\\nMax Pumperla, Edward Oakes, and Richard LiawLearning Ray\\nFlexible Distributed Python\\nfor Machine Learning\\nBoston Farnham Sebastopol Tokyo Beijing Boston Farnham Sebastopol Tokyo Beijing\\n\\n978-1-098-13516-4\\n[LSI]Learning Ray\\nby Max Pumperla, Edward Oakes, and Richard Liaw\\nCopyright © 2023 Max Pumperla and O’Reilly Media, Inc. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\\nalso available for most titles ( http://oreilly.com ). For more information, contact our corporate/institutional\\nsales department: 800-998-9938 or corporate@oreilly.com .\\nAcquisitions Editor:  Jessica Haberman\\nDevelopment Editor:  Jeff Bleiel\\nProduction Editor:  Katherine Tozer\\nCopyeditor:  Piper Editorial Consulting, LLC\\nProofreader:  Kim WimpsettIndexer:  Ellen Troutman-Zaig\\nInterior Designer:  David Futato\\nCover Designer:  Karen Montgomery\\nIllustrator:  Kate Dullea\\nFebruary 2023:  First Edition\\nRevision History for the First Edition\\n2023-02-13: First Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781098117221  for release details.\\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Learning Ray , the cover image, and\\nrelated trade dress are trademarks of O’Reilly Media, Inc.\\nThe views expressed in this work are those of the authors, and do not represent the publisher’s views.\\nWhile the publisher and the authors have used good faith efforts to ensure that the information and\\ninstructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\\nfor errors or omissions, including without limitation responsibility for damages resulting from the use\\nof or reliance on this work. Use of the information and instructions contained in this work is at your\\nown risk. If any code samples or other technology this work contains or describes is subject to open\\nsource licenses or the intellectual property rights of others, it is your responsibility to ensure that your use\\nthereof complies with such licenses and/or rights.\\nThis work is part of a collaboration between O’Reilly and Anyscale. See our statement of editorial\\nindependence .\\n\\nFür Alma\\n\\n\\n\\nTable of Contents\\nForeword. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xiii\\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xv\\n1.An Overview of Ray. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\\nWhat Is Ray?                                                                                                                       2\\nWhat Led to Ray?                                                                                                           2\\nRay’s Design Principles                                                                                                  4\\nThree Layers: Core, Libraries, and Ecosystem                                                           5\\nA Distributed Computing Framework                                                                           6\\nA Suite of Data Science Libraries                                                                                     8\\nRay AIR and the Data Science Workflow                                                                   8\\nData Processing with Ray Datasets                                                                            10\\nModel Training                                                                                                             12\\nHyperparameter Tuning                                                                                              16\\nModel Serving                                                                                                               18\\nA Growing Ecosystem                                                                                                     20\\nSummary                                                                                                                           21\\n2.Getting Started with Ray Core. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  23\\nAn Introduction to Ray Core                                                                                         24\\nA First Example Using the Ray API                                                                           25\\nAn Overview of the Ray Core API                                                                             35\\nUnderstanding Ray System Components                                                                     36\\nScheduling and Executing Work on a Node                                                             36\\nThe Head Node                                                                                                             39\\nDistributed Scheduling and Execution                                                                     39\\nvii\\n\\nA Simple MapReduce Example with Ray                                                                     41\\nMapping and Shuffling Document Data                                                                   43\\nReducing Word Counts                                                                                               45\\nSummary                                                                                                                           47\\n3.Building Your First Distributed Application. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  49\\nIntroducing Reinforcement Learning                                                                           49\\nSetting Up a Simple Maze Problem                                                                               50\\nBuilding a Simulation                                                                                                      55\\nTraining a Reinforcement Learning Model                                                                  59\\nBuilding a Distributed Ray App                                                                                     62\\nRecapping RL Terminology                                                                                            66\\nSummary                                                                                                                           67\\n4.Reinforcement Learning with Ray RLlib. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  69\\nAn Overview of RLlib                                                                                                     70\\nGetting Started with RLlib                                                                                              71\\nBuilding a Gym Environment                                                                                    71\\nRunning the RLlib CLI                                                                                                73\\nUsing the RLlib Python API                                                                                       75\\nConfiguring RLlib Experiments                                                                                     82\\nResource Configuration                                                                                              83\\nRollout Worker Configuration                                                                                   83\\nEnvironment Configuration                                                                                       84\\nWorking with RLlib Environments                                                                               85\\nAn Overview of RLlib Environments                                                                        85\\nWorking with Multiple Agents                                                                                   86\\nWorking with Policy Servers and Clients                                                                 90\\nAdvanced Concepts                                                                                                         93\\nBuilding an Advanced Environment                                                                         94\\nApplying Curriculum Learning                                                                                  95\\nWorking with Offline Data                                                                                         97\\nOther Advanced Topics                                                                                               98\\nSummary                                                                                                                           99\\n5.Hyperparameter Optimization with Ray Tune. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  101\\nTuning Hyperparameters                                                                                              102\\nBuilding a Random Search Example with Ray                                                      102\\nWhy Is HPO Hard?                                                                                                    104\\nAn Introduction to Tune                                                                                              105\\nHow Does Tune Work?                                                                                              106\\nviii | Table of Contents\\n\\nConfiguring and Running Tune                                                                               110\\nMachine Learning with Tune                                                                                       115\\nUsing RLlib with Tune                                                                                               115\\nTuning Keras Models                                                                                                 116\\nSummary                                                                                                                         119\\n6.Data Processing with Ray. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  121\\nRay Datasets                                                                                                                    122\\nRay Datasets Basics                                                                                                    123\\nComputing Over Ray Datasets                                                                                 126\\nDataset Pipelines                                                                                                        127\\nExample: Training Copies of a Classifier in Parallel                                             130\\nExternal Library Integrations                                                                                       134\\nBuilding an ML Pipeline                                                                                               136\\nSummary                                                                                                                         138\\n7.Distributed Training with Ray Train. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  139\\nThe Basics of Distributed Model Training                                                                 139\\nIntroduction to Ray Train by Example                                                                       141\\nPredicting Big Tips in NYC Taxi Rides                                                                   141\\nLoading, Preprocessing, and Featurization                                                            142\\nDefining a Deep Learning Model                                                                            143\\nDistributed Training with Ray Train                                                                       144\\nDistributed Batch Inference                                                                                     147\\nMore on Trainers in Ray Train                                                                                     148\\nMigrating to Ray Train with Minimal Code Changes                                          150\\nScaling Out Trainers                                                                                                  152\\nPreprocessing with Ray Train                                                                                   153\\nIntegrating Trainers with Ray Tune                                                                         154\\nUsing Callbacks to Monitor Training                                                                      156\\nSummary                                                                                                                         156\\n8.Online Inference with Ray Serve. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  157\\nKey Characteristics of Online Inference                                                                     158\\nML Models Are Compute Intensive                                                                        158\\nML Models Aren’t Useful in Isolation                                                                     159\\nAn Introduction to Ray Serve                                                                                      160\\nArchitectural Overview                                                                                             160\\nDefining a Basic HTTP Endpoint                                                                            161\\nScaling and Resource Allocation                                                                              163\\nRequest Batching                                                                                                        165\\nTable of Contents | ix\\n\\nMultimodel Inference Graphs                                                                                  166\\nEnd-to-End Example: Building an NLP-Powered API                                            170\\nFetching Content and Preprocessing                                                                      172\\nNLP Models                                                                                                                172\\nHTTP Handling and Driver Logic                                                                          173\\nPutting It All Together                                                                                               175\\nSummary                                                                                                                         176\\n9.Ray Clusters. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  179\\nManually Creating a Ray Cluster                                                                                 180\\nDeployment on Kubernetes                                                                                          182\\nSetting Up Y our First KubeRay Cluster                                                                   183\\nInteracting with the KubeRay Cluster                                                                     184\\nExposing KubeRay                                                                                                     186\\nConfiguring KubeRay                                                                                                187\\nConfiguring Logging for KubeRay                                                                          189\\nUsing the Ray Cluster Launcher                                                                                  190\\nConfiguring Y our Ray Cluster                                                                                  190\\nUsing the Cluster Launcher CLI                                                                              191\\nInteracting with a Ray Cluster                                                                                  191\\nWorking with Cloud Clusters                                                                                      192\\nAWS                                                                                                                              192\\nUsing Other Cloud Providers                                                                                   193\\nAutoscaling                                                                                                                     194\\nSummary                                                                                                                         194\\n10. Getting Started with the Ray AI Runtime. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  195\\nWhy Use AIR?                                                                                                                195\\nKey AIR Concepts by Example                                                                                    197\\nRay Datasets and Preprocessors                                                                               198\\nTrainers                                                                                                                        199\\nTuners and Checkpoints                                                                                           201\\nBatch Predictors                                                                                                         203\\nDeployments                                                                                                               204\\nWorkloads That Are Suited for AIR                                                                            207\\nAIR Workload Execution                                                                                          209\\nAIR Memory Management                                                                                       211\\nAIR Failure Model                                                                                                      212\\nAutoscaling AIR Workloads                                                                                     213\\nSummary                                                                                                                         213\\nx | Table of Contents\\n\\n11. Ray’s Ecosystem and Beyond. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  215\\nA Growing Ecosystem                                                                                                   216\\nData Loading and Processing                                                                                   216\\nModel Training                                                                                                           218\\nModel Serving                                                                                                             222\\nBuilding Custom Integrations                                                                                  225\\nAn Overview of Ray’s Integrations                                                                          226\\nRay and Other Systems                                                                                                 227\\nDistributed Python Frameworks                                                                             227\\nRay AIR and the Broader ML Ecosystem                                                               228\\nHow to Integrate AIR into Y our ML Platform                                                       230\\nWhere to Go from Here?                                                                                              231\\nSummary                                                                                                                         232\\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  235\\nTable of Contents | xi\\n\\n\\n\\nForeword\\nFor the past decade, the computation demands of machine learning and data applica‐\\ntions have vastly outgrown the capabilities of a single server or a single processor,\\nincluding hardware accelerators such as GPUs and TPUs. This trend leaves us no\\nchoice but to distribute these applications. Unfortunately, building such distributed\\napplications is notoriously difficult.\\nOver the past few years, Ray has emerged as the framework of choice to simplify\\nthe development of such applications. Ray includes a flexible core and a set of\\npowerful libraries that enable the developers to easily scale a variety of workloads,\\nincluding training, hyperparameter tuning, reinforcement learning, model serving,\\nand batch processing of unstructured data. Ray is one of the most popular open\\nsource projects and has been used by thousands of companies to implement every‐\\nthing from machine learning platforms to recommendation systems, fraud detection,\\nand training some of the largest models, including Open AI’s ChatGPT.\\nIn this book, Max Pumperla, Edward Oakes, and Richard Liaw have done an out‐\\nstanding job in providing a gentle and comprehensive introduction to Ray and its\\nlibraries using easy to follow examples. At the end of this book, you will master\\nthe key concepts and abstractions in Ray and be able to develop and quickly scale\\nend-to-end machine learning applications from your laptop to large on-premise\\nclusters or to the cloud.\\n— Ion Stoica\\nCofounder of Anyscale and Databricks, and\\nProfessor, UC Berkeley Berkeley, California\\nJanuary 2023\\nxiii\\n\\n\\n\\nPreface\\nDistributed computing is a fascinating topic. Looking back at the early days of\\ncomputing, one can’t help but be impressed by the fact that so many companies today\\ndistribute their workloads across clusters of computers. It’s impressive that we have\\nfigured out efficient ways to do so, but scaling out is also becoming more and more of\\na necessity. Individual computers keep getting faster, and yet our need for large-scale\\ncomputing keeps exceeding what single machines can do.\\nRecognizing that scaling is both a necessity and a challenge, Ray aims to make dis‐\\ntributed computing simple for developers. It makes distributed computing accessible\\nto nonexperts and makes it possible to scale your Python scripts across multiple\\nnodes fairly easily. Ray is good at scaling both data- and compute-heavy workloads ,\\nsuch as data preprocessing and model training—and it explicitly targets machine\\nlearning (ML) workloads with the need to scale. While it is possible today to scale\\nthese two types of workloads without Ray, you would likely have to use different APIs\\nand distributed systems for each. And managing several distributed systems can be\\nmessy and inefficient in many ways.\\nThe addition of the Ray AI Runtime (AIR) with the release of Ray 2.0 in August\\n2022 increased the support for complex ML workloads in Ray even further. AIR is\\na collection of libraries and tools that make it easy to build and deploy end-to-end\\nML applications in a single distributed system. With AIR, even the most complex\\nworkflows can usually be expressed as a single Python script . That means you can run\\nyour programs locally first, which can make a big difference in terms of debugging\\nand development speed.\\nData scientists benefit from Ray because they can rely on a growing ecosystem of Ray\\nML libraries and third-party integrations. Ray AIR helps you to quickly prototype\\nideas and go more easily from development to production. Unlike many other dis‐\\ntributed systems, Ray has native support for GPUs as well, which can be particularly\\nimportant to roles like ML engineers. To support data engineers, Ray also has tight\\nintegrations with tools like Kubernetes and can be deployed in multicloud setups.\\nxv\\n\\nAnd you can use it as a unified compute layer to provide scaling, fault tolerance,\\nscheduling, and orchestration of your workloads. In other words, it’s well worth\\ninvesting in learning Ray  for a variety of roles.\\nWho Should Read This Book\\nIt’s likely that you picked up this book because you’re interested in some aspects\\nof Ray. Maybe you’re a distributed systems engineer who wants to know how Ray’s\\nengine works. Y ou might also be a software developer interested in picking up a\\nnew technology. Or you could be a data engineer who wants to evaluate how Ray\\ncompares to similar tools. Y ou could also be a machine learning practitioner or data\\nscientist who needs to find ways to scale experiments.\\nNo matter your concrete role, the common denominator to get the most out of this\\nbook is to feel comfortable programming in Python. This book’s examples are written\\nin Python, and an intermediate knowledge of the language is a requirement. Explicit\\nis better than implicit, as you know full well as a Pythonista. So, let us be explicit by\\nsaying that knowing Python implies to me that you know how to use the command\\nline on your system, how to get help when stuck, and how to set up a programming\\nenvironment on your own.\\nIf you’ve never worked with distributed systems before, that’s OK. We cover all the\\nbasics you need to get started with that in the book. On top of that, you can run\\nmost code examples presented here on your laptop. Covering the basics means that\\nwe can’t go into too much detail about distributed systems. This book is ultimately\\nfocused on application developers using Ray, specifically for data science and ML.\\nFor the later chapters of this book, you’ll need some familiarity with ML, but we\\ndon’t expect you to have worked in the field. In particular, you should have a basic\\nunderstanding of the ML paradigm and how it differs from traditional programming.\\nY ou should also know the basics of using NumPy and Pandas. Also, you should at\\nleast feel comfortable reading  examples using the popular TensorFlow and PyTorch\\nlibraries. It’s enough to follow the flow of the code, on the API level, but you\\ndon’t need to know how to write your own models. We cover examples using both\\ndominant deep learning libraries (TensorFlow and PyTorch) to illustrate how you can\\nuse Ray for ML workloads, regardless of your preferred framework.\\nWe cover a lot of ground in advanced ML topics, but the main focus is on Ray as a\\ntechnology and how to use it. The ML examples we discuss might be new to you and\\ncould require a second reading, but you can still focus on Ray’s API and how to use it\\nin practice. Knowing the requirements, here’s what you might get out of this book:\\nxvi | Preface\\n\\n•If you are a data scientist, Ray will open up new ways for you to think about•\\nand build distributed ML applications. Y ou will know how to do hyperparameter\\nselection for your experiments at scale, gain practical knowledge on large-scale\\nmodel training, and get to know a state-of-the-art reinforcement learning library.\\n•If you are a data engineer, you will learn to use Ray Datasets for large-scale data•\\ningesting, how to improve your pipelines by leveraging tools such as Dask on\\nRay, and how to effectively deploy models at scale.\\n•If you are an engineer, you will understand how Ray works under the hood, how•\\nto run and scale Ray Clusters in the cloud, and how Ray can be used to build\\napplications that integrate with projects you know.\\nY ou can learn all of these topics regardless of your role, of course. Our hope is that by\\nthe end of this book, you will have learned to appreciate Ray for all its strengths.\\nGoals of This Book\\nThis book was written primarily for readers who are new to Ray and want to get the\\nmost out of it quickly. We chose the material in such a way that you will understand\\nthe core ideas behind Ray and learn to use its main building blocks. Having read\\nit, you will feel comfortable navigating more complex topics on your own that go\\nbeyond this introduction.\\nWe should also be clear about what this book is not. It’s not built to give you the most\\ninformation possible, like API references or definitive guides. It’s also not crafted to\\nhelp you tackle concrete tasks, like how-to guides or cookbooks do. This book is\\nfocused on learning and understanding Ray and giving you interesting examples to\\nstart with.\\nSoftware develops and deprecates quickly, but the fundamental concepts underlying\\nsoftware often remain stable even across major release cycles. We’re trying to strike\\na balance here between conveying ideas and providing you with concrete code exam‐\\nples. The ideas you find in this book will ideally remain useful even when the code\\neventually needs updating.\\nWhile Ray’s documentation keeps getting better, we do believe that books can offer\\nqualities that are difficult to match in a project’s documentation. Since you’re reading\\nthese lines, we realize we might be knocking down open doors with this statement.\\nBut some of the best tech books we know spark interest in a project and make you\\nwant to dig through terse API references that you’ d never have touched otherwise. We\\nhope this is one of those books.\\nPreface | xvii\\n\\nNavigating This Book\\nWe organized this book to guide you naturally from core concepts to more sophisti‐\\ncated topics of Ray. Many of the ideas explained come with example code that you\\ncan find in the book’s GitHub repo .\\nIn a nutshell, the first three chapters of the book teach the basics of Ray as a\\ndistributed Python framework with practical examples. Chapters 4 to 10 introduce\\nRay’s high-level libraries and show how to build applications with them. The last\\nchapter gives you a conclusive overview of Ray’s ecosystem and shows you where to\\ngo next. Here’s what you can expect from each chapter:\\nChapter 1, “ An Overview of Ray”\\nIntroduces you to Ray as a system composed of three layers: its core, its ML\\nlibraries, and its ecosystem. Y ou’ll run your first examples with Ray’s libraries in\\nthis chapter to give you a glimpse of what you can do with Ray.\\nChapter 2, “Getting Started with Ray Core”\\nWalks you through the foundations of the Ray project, namely, its core API. It\\nalso discusses how Ray tasks and actors naturally extend from Python functions\\nand classes. Y ou will also learn about Ray’s system components and how they\\nwork together.\\nChapter 3, “Building Your First Distributed Application”\\nGuides you through implementing a distributed reinforcement learning applica‐\\ntion with Ray Core. Y ou will implement this app from scratch and see Ray’s\\nflexibility in distributing your Python code in action.\\nChapter 4, “Reinforcement Learning with Ray RLlib”\\nGives you a quick introduction to reinforcement learning and shows how Ray\\nimplements important concepts in RLlib. After building some examples together,\\nwe’ll also dive into more advanced topics like curriculum learning or working\\nwith offline data.\\nChapter 5, “Hyperparameter Optimization with Ray Tune”\\nCovers why efficiently tuning hyperparameters is hard, how Ray Tune works\\nconceptually, and how you can use it in practice for your machine learning\\nprojects.\\nChapter 6, “Data Processing with Ray”\\nIntroduces you to the Ray Datasets abstraction of Ray and how it fits into the\\nlandscape of other data processing systems. Y ou will also learn how to work with\\nthird-party integrations such as Dask on Ray.\\nxviii | Preface\\n\\nChapter 7, “Distributed Training with Ray Train”\\nProvides you with the basics of distributed model training and shows you how to\\nuse Ray Train with ML frameworks like PyTorch. We also show you how to add\\ncustom preprocessors to your models, how to monitor training with callbacks,\\nand how to tune the hyperparameters of your models with Tune.\\nChapter 8, “Online Inference with Ray Serve”\\nTeaches you the basics of exposing your trained ML models as API endpoints\\nthat can be queried from anywhere. We discuss how Ray Serve addresses the\\nchallenges of online inference, cover its architecture, and show you how to use it\\nin practice.\\nChapter 9, “Ray Clusters”\\nDiscusses how you configure, launch, and scale Ray Clusters for your applica‐\\ntions. Y ou’ll learn about Ray’s Cluster launcher CLI and autoscaler, as well as\\nhow to set up clusters in the cloud. We’ll also show you how to deploy Ray on\\nKubernetes and with other cluster managers.\\nChapter 10, “Getting Started with the Ray AI Runtime”\\nIntroduces you to Ray AIR, a unified toolkit for your ML workloads that\\noffers many third-party integrations for model training or accessing custom data\\nsources.\\nChapter 11, “Ray’s Ecosystem and Beyond”\\nGives you an overview of the many interesting extensions and integrations that\\nRay has attracted over the years.\\nHow to Use the Code Examples\\nY ou can find all the code for this book in its GitHub repository . In the GitHub repo\\nyou’ll find a notebook  folder with notebooks for each chapter. We built the examples\\nin such a way that you can either type along as you read or follow the main text and\\nrun the code from GitHub at another time. The choice is yours.\\nFor the examples we assume that you have Python 3.7 or later installed. At the time\\nof this writing, support for Python 3.10 for Ray is experimental, so we can currently\\nonly recommend a Python version no later than 3.9. All code examples assume that\\nyou have Ray installed, and each chapter adds its own specific requirements. The\\nexamples have been tested on Ray version 2.2.0, and we recommend that you stick to\\nthis version for the whole book.\\nPreface | xix\\n\\nConventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\\nConstant width\\nUsed for program listings, as well as within paragraphs to refer to program\\nelements such as variable or function names, databases, data types, environment\\nvariables, statements, and keywords.\\nConstant width bold\\nShows commands or other text that should be typed literally by the user.\\n<Text in angle brackets>\\nShould be replaced with user-supplied values or by values determined by context.\\nThis element signifies a general note.\\nThis element indicates a warning or caution.\\nUsing Code Examples\\nSupplemental material (code examples, exercises, etc.) is available for download at\\nhttps://oreil.ly/learning_ray_repo .\\nIf you have a technical question or a problem using the code examples, please send\\nemail to bookquestions@oreilly.com .\\nThis book is here to help you get your job done. In general, if example code is\\noffered with this book, you may use it in your programs and documentation. Y ou\\ndo not need to contact us for permission unless you’re reproducing a significant\\nportion of the code. For example, writing a program that uses several chunks of code\\nfrom this book does not require permission. Selling or distributing examples from\\nO’Reilly books does require permission. Answering a question by citing this book\\nand quoting example code does not require permission. Incorporating a significant\\namount of example code from this book into your product’s documentation does\\nrequire permission.\\nxx | Preface\\n\\nWe appreciate, but generally do not require, attribution. An attribution usually\\nincludes the title, author, publisher, and ISBN. For example: “ Learning Ray  by Max\\nPumperla, Edward Oakes, and Richard Liaw (O’Reilly). Copyright 2023 Max Pum‐\\nperla and O’Reilly Media, Inc., 978-1-098-11722-1. ”\\nIf you feel your use of code examples falls outside fair use or the permission given\\nabove, feel free to contact us at permissions@oreilly.com .\\nO’Reilly Online Learning\\nFor more than 40 years, O’Reilly Media  has provided technol‐\\nogy and business training, knowledge, and insight to help\\ncompanies succeed.\\nOur unique network of experts and innovators share their knowledge and expertise\\nthrough books, articles, and our online learning platform. O’Reilly’s online learning\\nplatform gives you on-demand access to live training courses, in-depth learning\\npaths, interactive coding environments, and a vast collection of text and video from\\nO’Reilly and 200+ other publishers. For more information, visit http://oreilly.com .\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\n707-829-0515 (international or local)\\n707-829-0104 (fax)\\nWe have a web page for this book, where we list errata, examples, and any additional\\ninformation. Y ou can access this page at https://oreil.ly/learning-ray .\\nEmail bookquestions@oreilly.com  to comment or ask technical questions about this\\nbook.\\nFor news and information about our books and courses, visit https://oreilly.com .\\nFind us on LinkedIn: https://linkedin.com/company/oreilly-media .\\nFollow us on Twitter: https://twitter.com/oreillymedia .\\nWatch us on Y ouTube: https://youtube.com/oreillymedia .\\nPreface | xxi\\n\\nAcknowledgments\\nWe’ d like to acknowledge the whole team at O’Reilly for helping us make this book\\npossible. In particular, we’ d like to thank our tireless editor, Jeff Bleiel, for invaluable\\ninput and feedback. Many thanks to Jess Haberman for many fruitful discussions\\nand having an open mind in the early stages of the process. We’ d also like to thank\\nKatherine Tozer, Chelsea Foster, and Cassandra Furtado, among many others at\\nO’Reilly.\\nMany thanks to all the reviewers for their valuable feedback and suggestions: Mark\\nSaroufim, Kevin Ferguson, Adam Breindel, and Jorge Davila-Chacon. We’ d also like\\nto thank the many colleagues at Anyscale who helped us with the book in any\\ncapacity, including Sven Mika, Stephanie Wang, Antoni Baum, Christy Bergman,\\nDmitri Gekhtman, Zhe Zhang, and many others.\\nOn top of that, we’ d like to wholeheartedly thank the Ray contributors team and\\nthe community for their support and feedback, as well as many key stakeholders at\\nAnyscale supporting this project.\\nI (Max) would also like to thank the team at Pathmind for their support in the early\\nphases of the project, especially Chris Nicholson, who has been more helpful over\\nthe years than I could describe here. Special thanks go out to the Espresso Society\\nin Winterhude for helping me turn coffee into books, and an increasing array of\\nGPT-3-based tools for helping me finish half-sentences when the caffeine wore off.\\nI would also like to express my gratitude to my family for their encouragement and\\npatience. As always, none of this would have been possible without Anne, who always\\nsupports me when it counts—even if I take on one-too-many projects such as this\\none.\\nxxii | Preface\\n\\nCHAPTER 1\\nAn Overview of Ray\\nOne of the reasons we need efficient distributed computing is that we’re collecting\\never more data with great variety at increasing speeds. The storage systems, data\\nprocessing, and analytics engines that have emerged in the past decade are crucial\\nto the success of many companies. Interestingly, most “big data” technologies are\\nbuilt for and operated by (data) engineers who are in charge of data collection and\\nprocessing tasks. The rationale is to free up data scientists to do what they’re best at.\\nAs a data science practitioner, you might want to focus on training complex machine\\nlearning models, running efficient hyperparameter selection, building entirely new\\nand custom models or simulations, or serving your models to showcase them.\\nAt the same time, it might be inevitable  to scale these workloads to a compute\\ncluster. To do that, the distributed system of your choice needs to support all of these\\nfine-grained “big compute” tasks, potentially on specialized hardware. Ideally, it also\\nfits into the big data tool chain you’re using and is fast enough to meet your latency\\nrequirements. In other words, distributed computing has to be powerful and flexible\\nenough for complex data science workloads—and Ray can help you with that.\\nPython is likely the most popular language for data science today; it’s certainly the\\none we find the most useful for our daily work. Python is now more than 30 years\\nold, but it still has a growing and active community. The rich PyData ecosystem  is an\\nessential part of a data scientist’s toolbox. How can you make sure to scale out your\\nworkloads while still leveraging the tools you need? That’s a difficult problem, espe‐\\ncially since communities can’t be forced to just toss their toolbox or programming\\nlanguage. That means distributed computing tools for data science have to be built\\nfor their existing community.\\n1\\n\\n1By “Python-first” we mean that all higher-level libraries are written in Python and that the development of\\nnew features is driven by the needs of the Python community. Having said this, Ray has been designed to\\nsupport multiple language bindings and, for example, comes with a Java API. So, it’s not out of the question\\nthat Ray might support other languages that are important to the data science ecosystem.What Is Ray?\\nWhat we like about Ray is that it checks all these boxes. It’s a flexible distributed\\ncomputing framework built for the Python data science community.\\nRay is easy to get started and keeps simple things simple. Its core API is as lean as\\nit gets and helps you reason effectively about the distributed programs you want to\\nwrite. Y ou can efficiently parallelize Python programs on your laptop and run the\\ncode you tested locally on a cluster practically without any changes. Its high-level\\nlibraries are easy to configure and can seamlessly be used together. Some of them, like\\nRay’s reinforcement learning library, would likely have a bright future as standalone\\nprojects, distributed or not. While Ray’s core is built in C++, it’s been a Python-first\\nframework since day one,1 integrates with many important data science tools, and can\\ncount on a growing ecosystem.\\nDistributed Python is not new, and Ray is not the first framework in this space (nor\\nwill it be the last), but it is special in what it has to offer. Ray is particularly strong\\nwhen you combine several of its modules and have custom, machine learning–heavy\\nworkloads that would be difficult to implement otherwise. It makes distributed com‐\\nputing easy enough to run your complex workloads flexibly by leveraging the Python\\ntools you know and want to use. In other words, by learning Ray  you get to know\\nflexible  distributed Python for machine learning . And showing you how is what this\\nbook is all about.\\nIn this chapter you’ll get a first glimpse of what Ray can do for you. We will discuss\\nthe three layers that make up Ray: its core engine, high-level libraries, and ecosystem.\\nThroughout the chapter we’ll first show you code examples to give you a feel for\\nRay. Y ou can view this chapter as a quick preview of the book; we defer any in-depth\\ntreatment of Ray’s APIs and components to later chapters.\\nWhat Led to Ray?\\nProgramming distributed systems is hard. It requires specific knowledge and experi‐\\nence you might not have. Ideally, such systems get out of your way and provide\\nabstractions to let you focus on your job. But in practice, as Joel Spolsky notes ,\\n“all nontrivial abstractions, to some degree, are leaky, ” and getting clusters of comput‐\\ners to do what you want is undoubtedly difficult. Many software systems require\\nresources that far exceed what single servers can do. Even if one server were enough,\\nmodern systems need to be failsafe and provide features like high availability. That\\n2 | Chapter 1: An Overview of Ray\\n\\n2Moore’s law held for a long time, but there might be signs that it’s slowing down. Some even say it’s dead .\\nWe’re not here to argue these points. What’s important is not that our computers generally keep getting faster,\\nbut the relation to the amount of compute we need.\\n3There are many ways to speed up ML training, from basic to sophisticated. For instance, we’ll spend a\\nconsiderable amount of time elaborating on distributed data processing in Chapter 6  and distributed model\\ntraining in Chapter 7 .\\n4Anyscale , the company behind Ray, is building a managed Ray platform and offers hosted solutions for your\\nRay applications.means your applications might have to run on multiple machines, or even datacen‐\\nters, just to make sure they’re running reliably.\\nEven if you’re not too familiar with machine learning (ML) or artificial intelligence\\n(AI) more generally, you must have heard of recent breakthroughs in the field.  To\\nname just two, systems like Deepmind’s AlphaFold  for solving the protein folding\\nproblem and OpenAI’s Codex  for helping software developers with the tedium of\\ntheir jobs, have made the news lately. Y ou might also have heard that ML systems\\ngenerally require large amounts of data to be trained, and that ML models tend to get\\nlarger. OpenAI has shown exponential growth in compute needed to train AI models\\nin their paper “ AI and Compute” . The number of operations needed for AI systems in\\ntheir study is measured in petaflops (thousands of trillions of operations per second)\\nand has been doubling every 3.4 months  since 2012.\\nCompare this to Moore’s law,2 which states that the number of transistors in com‐\\nputers would double every two years. Even if you’re bullish on Moore’s law, you\\ncan see how there’s a clear need for distributed computing in ML. Y ou should also\\nunderstand that many tasks in ML can be naturally decomposed to run in parallel. So,\\nwhy not speed things up if you can?3\\nDistributed computing is generally perceived as hard. But why is that? Shouldn’t it\\nbe realistic to find good abstractions to run your code on clusters without having to\\nconstantly think about individual machines and how they interoperate? What if we\\nspecifically focused on AI workloads?\\nResearchers at RISELab  at UC Berkeley created Ray to address these questions. They\\nwere looking for efficient ways to speed up their workloads by distributing them.\\nThe workloads they had in mind were quite flexible in nature and didn’t fit into the\\nframeworks available at the time. RISELab also wanted to build a system that took\\ncare of how the work was distributed. With reasonable default behaviors in place,\\nresearchers should be able to focus on their work, regardless of the specifics of their\\ncompute cluster. And ideally they should have access to all their favorite tools in\\nPython. For this reason, Ray was built with an emphasis on high-performance and\\nheterogeneous workloads.4 To understand these points better, let’s have a closer look\\nat Ray’s design philosophy.\\nWhat Is Ray? | 3\\n\\nRay’s Design Principles\\nRay is built with several design principles in mind. Its API is designed for simplicity\\nand generality, and its compute model aims for flexibility. Its system architecture is\\ndesigned for performance and scalability. Let’s look at each of these in more detail.\\nSimplicity and abstraction\\nRay’s API not only banks on simplicity, it’s also intuitive to pick up (as you’ll see in\\nChapter 2 ). It doesn’t matter whether you want to use all the CPU cores on your\\nlaptop or leverage all the machines in your cluster. Y ou might have to change a\\nline of code or two, but the Ray code you use stays essentially the same.  And as\\nwith any good distributed system, Ray manages task distribution and coordination\\nunder the hood. That’s great, because you’re not bogged down by reasoning about the\\nmechanics of distributed computing. A good abstraction layer allows you to focus on\\nyour work, and we think Ray has done a great job of giving you one.\\nSince Ray’s API is so generally applicable and pythonic , it’s easy to integrate with other\\ntools. For instance, Ray actors can call into or be called by existing distributed Python\\nworkloads. In that sense, Ray makes for good “glue code” for distributed workloads,\\ntoo, as it’s performant and flexible enough to communicate between different systems\\nand frameworks.\\nFlexibility and heterogeneity\\nFor AI workloads, in particular when dealing with paradigms like reinforcement\\nlearning, you need a flexible programming model. Ray’s API is designed to make\\nit easy to write flexible and composable code. Simply put, if you can express your\\nworkload in Python, you can distribute it with Ray. Of course, you still need to\\nmake sure you have enough resources available and be mindful of what you want to\\ndistribute. But Ray doesn’t limit what you can do with it.\\nRay is also flexible when it comes to heterogeneity  of computations. For instance, let’s\\nsay you work on a complex simulation. Simulations can usually be decomposed into\\nseveral tasks or steps. Some of these steps might take hours to run, others just a few\\nmilliseconds, but they always need to be scheduled and executed quickly. Sometimes\\na single task in a simulation can take a long time, but other, smaller tasks should be\\nable to run in parallel without blocking it. Also, subsequent tasks may depend on\\nthe outcome of an upstream task, so you need a framework to allow for dynamic\\nexecution  that deals well with task dependencies. Ray gives you full flexibility when\\nrunning heterogeneous workflows like that.\\nY ou also need to ensure you are flexible in your resource usage, and Ray supports\\nheterogeneous hardware. For instance, some tasks might have to run on a GPU, while\\nothers run best on a couple of CPU cores. Ray provides you with that flexibility.\\n4 | Chapter 1: An Overview of Ray\\n\\n5This might sound drastic, but it’s not a joke. To name just one example, in March 2021 a French datacenter\\npowering millions of websites burned down completely . If your whole cluster burns down, we’re afraid Ray\\ncan’t help you.\\n6This is a Python book, so we’ll exclusively focus on Python, but you should know that Ray also has a Java API,\\nwhich is less mature than its Python equivalent at this point.Speed and scalability\\nAnother of Ray’s design principles is the speed at which Ray executes its tasks. It can\\nhandle millions of tasks per second, and you incur very low latencies with it. Ray is\\nbuilt to execute its tasks with just milliseconds of latency.\\nFor a distributed system to be fast, it also needs to scale well. Ray is efficient at\\ndistributing and scheduling your tasks across your compute cluster. And it does so in\\na fault-tolerant way, too. As you’ll learn in detail in Chapter 9 , Ray Clusters support\\nautoscaling  to support highly elastic workloads. Ray’s autoscaler tries to launch or\\nstop machines in your cluster to match the current demand. This helps both to\\nminimize costs and to ensure that your cluster has enough resources to run your\\nworkload.\\nIn distributed systems, it’s not a question of if, but when, things will go wrong. A\\nmachine might have an outage, abort a task, or simply go up in flames.5 In any case,\\nRay is built to recover quickly from failures, which contributes to its overall speed.\\nAs we haven’t talked about Ray’s architecture ( Chapter 2  will introduce you to it), we\\ncan’t tell you how these design principles are realized just yet. Let’s instead shift our\\nattention to what Ray can do for you in practice.\\nThree Layers: Core, Libraries, and Ecosystem\\nNow that you know why Ray was built and what its creators had in mind, let’s look at\\nthe three layers of Ray.  This presentation is not the only way to slice it, but it’s the way\\nthat makes most sense for this book:\\n•A low-level, distributed computing framework for Python with a concise core•\\nAPI and tooling for cluster deployment called Ray Core.6\\n•A set of high-level libraries built and maintained by the creators of Ray. This•\\nincludes the so-called Ray AIR to use these libraries with a unified API in\\ncommon machine learning workloads.\\n•A growing ecosystem of integrations and partnerships with other notable•\\nprojects that span many aspects of the first two layers.\\nThere’s a lot to unpack here, and we’ll look into each of these layers individually in the\\nremainder of this chapter.\\nWhat Is Ray? | 5\\n\\n7One of the reasons so many libraries are built on top of Ray Core is that it’s so lean and straightforward to\\nreason about. One of the goals of this book is to inspire you to write your own applications, or even libraries,\\nwith Ray.Y ou can imagine Ray’s core engine with its API at the center of things, on which\\neverything else builds. Ray’s data science libraries build on top of Ray Core and\\nprovide a domain-specific abstraction layer.7 In practice, many data scientists will use\\nthese libraries directly, while ML or platform engineers might rely heavily on building\\ntheir tools as extensions of the Ray Core API. Ray AIR can be seen as an umbrella\\nthat links Ray libraries and offers a consistent framework for dealing with common\\nAI workloads. And the growing number of third-party integrations for Ray is another\\ngreat entry point for experienced practitioners. Let’s look into each one of the layers\\none by one.\\nA Distributed Computing Framework\\nAt its core, Ray is a distributed computing framework. We’ll provide you with just\\nthe basic terminology here and talk about Ray’s architecture in depth in Chapter 2 . In\\nshort, Ray sets up and manages clusters of computers so that you can run distributed\\ntasks on them.  A Ray Cluster consists of nodes that are connected to each other via a\\nnetwork. Y ou program against the so-called driver , the program root, which lives on\\nthe head node . The driver can run jobs, a collection of tasks, that are run on the nodes\\nin the cluster. Specifically, the individual tasks of a job are run on worker  processes\\non worker nodes . Figure 1-1  illustrates the basic structure of a Ray Cluster. Note\\nthat we’re not concerned with communication between nodes just yet; this diagram\\nmerely shows the layout of a Ray Cluster.\\nFigure 1-1. The basic components of a Ray Cluster\\nWhat’s interesting is that a Ray Cluster can also be a local cluster , a cluster consisting\\nof just your own computer. In this case, there’s just one node, namely, the head node,\\nwhich has the driver process and some worker processes. The default number of\\nworker processes is the number of CPUs available on your machine.\\n6 | Chapter 1: An Overview of Ray\\n\\n8We generally introduce dependencies in this book only when we need them, which should make it easier to\\nfollow along. In contrast, the notebooks on GitHub  give you the option to install all dependencies up front so\\nthat you can focus on running the code instead.\\n9At the time of this writing, there’s no Python 3.10 support for Ray, so sticking to a version between 3.7 and 3.9\\nshould work best to follow this book.\\nWith that knowledge at hand, it’s time to get your hands dirty and run your first\\nlocal Ray Cluster. Installing Ray on any of the major operating systems should work\\nseamlessly using pip:\\npip install \"ray[rllib, serve, tune]==2.2.0\"\\nWith a simple pip install ray , you will install just the basics of Ray. Since we want\\nto explore some advanced features, we installed the “extras” rllib , serve , and tune ,\\nwhich we’ll discuss in a bit.8 Depending on your system configuration, you may not\\nneed the quotation marks in this installation command.\\nNext, go ahead and start a Python session. Y ou could, for instance, use the ipython\\ninterpreter, which is often suitable for following simple examples.  In your Python\\nsession you can now easily import and initialize Ray:\\nimport ray\\nray.init()\\nIf you don’t feel like typing in the commands yourself, you can also\\njump into the Jupyter notebook for this chapter  and run the code\\nthere. The choice is up to you, but in any case please remember to\\nuse Python version 3.7 or later.9\\nWith those two lines of code, you’ve started a Ray Cluster on your local machine.\\nThis cluster can utilize all the cores available on your computer as workers. Right now\\nyour Ray Cluster doesn’t do much, but that’s about to change.\\nThe init  function you use to start the cluster is one of the six fundamental API calls\\nthat you will learn about in depth in Chapter 2 . Overall, the Ray Core API  is very\\naccessible and easy to use. But since it is also a rather low-level interface, it takes time\\nto build interesting examples with it. Chapter 2  has an extensive first example to get\\nyou started with the Ray Core API, and in Chapter 3  you’ll see how to build a more\\ninteresting Ray application for reinforcement learning.\\nIn the preceding code you didn’t provide any arguments to the ray.init(...)  func‐\\ntion. If you wanted to run Ray on a “real” cluster, you’ d have to pass more arguments\\nto init . This init  call is often called the Ray Client , and it is used to interactively\\nA Distributed Computing Framework | 7\\n\\n10There are other means of interacting with Ray Clusters, such as the Ray Jobs CLI .\\n11We never liked the categorization of data science as an intersection of disciplines, like math, coding, and\\nbusiness. Ultimately, that doesn’t tell you what practitioners do.connect to an existing Ray Cluster.10 Y ou can read more about using the Ray Client to\\nconnect to your production clusters in the Ray documentation .\\nOf course, if you’ve ever worked with compute clusters, you know there are many\\npitfalls and intricacies. For instance, you can deploy Ray applications on clusters hos‐\\nted by cloud providers such as Amazon Web Services (AWS), Google Cloud Platform\\n(GCP), or Microsoft Azure—and each choice needs good tooling for deployment and\\nmaintenance. Y ou can also spin up a cluster on your own hardware or use tools such\\nas Kubernetes to deploy your Ray Clusters. In Chapter 9  (following chapters with\\nconcrete Ray applications), we’ll come back to the topic of scaling workloads with\\nRay Clusters.\\nBefore moving on to Ray’s higher-level libraries, let’s briefly summarize the two\\nfoundational components of Ray as a distributed computation framework:\\nRay Clusters\\nThis component is in charge of allocating resources, creating nodes, and ensuring\\nthey are healthy. A good way to get started with Ray Clusters is its dedicated\\nquick start guide .\\nRay Core\\nOnce your cluster is up and running, you use the Ray Core API to program\\nagainst it. Y ou can get started with Ray Core by following the official walk-\\nthrough  for this component.\\nA Suite of Data Science Libraries\\nMoving on to the second layer of Ray, in this section we’ll briefly introduce all the\\ndata science libraries that Ray comes with.  To do so, let’s first take a bird’s-eye view of\\nwhat it means to do data science. Once you understand this context, it’s much easier\\nto review Ray’s higher-level libraries and see how they can be useful to you.\\nRay AIR and the Data Science Workflow\\nThe somewhat elusive term “data science” (DS) has evolved quite a bit in recent\\nyears, and you can find many definitions of varying usefulness online.11 To us, it’s\\nthe practice of gaining insights and building real-world applications by leveraging data .\\nThat’s quite a broad definition of an inherently practical and applied field that centers\\naround building and understanding things. In that sense, describing practitioners of\\n8 | Chapter 1: An Overview of Ray\\n\\n12As a fun exercise, we recommend reading Paul Graham’s famous “Hackers and Painters” essay  on this topic\\nand replace “computer science” with “data science. ” What would hacking 2.0 be?\\n13If you want to understand more about the holistic view of the data science process when building ML\\napplications, Building Machine Learning Powered Applications  by Emmanuel Ameisen (O’Reilly) is entirely\\ndedicated to it.this field as “data scientists” is about as bad a misnomer as describing hackers as\\n“computer scientists. ”12\\nIn broad strokes, doing data science is an iterative process that entails requirements\\nengineering, data collection and processing, building models and evaluating them,\\nand deploying solutions. Machine learning is not necessarily part of this process but\\noften is. If ML is involved, you can further specify some steps:\\nData processing\\nTo train ML models, you need data in a format that your ML model understands.\\nThe process of transforming and selecting what data should be fed into your\\nmodel is often called feature engineering . This step can be messy. Y ou’ll benefit a\\nlot if you can rely on common tools to do the job.\\nModel training\\nIn ML you need to train your algorithms on data that got processed in the\\nprevious step. This includes selecting the right algorithm for the job, and it helps\\nif you can choose from a wide variety.\\nHyperparameter tuning\\nMachine learning models have parameters that are tuned in the model training\\nstep. Most ML models also have another set of parameters called hyperparameters\\nthat can be modified prior to training. These parameters can heavily influence\\nthe performance of your resulting ML model and need to be tuned properly.\\nThere are good tools to help automate that process.\\nModel serving\\nTrained models need to be deployed.  To serve a model means to make it available\\nto whomever needs access by whatever means necessary. In prototypes, you often\\nuse simple HTTP servers, but there are many specialized software packages for\\nML model serving.\\nThis list is by no means exhaustive, and there’s a lot more to be said about building\\nML applications.13 However, it is true that these four steps are crucial for the success\\nof a data science project using ML.\\nRay has dedicated libraries for each of the four ML-specific steps we just listed.\\nSpecifically, you can take care of your data processing needs with Ray Datasets ,\\nrun distributed model training with Ray Train , run your reinforcement learning\\nA Suite of Data Science Libraries | 9\\n\\n14In Chapter 6  we will introduce you to the fundamentals of what makes Ray Datasets work, including its use of\\nArrow. For now, we want to focus on its API and concrete usage patterns.workloads with Ray RLlib , tune your hyperparameters efficiently with Ray Tune , and\\nserve your models with Ray Serve . And the way Ray is built, all these libraries are\\ndistributed by design , a point we can’t stress enough.\\nWhat’s more is that all of these steps are part of a process and are rarely tackled in\\nisolation. Not only do you want all the libraries involved to seamlessly interoperate, it\\ncan also be a decisive advantage if you can work with a consistent API throughout the\\nwhole data science process. This is exactly what Ray AIR was built for: having a com‐\\nmon runtime and API for your experiments and the ability to scale your workloads\\nwhen you’re ready. Figure 1-2  shows a quick overview of all the components of AIR.\\nFigure 1-2. Ray AIR as an umbrella of all current data science libraries of Ray\\nWhile introducing the Ray AI Runtime API would be too much for this chapter (you\\ncan jump ahead to Chapter 10  for that), we’ll introduce you to all the building blocks\\nthat feed into it. Let’s go through each of Ray’s DS libraries one by one.\\nData Processing with Ray Datasets\\nThe first high-level library of Ray we’ll talk about is Ray Datasets. This library\\ncontains a data structure aptly called Dataset , a multitude of connectors for loading\\ndata from various formats and systems, an API for transforming such datasets, a\\nway to build data processing pipelines with them, and many integrations with other\\ndata processing frameworks. The Dataset  abstraction builds on the powerful Arrow\\nframework .14\\nTo use Ray Datasets, you need to install Arrow for Python, for instance by running\\npip install pyarrow . The following simple example creates a distributed Dataset\\non your local Ray Cluster from a Python data structure. Specifically, you’ll create a\\ndataset from a Python dictionary containing a string name  and an integer-valued data\\nfor 10,000 entries:\\nimport ray\\n10 | Chapter 1: An Overview of Ray\\n\\n15We’ll elaborate more on this in later chapters, specifically in Chapter 6 , but note that Ray Datasets is not\\nmeant as a general-purpose data processing library. Tools such as Spark have more mature and optimized\\nsupport for large-scale data processing.items = [{\"name\": str(i), \"data\": i} for i in range(10000)]\\nds = ray.data.from_items (items)   \\nds.show(5)  \\nCreating a Dataset  by using from_items  from the ray.data  module.\\nPrinting the first five items of the Dataset .\\nTo show  a Dataset  means to print some of its values. Y ou should see precisely five\\nelements on your command line, like this:\\n{\\'name\\': \\'0\\', \\'data\\': 0}\\n{\\'name\\': \\'1\\', \\'data\\': 1}\\n{\\'name\\': \\'2\\', \\'data\\': 2}\\n{\\'name\\': \\'3\\', \\'data\\': 3}\\n{\\'name\\': \\'4\\', \\'data\\': 4}\\nGreat, now you have some rows, but what can you do with that data? The Dataset\\nAPI bets heavily on functional programming, as this paradigm is well suited for data\\ntransformations.\\nEven though Python 3 made a point of hiding some of its functional program‐\\nming capabilities, you’re probably familiar with functionality such as map, filter ,\\nflat_map , and others. If not, it’s easy enough to pick up: map takes each element\\nof your dataset and transforms it into something else, in parallel;  filter  removes\\ndata points according to a Boolean filter function; and the slightly more elaborate\\nflat_map  first maps values similarly to map, but then it also “flattens” the result. For\\ninstance, if map produced a list of lists, flat_map  would flatten out the nested lists\\nand give you just a list. Equipped with these three functional API calls,15 let’s see how\\neasily you can transform your dataset ds:\\nsquares = ds.map(lambda x: x[\"data\"] ** 2)  \\nevens = squares.filter(lambda x: x % 2 == 0)  \\nevens.count()\\ncubes = evens.flat_map (lambda x: [x, x**3])  \\nsample = cubes.take(10)  \\nprint(sample)\\nWe map each row of ds to only keep the square value of its data  entry.\\nA Suite of Data Science Libraries | 11\\n\\nThen we filter  the squares  to keep only even numbers (a total of five thousand\\nelements).\\nWe then use flat_map  to augment the remaining values with their respective\\ncubes.\\nTo take  a total of 10 values means to leave Ray and return a Python list with\\nthese values that we can print.\\nThe drawback of Dataset  transformations is that each step gets executed synchro‐\\nnously. In this example that is a nonissue, but for complex tasks that, for example,\\nmix reading files and processing data, you would want an execution that can overlap\\nindividual tasks. DatasetPipeline  does exactly that. Let’s rewrite the previous exam‐\\nple into a pipeline:\\npipe = ds.window()  \\nresult = pipe\\\\\\n    .map(lambda x: x[\"data\"] ** 2)\\\\\\n    .filter(lambda x: x % 2 == 0)\\\\\\n    .flat_map (lambda x: [x, x**3])  \\nresult.show(10)\\nY ou can turn a Dataset  into a pipeline by calling .window()  on it.\\nPipeline steps can be chained to yield the same result as before.\\nThere’s a lot more to be said about Ray Datasets, especially its integration with nota‐\\nble data processing systems, but we’ll defer an in-depth discussion until Chapter 6 .\\nModel Training\\nMoving on to the next set of libraries, let’s look at the distributed training capabilities\\nof Ray. For that, you have access to two libraries. One is dedicated to reinforcement\\nlearning specifically; the other one has a different scope and is aimed primarily at\\nsupervised learning tasks.\\nReinforcement learning with Ray RLlib\\nLet’s start with Ray RLlib  for reinforcement learning (RL). This library is powered\\nby the modern ML frameworks TensorFlow and PyTorch, and you can choose which\\none to use. Both frameworks seem to converge more and more conceptually, so you\\ncan pick the one you like most without losing much in the process. Throughout the\\nbook we use both TensorFlow and PyTorch examples so you can get a feel for both\\nframeworks when using Ray.\\n12 | Chapter 1: An Overview of Ray\\n\\n16If you’re on a Mac, you’ll have to install tensorflow-macos . In general, if you encounter any issues installing\\nRay or its dependencies on your system, please refer to the installation guide .For this section, go ahead and install TensorFlow with pip install tensorflow\\nright now.16 To run the code example, you also need to install the gym library with pip\\ninstall \"gym==0.25.0\" .\\nOne of the easiest ways to run examples with RLlib is to use the command-\\nline tool rllib , which we already installed implicitly when we ran pip install\\n\"ray[rllib]\" . Once you run more complex examples in Chapter 4 , you will mostly\\nrely on its Python API, but for now we want to get a first taste of running RL\\nexperiments with RLlib.\\nWe’ll look at a fairly classic control problem of balancing a pole on a cart. Imagine\\nyou have a pole like the one in Figure 1-3 , fixed at a joint of a cart, and subject to\\ngravity. The cart is free to move along a frictionless track, and you can manipulate the\\ncart by giving it a push from the left or the right with a fixed force. If you do this well\\nenough, the pole will remain in an upright position. For each time step the pole didn’t\\nfall over, we get a reward of 1. Collecting a high reward is our goal, and the question\\nis whether we can teach a reinforcement learning algorithm to do this for us.\\nFigure 1-3. Controlling a pole attached to a cart by asserting force to the left or the right\\nSpecifically, we want to train a reinforcement learning agent that can carry out two\\nactions, namely, push to the left or to the right, observe what happens when interact‐\\ning with the environment in that way, and learn from the experience by maximizing\\nthe reward.\\nTo tackle this problem with Ray RLlib, we can use a so-called tuned  example, which\\nis a preconfigured algorithm that runs well for a given problem. Y ou can run a tuned\\nexample with a single command.  RLlib comes with many such examples, and you can\\nlist them all with rllib example list .\\nOne of the available examples is cartpole-ppo , a tuned example that uses the PPO\\nalgorithm to solve the cart–pole problem, specifically, the CartPole-v1  environment\\nfrom OpenAI Gym. Y ou can take a look at the configuration of this example by\\ntyping rllib example get cartpole-ppo , which will first download the example\\nA Suite of Data Science Libraries | 13\\n\\nfile from GitHub and then print its configuration. This configuration is encoded in\\nYAML file format and reads as follows:\\ncartpole-ppo :\\n    env: CartPole-v1   \\n    run: PPO  \\n    stop:\\n        episode_reward_mean : 150  \\n        timesteps_total : 100000\\n    config: \\n        framework : tf\\n        gamma: 0.99\\n        lr: 0.0003\\n        num_workers : 1\\n        observation_filter : MeanStdFilter\\n        num_sgd_iter : 6\\n        vf_loss_coeff : 0.01\\n        model:\\n            fcnet_hiddens : [32]\\n            fcnet_activation : linear\\n            vf_share_layers : true\\n        enable_connectors : True\\nThe CartPole-v1  environment simulates the problem we just described.\\nUse a powerful RL algorithm called Proximal Policy Optimization, or PPO.\\nOnce we reach a reward of 150, stop the experiment.\\nPPO needs some RL-specific configuration to make it work for this problem.\\nThe details of this configuration file don’t matter much at this point, so don’t get dis‐\\ntracted by them. The important part is that you specify the Cartpole-v1  environment\\nand sufficient RL-specific configuration to ensure the training procedure works. Run‐\\nning this configuration doesn’t require any special hardware and finishes in a matter\\nof minutes. To train this example, you’ll have to install the PyGame dependency with\\npip install pygame  and then simply run:\\nrllib example run cartpole-ppo\\nIf you run this, RLlib creates a named experiment and logs important metrics such\\nas the reward  or the episode_reward_mean  for you. In the output of the training\\nrun, you should also see information about the machine ( loc, meaning hostname and\\nport), as well as the status of your training runs. If your run is TERMINATED  but you’ve\\nnever seen a successfully RUNNING  experiment in the log, something must have gone\\nwrong. Here’s a sample snippet of a training run:\\n14 | Chapter 1: An Overview of Ray\\n\\n+-----------------------------+----------+----------------+\\n| Trial name                  | status   | loc            |\\n|-----------------------------+----------+----------------|\\n| PPO_CartPole-v0_9931e_00000 | RUNNING  | 127.0.0.1:8683 |\\n+-----------------------------+----------+----------------+\\nWhen the training run finishes and things went well, you should see the following\\noutput:\\nYour training finished.\\nBest available checkpoint for each trial:\\n  <checkpoint-path>/checkpoint_<number>\\nY ou can now evaluate your trained algorithm from any checkpoint, for example, by\\nrunning:\\n╭───────────────────────────────────────────────────────────────────────── ╮\\n│   rllib evaluate <checkpoint-path>/checkpoint_<number> --algo PPO       │\\n╰───────────────────────────────────────────────────────────────────────── ╯\\nY our local Ray checkpoint folder is ~/ray-results  by default. For the training con‐\\nfiguration we used, your <checkpoint-path>  should be of the form ~/ray_results/\\ncartpole-ppo/PPO_CartPole-v1_<experiment_id> . During the training procedure,\\nyour intermediate and final model checkpoints get generated into this folder.\\nTo evaluate the performance of your trained RL algorithm, you can now evaluate it\\nfrom checkpoint  by copying the command the previous example training run printed:\\nrllib evaluate  <checkpoint-path>/checkpoint_<number>  --algo PPO\\nRunning this command will print evaluation results, namely, the rewards achieved by\\nyour trained RL algorithm on the CartPole-v1  environment.\\nThere’s much more that you can do with RLlib, and we’ll cover more of it in Chap‐\\nter 4 . The point of this example was to show you how easily you can get started with\\nRLlib and the rllib  command-line tool, just by leveraging the example  and evaluate\\ncommands.\\nDistributed training with Ray Train\\nRay RLlib is dedicated to reinforcement learning, but what do you do if you need to\\ntrain models for other types of machine learning, like supervised learning? Y ou can\\nuse another Ray library for distributed training in this case: Ray Train . At this point,\\nwe don’t have enough knowledge of frameworks such as TensorFlow to give you a\\nconcise and informative example for Ray Train. If you’re interested in distributed\\ntraining, you can jump ahead to Chapter 6 .\\nA Suite of Data Science Libraries | 15\\n\\nHyperparameter Tuning\\nNaming things is hard, but Ray Tune , which you can use to tune all sorts of parame‐\\nters, hits the spot. It was built specifically to find good hyperparameters for machine\\nlearning models. The typical setup is as follows:\\n•Y ou want to run an extremely computationally expensive training function. In•\\nML, it’s not uncommon to run training procedures that take days, if not weeks,\\nbut let’s say you’re dealing with just a couple of minutes.\\n•As a result of training, you compute a so-called objective function. Usually•\\nyou want to either maximize your gains or minimize your losses in terms of\\nperformance of your experiment.\\n•The tricky bit is that your training function might depend on certain parameters,•\\ncalled hyperparameters, that influence the value of your objective function.\\n•Y ou may have a hunch what individual hyperparameters should be, but tuning•\\nthem all can be difficult. Even if you can restrict these parameters to a sensible\\nrange, it’s usually prohibitive to test a wide range of combinations. Y our training\\nfunction is simply too expensive.\\nWhat can you do to efficiently sample hyperparameters and get “good enough” results\\non your objective? The field concerned with solving this problem is called hyperpara‐\\nmeter optimization  (HPO), and Ray Tune has an enormous suite of algorithms for\\ntackling it. Let’s look an example of Ray Tune used for the situation we just explained.\\nThe focus is yet again on Ray and its API, not on a specific ML task (which we simply\\nsimulate for now):\\nfrom ray import tune\\nimport math\\nimport time\\ndef training_function (config):  \\n    x, y = config[\"x\"], config[\"y\"]\\n    time.sleep(10)\\n    score = objective (x, y)\\n    tune.report(score=score)  \\ndef objective (x, y):\\n    return math.sqrt((x**2 + y**2)/2)  \\nresult = tune.run(  \\n    training_function ,\\n    config={\\n        \"x\": tune.grid_search ([-1, -.5, 0, .5, 1]),  \\n16 | Chapter 1: An Overview of Ray\\n\\n        \"y\": tune.grid_search ([-1, -.5, 0, .5, 1])\\n    })\\nprint(result.get_best_config (metric=\"score\", mode=\"min\"))\\nSimulate an expensive training function that depends on two hyperparameters, x\\nand y, read from a config .\\nAfter sleeping for 10 seconds to simulate training and computing the objective,\\nthe score is reported to tune .\\nThe objective computes the mean of the squares of x and y and returns the\\nsquare root of this term. This type of objective is fairly common in ML.\\nUse tune.run  to initialize hyperparameter optimization on our\\ntraining_function .\\nA key part is to provide a parameter space for x and y for tune  to search over.\\nNotice how the output of this run is structurally similar to what you saw in the\\nRLlib example. That’s no coincidence, as RLlib (like many other Ray libraries) uses\\nRay Tune under the hood. If you look closely, you will see PENDING  runs that wait\\nfor execution, as well as RUNNING  and TERMINATED  runs. Tune takes care of selecting,\\nscheduling, and executing your training runs automatically.\\nSpecifically, this Tune example finds the best possible choices of parameters x and y\\nfor a training_function  with a given objective  we want to minimize. Even though\\nthe objective function might look a little intimidating at first, since we compute the\\nsum of squares of x and y, all values will be non-negative. That means the smallest\\nvalue is obtained at x=0 and y=0, which evaluates the objective function to 0.\\nWe do a so-called grid search  over all possible parameter combinations. As we explic‐\\nitly pass in 5 possible values for both x and y, that’s a total of 25 combinations that\\nget fed into the training function. Since we instruct training_function  to sleep\\nfor 10 seconds, testing all combinations of hyperparameters sequentially would take\\nmore than 4 minutes total. Since Ray is smart about parallelizing this workload, this\\nwhole experiment took only about 35 seconds for us, but it might take much longer,\\ndepending on where you run it.\\nNow, imagine each training run would have taken several hours, and we’ d have 20\\ninstead of 2 hyperparameters. That makes grid search infeasible, especially if you\\ndon’t have educated guesses on the parameter range. In such situations you’ll have to\\nuse more elaborate HPO methods from Ray Tune, as discussed in Chapter 5 .\\nA Suite of Data Science Libraries | 17\\n\\n17Depending on the operating system you’re using, you may need to install the Rust compiler first to make this\\nwork. For instance, on a Mac, you can install it with brew install rust .Model Serving\\nThe last of Ray’s high-level libraries we’ll discuss specializes in model serving and is\\nsimply called Ray Serve . To see an example of it in action, you need a trained ML\\nmodel to serve. Luckily, nowadays, you can find many interesting models on the\\ninternet that have already been trained for you. For instance, Hugging Face has a\\nvariety of models available for you to download directly in Python. The model we’ll\\nuse is a language model called GPT-2  that takes text as input and produces text to\\ncontinue or complete the input. For example, you can prompt a question and GPT-2\\nwill try to complete it.\\nServing such a model is a good way to make it accessible. Y ou may not know how\\nto load and run a TensorFlow model on your computer, but you do know how to\\nask a question in plain English. Model serving hides the implementation details of\\na solution and lets users focus on providing inputs and understanding outputs of a\\nmodel.\\nTo proceed, make sure to run pip install transformers  to install the Hugging Face\\nlibrary that has the model we want to use.17 With that we can now import and start\\nan instance of Ray’s serve  library, load and deploy a GPT-2 model, and ask it for the\\nmeaning of life, like so:\\nfrom ray import serve\\nfrom transformers  import pipeline\\nimport requests\\nserve.start()  \\n@serve.deployment   \\ndef model(request):\\n    language_model  = pipeline (\"text-generation\" , model=\"gpt2\")  \\n    query = request.query_params [\"query\"]\\n    return language_model (query, max_length =100)  \\nmodel.deploy()  \\nquery = \"What\\'s the meaning of life?\"\\nresponse  = requests .get(f\"http://localhost:8000/model?query= {query}\")  \\nprint(response .text)\\n18 | Chapter 1: An Overview of Ray\\n\\nStart serve  locally.\\nThe @serve.deployment  decorator turns a function with a request  parameter\\ninto a serve  deployment.\\nLoading language_model  inside the model  function for every request is ineffi‐\\ncient, but it’s the quickest way to show you a deployment.\\nAsk the model to give us at most 100 characters to continue our query.\\nFormally deploy the model so that it can start receiving requests over HTTP .\\nUse the indispensable requests  library to get a response for any question you\\nmight have.\\nIn Chapter 9  you will learn how to properly deploy models in various scenarios,\\nbut for now we encourage you to play around with this example and test different\\nqueries. Running the last two lines of code repeatedly will give you different answers\\npractically every time. Here’s a darkly poetic gem, raising more questions, from one\\nquery that we’ve slightly censored for underaged readers:\\n[{\\n    \"generated_text\": \"What\\'s the meaning of life?\\\\n\\\\n\\n     Is there one way or another of living?\\\\n\\\\n\\n     How does it feel to be trapped in a relationship?\\\\n\\\\n\\n     How can it be changed before it\\'s too late?\\n     What did we call it in our time?\\\\n\\\\n\\n     Where do we fit within this world and what are we going to live for?\\\\n\\\\n\\n     My life as a person has been shaped by the love I\\'ve received from others.\"\\n}]\\nThis concludes our whirlwind tour of Ray’s data science libraries, the second of Ray’s\\nlayers. Ultimately, all high-level Ray libraries presented in this chapter are extensions\\nof the Ray Core API. Ray makes it relatively easy to build new extensions, and there\\nare a few more that we can’t discuss in full in this book. For instance, there is the\\nrelatively recent addition of Ray Workflows , which allows you to define and run\\nlong-running applications with Ray.\\nBefore we wrap up this chapter, let’s have a very brief look at the third layer, the\\ngrowing ecosystem around Ray.\\nA Suite of Data Science Libraries | 19\\n\\n18Spark was created by another lab in Berkeley, AMPLab. The internet is full of blog posts claiming that Ray\\nshould therefore be seen as a replacement of Spark. It’s better to think of them as tools with different strengths\\nthat are both likely here to stay.\\n19Before the deep learning framework Keras  became an official part of TensorFlow, it started out as a conve‐\\nnient API specification for various lower-level frameworks such as Theano or CNTK. In that sense, Ray RLlib\\nhas the chance to become “Keras for RL, ” and Ray Tune might just be “Keras for HPO. ” The missing piece for\\nmore adoption might just be a more elegant API for both.A Growing Ecosystem\\nRay’s high-level libraries are powerful and deserve a much deeper treatment through‐\\nout the book. While their usefulness for the data science experimentation lifecycle\\nis undeniable, we also don’t want to give the impression that Ray is all you need\\nfrom now on. No surprise, the best and most successful frameworks are the ones\\nthat integrate well with existing solutions and ideas. It’s better to focus on your core\\nstrengths and leverage other tools for what’s missing in your solution, and Ray does\\nthis quite well.\\nThroughout the book, and in Chapter 11  in particular, we will discuss many useful\\nthird-party libraries built on top of Ray. The Ray ecosystem also has a lot of integra‐\\ntions with existing tools. To give you an example of that, recall that Ray Datasets is\\nRay’s data loading and compute library.  If you happen to have an existing project that\\nalready uses data processing engines like Spark or Dask,18 you can use those tools\\ntogether with Ray. Specifically, you can run the entire Dask ecosystem on top of a Ray\\nCluster using the Dask-on-Ray scheduler, or you can use the Spark on Ray project  to\\nintegrate your Spark workloads with Ray. Likewise, the Modin project  is a distributed\\ndrop-in replacement for Pandas DataFrames that uses Ray (or Dask) as a distributed\\nexecution engine (“Pandas on Ray”).\\nThe common theme here is that Ray doesn’t try to replace all these tools, but rather\\nintegrates with them while still giving you access to its native Ray Datasets library.\\nWe’ll go into much more detail about the relationship of Ray with other tools in the\\nbroader ecosystem in Chapter 11 .\\nOne important aspect of many Ray libraries is that they seamlessly integrate common\\ntools as backends . Ray often creates common interfaces, instead of trying to create\\nnew standards.19 These interfaces allow you to run tasks in a distributed fashion,\\na property most of the respective backends don’t have, or not to the same extent.\\nFor instance, Ray RLlib and Train are backed by the full power of TensorFlow and\\nPyTorch.  And Ray Tune supports algorithms from practically every notable HPO\\ntool available, including Hyperopt, Optuna, Nevergrad, Ax, SigOpt, and many others.\\nNone of these tools is distributed by default, but Tune unifies them in a common\\ninterface for distributed workloads .\\n20 | Chapter 1: An Overview of Ray\\n\\nSummary\\nFigure 1-4  gives you an overview of the three layers of Ray as we laid them out. Ray’s\\ncore distributed execution engine sits at the center of the framework. The Ray Core\\nAPI is a versatile library for distributed computing, and Ray Clusters allow you to\\ndeploy your workloads in a variety of ways.\\nFor practical data science workflows you can use Ray Datasets for data processing,\\nRay RLlib for reinforcement learning, Ray Train for distributed model training, Ray\\nTune for hyperparameter tuning, and Ray Serve for model serving. Y ou’ve seen\\nexamples for each of these libraries and have an idea of what their APIs entail. Ray\\nAIR provides a unified API for all other Ray ML libraries and was built with the\\nneeds of data scientists in mind.\\nOn top of that, Ray’s ecosystem has many extensions, integrations, and backends that\\nwe’ll look more into later. Maybe you can already spot a few tools you know and like\\nin Figure 1-4 ?\\nFigure 1-4. Ray in three layers\\nThe Ray Core API sits at the center of Figure 1-4 , surrounded by the libraries RLlib,\\nRay Tune, Ray Train, Ray Serve, Ray Datasets, and the many third-party integrations\\nthat are too many to list here.\\nSummary | 21\\n\\n\\n\\n1Note that Ray comes with a drop-in replacement for multiprocessing  that you might find useful for certain\\nworkloads.\\n2This represents a trade-off in terms of generality versus specialization. By also providing specialized yet\\ninteroperable libraries on top of the Core API, Ray provides tooling at various levels of abstraction.CHAPTER 2\\nGetting Started with Ray Core\\nFor a book on distributed Python, it’s not without a certain irony that Python on\\nits own is largely ineffective for distributed computing. Its interpreter is effectively\\nsingle threaded. For instance, this makes it difficult to leverage multiple CPUs on the\\nsame machine, let alone a whole cluster of machines, using plain Python. That means\\nyou need extra tooling, and luckily the Python ecosystem has some options for you.\\nLibraries like multiprocessing  can help you distribute work on a single machine,1\\nbut not beyond.\\nSeen as a Python library, the Ray Core API is powerful enough to make general dis‐\\ntributed programming more accessible to the Python community as a whole. By way\\nof analogy, some companies get by with deploying pretrained ML models for their\\nuse cases, but that strategy is not always effective. It’s often inevitable to need to train\\ncustom models to be successful. In the same way, your distributed workloads might\\njust fit into the (potentially limiting) programming model of existing frameworks, but\\nRay Core can unlock the full spectrum of building distributed applications, due to its\\ngenerality.2 As it is so fundamental, we dedicate this whole chapter to the basics of\\nRay Core and spend all of Chapter 3  on building an interesting application with the\\nCore API. This way you’re equipped with practical knowledge about Ray Core and\\ncan use it in later chapters and your own projects.\\nIn this chapter you’ll understand how Ray Core handles distributed computing by\\nspinning up a local cluster, and you’ll learn how to use Ray’s lean and powerful API to\\nparallelize some interesting computations. For instance, you’ll build an example that\\n23\\n\\n3The dashboard is being redesigned as we write these lines. As much as we’ d like to show you screenshots of it\\nand walk you through it, you’ll have to discover it for yourself  for now.\\nruns a data-parallel task efficiently and asynchronously on Ray, in a convenient way\\nthat’s not easily replicable with other tooling. We discuss how tasks  and actors  work as\\ndistributed versions of functions and classes in Python. Y ou’ll also learn how to put\\nobjects  in Ray’s object store and how to retrieve them.  We give you concrete examples\\nfor these three fundamental concepts (tasks, actors, and objects), using just six basic\\nAPI calls of the Ray Core API. Lastly, we’ll discuss the system components underlying\\nRay and what its architecture looks like. In other words, in this chapter we’ll give you\\na look under the hood of Ray’s engine.\\nAn Introduction to Ray Core\\nThe bulk of this chapter is an extended Ray Core example that we’ll build together.\\nMany of Ray’s concepts can be explained with a good example, so that’s exactly what\\nwe’ll do.\\nAs before, you can follow this example by typing the code yourself\\n(which is highly recommended) or by following the notebook for\\nthis chapter . In any case, make sure you have Ray installed, for\\ninstance with pip install ray .\\nIn Chapter 1  we showed you how start a local cluster simply by calling import ray\\nand then initializing it with ray.init() . After running this code you will see output\\nof the following form. We omit a lot of information in this example output, as that\\nwould require you to understand more of Ray’s internals first:\\n... INFO services.py:1263 -- View the Ray dashboard at http://127.0.0.1:8265\\n{\\'node_ip_address\\': \\'192.168.1.41\\',\\n ...\\n \\'node_id\\': \\'...\\'}\\nThis output indicates that your Ray Cluster is up and running. As you can see from\\nthe first line of the output, Ray comes with its own, prepackaged dashboard.3 Y ou can\\ncheck it out at http://127.0.0.1:8265 , unless your output shows a different port. Y ou\\ncan take your time if you want to explore the dashboard. For instance, you should\\nsee all your CPU cores listed and the total utilization of your (trivial) Ray application.\\nTo see the resource utilization of your Ray Cluster in Python, you can simply call\\nray.cluster_resources() . The output should look something like this:\\n{\\'CPU\\': 12.0,\\n \\'memory\\': 14203886388.0,\\n24 | Chapter 2: Getting Started with Ray Core\\n\\n4In case you fall into these categories, it might be comforting to hear that many data scientists rarely use Ray\\nCore directly. Instead, they work directly with Ray’s higher-level libraries like Datasets, Train, or Tune. \\'node:127.0.0.1\\': 1.0,\\n \\'object_store_memory\\': 2147483648.0}\\nY ou’ll need a running Ray Cluster to run the examples in this chapter, so make sure\\nyou’ve started one before continuing. The goal of this section is to give you a quick\\nintroduction to the Ray Core API, which we’ll simply refer to as the Ray API from\\nnow on.\\nFor Python programmers, the great thing about the Ray API is that it hits so close\\nto home. It uses familiar concepts such as decorators, functions, and classes to\\nprovide you with a fast learning experience. The Ray API aims to provide a universal\\nprogramming interface for distributed computing. That’s certainly no easy feat, but\\nwe think Ray succeeds in this respect, as it provides you with good abstractions that\\nare intuitive to learn and use. Ray’s engine does all the heavy lifting for you in the\\nbackground. This design philosophy is what enables Ray to be used with existing\\nPython libraries and systems.\\nNote that the reason we start out with Ray Core in this book is that we believe it\\nhas massive potential to make distributed computing more accessible. In essence, this\\nchapter is all about getting a peek behind the curtains of what makes Ray work so\\nwell and how you can pick up its fundamentals. If you’re a less experienced Python\\nprogrammer or just want to focus on higher-level tasks, Ray Core might take some\\ngetting used to.4 Having said that, we emphatically recommend learning the Ray Core\\nAPI, as it’s a great way to get into distributed computing with Python.\\nA First Example Using the Ray API\\nTo give you an example, take the following function that retrieves and processes data\\nfrom a database. Our sample database is a plain Python list containing the words of\\nthe title of this book.  We act as if retrieving an individual item  from this database and\\nfurther processing it is expensive by letting Python sleep :\\nimport time\\ndatabase  = [  \\n    \"Learning\" , \"Ray\",\\n    \"Flexible\" , \"Distributed\" , \"Python\" , \"for\", \"Machine\" , \"Learning\"\\n]\\ndef retrieve (item):\\n    time.sleep(item / 10.)  \\n    return item, database [item]\\nAn Introduction to Ray Core | 25\\n\\nA sample database containing string data with the title of this book.\\nEmulate a data-crunching operation that takes a long time.\\nOur database has eight items in total. If we were to retrieve all items sequentially, how\\nlong would that take? For the item with index 5, we wait for half a second (5 / 10) and\\nso on. In total, we can expect a runtime of around (0 + 1 + 2 + 3 + 4 + 5 + 6 + 7) / 10\\n= 2.8 seconds. Let’s see if that’s what we actually get:\\ndef print_runtime (input_data , start_time ):\\n    print(f\\'Runtime: {time.time() - start_time :.2f} seconds, data:\\' )\\n    print(*input_data , sep=\"\\\\n\")\\nstart = time.time()\\ndata = [retrieve (item) for item in range(8)]  \\nprint_runtime (data, start)  \\nUses a list comprehension to retrieve all eight items.\\nUnpacks the data to print each item on its own line.\\nIf you run this code, you should see the following output:\\nRuntime: 2.82 seconds, data:\\n(0, \\'Learning\\')\\n(1, \\'Ray\\')\\n(2, \\'Flexible\\')\\n(3, \\'Distributed\\')\\n(4, \\'Python\\')\\n(5, \\'for\\')\\n(6, \\'Machine\\')\\n(7, \\'Learning\\')\\nThere’s a little overhead that brings the total runtime to 2.82 seconds. On your\\nend this might be slightly less, or much more, depending on your computer. The\\nimportant takeaway is that our naive Python implementation is not able to run this\\nfunction in parallel.\\nThis may not come as a surprise to you, but you could at least have suspected that\\nPython list comprehensions are more efficient in that regard. The runtime we got is\\npretty much the worst-case scenario, namely, the 2.8 seconds we calculated prior to\\nrunning the code. If you think about it, it might even be a bit frustrating to see that a\\nprogram that essentially sleeps most of its runtime is that slow overall. Ultimately you\\ncan blame the Global Interpreter Lock  (GIL) for that, but it gets enough of the blame\\nalready.\\n26 | Chapter 2: Getting Started with Ray Core\\n\\nPython’s Global Interpreter Lock\\nThe GIL is undoubtedly one of the most infamous features of the Python language.\\nIn a nutshell, it’s a lock that makes sure only one thread on your computer can ever\\nexecute your Python code at a time. If you use multithreading, the threads need to\\ntake turns controlling the Python interpreter.\\nThe GIL has been implemented for good reasons. For one, it makes memory\\nmanagement that much easier in Python. Another key advantage is that it makes\\nsingle-threaded programs quite fast. Programs that primarily use lots of system input\\nand output (we say they are I/O-bound), like reading files or databases, benefit\\nas well. One of the major downsides is that CPU-bound programs are essentially\\nsingle-threaded. In fact, CPU-bound tasks might even run faster  when not using\\nmultithreading, as the latter incurs write-lock overheads on top of the GIL.\\nGiven all that, the GIL might somewhat paradoxically be one of the reasons for\\nPython’s popularity, if you believe Larry Hastings . Interestingly, Hastings also led\\n(unsuccessful) efforts to remove it in a project called GILectomy , which is exactly the\\nkind of complicated surgery that it sounds like. The jury is still out, but Sam Gross\\nmight just have found a way to remove the GIL in his nogil  branch of Python 3.9.\\nFor now, if you absolutely have to work around the GIL, consider using an implemen‐\\ntation different from CPython. CPython is Python’s standard implementation, and\\nif you don’t know that you’re using it, you’re definitely using it. Implementations\\nlike Jython, IronPython, or PyPy don’t have a GIL, but they come with their own\\ndrawbacks.\\nFunctions and remote Ray tasks\\nIt’s reasonable to assume that such a task can benefit from parallelization. Perfectly\\ndistributed, the runtime should not take much longer than the longest subtask,\\nnamely, 7/ 10 = 0.7 seconds. So, let’s see how you can extend this example to run on\\nRay. To do so, start by using the @ray.remote  decorator as follows:\\n@ray.remote  \\ndef retrieve_task (item):\\n    return retrieve (item)  \\nMake any Python function a Ray task with just this decorator.\\nAll else remains unchanged. retrieve_task  just passes through to retrieve .\\nIn this way, the function retrieve_task  becomes a so-called Ray task. In essence, a\\nRay task is a function that gets executed on a different process than it was called from,\\npotentially on a different machine.\\nAn Introduction to Ray Core | 27\\n\\n5Strictly speaking, this first example is a bit of an anti-pattern , as you should not normally share mutable state\\nacross Ray tasks through global variables. Having said that, for this toy data example you shouldn’t overthink\\nthis part. Rest assured that we’ll show you a better way of doing things in the next section.\\n6This book is geared to data science practitioners, so we won’t discuss the conceptual details of Ray’s architec‐\\nture here. If you’re curious and want to learn more about how Ray tasks are executed, check out the Ray\\narchitecture whitepaper .That’s an extremely convenient design choice, as you can focus on your Python code\\nfirst and don’t have to completely change your mindset or programming paradigm\\nto use Ray. Note that in practice you would have simply added the @ray.remote\\ndecorator to your original retrieve  function (after all, that’s the intended use of\\ndecorators), but to keep things as clear as possible, we didn’t want to touch previous\\ncode.\\nEasy enough, so what do you have to change in the code that retrieves the database\\nentries and measures performance? It turns out, not much. Example 2-1  shows how\\nyou would do that.5\\nExample 2-1. Measuring performance of your Ray task\\nstart = time.time()\\nobject_references  = [  \\n    retrieve_task .remote(item) for item in range(8)\\n]\\ndata = ray.get(object_references )  \\nprint_runtime (data, start)\\nTo run retrieve_task  on your local Ray Cluster, you use .remote()  and pass in\\nyour items as before. Each task returns an object.\\nTo get back actual data, and not just Ray object references, you use ray.get .\\nDid you spot the differences? Y ou have to execute your Ray task remotely using\\na .remote()  call.6 When Ray executes tasks remotely, even on your local cluster, it\\ndoes so asynchronously . The list items in object_references  in the last code snippet\\ndo not contain the results directly. In fact, if you check the Python type of the first\\nitem with type(object_references[0]) , you’ll see that it’s in fact an ObjectRef .\\nThese object references correspond to futures , which you need to ask the result of.\\nThis is what the call to ray.get(...)  is for. Whenever you call remote  on a Ray\\ntask, it will immediately return one or more object references. Y ou should consider\\nRay tasks the primary method of creating objects. In the next section we’ll show you\\nan example that chains multiple tasks together and lets Ray take care of passing and\\nresolving the objects between them.\\n28 | Chapter 2: Getting Started with Ray Core\\n\\n7This example has been adapted from Dean Wampler’s fantastic report “What Is Ray?” .We still want to work more on this example,7 but let’s take a step back here and\\nrecap what we did so far. Y ou started with a Python function and decorated it\\nwith @ray.remote . This made your function a Ray task. Then, instead of calling the\\noriginal function in your code, you called .remote(...)  on the Ray task. The last step\\nwas to use .get(...)  to get the results from your Ray Cluster. This procedure is so\\nintuitive that you might be able to create your own Ray task from another function\\nwithout having to look back at this example. Why don’t you give it a try right now?\\nComing back to our example: by using Ray tasks, what did we gain in terms of\\nperformance? The runtime clocks in at 0.71 seconds for us, which is just slightly more\\nthan the longest subtask, which comes in at 0.7 seconds. That’s great and much better\\nthan before, but we can further improve our program by leveraging more of Ray’s\\nAPI.\\nUsing the object store with put and get\\nOne thing you might have noticed is that in the definition of retrieve  we directly\\naccessed items from our database. When working on a local Ray Cluster, this is fine,\\nbut imagine you’re running on an actual cluster that includes several computers. How\\nwould all those computers access the same data? Remember from Chapter 1  that in\\na Ray Cluster there is one head node with a driver process (running ray.init() )\\nand many worker nodes with worker processes executing your tasks. By default, Ray\\nwill create as many worker processes as there are CPU cores on your machine. Our\\ndatabase is currently defined on the driver only, but the workers running your tasks\\nneed to have access to it to run the retrieve  task. Luckily, Ray provides an easy\\nway to share objects  between the driver and workers (or between workers). Y ou can\\nsimply use put to place your data into Ray’s distributed object store . In our definition\\nof retrieve_task  we explicitly pass in a db argument, to which we later will pass our\\ndb_object_ref  object:\\ndb_object_ref  = ray.put(database )  \\n@ray.remote\\ndef retrieve_task (item, db):  \\n    time.sleep(item / 10.)\\n    return item, db[item]\\nput your database into the object store and receive a reference to it. This way we\\ncan explicitly pass this reference to our Ray task later.\\nThe Ray task retrieve_task  takes the object reference as an argument.\\nAn Introduction to Ray Core | 29\\n\\nBy using the object store this way, you can let Ray handle data access across the whole\\ncluster.  We’ll talk about how exactly values are passed between nodes and within\\nworkers when talking about Ray’s infrastructure. While the interaction with the\\nobject store requires some overhead, it gives you performance gains when working\\nwith larger, more realistic datasets. For now, the important part is that this step is\\nessential in a truly distributed setting. If you like, try to rerun Example 2-1  with this\\nnew retrieve_task  function and confirm that it still runs as expected.\\nUsing Ray’s wait function for nonblocking calls\\nNote how in Example 2-1  we used ray.get(object_references)  to access results.\\nThis call is blocking , which means that our driver has to wait for all the results to\\nbe available. That’s not a big deal in our case; the program now finishes in under\\na second. But imagine that the processing of each database item would take several\\nminutes. In that case, you would want to free up the driver process for other tasks,\\ninstead of sitting idly by. Also, it would be great to process results as they come\\nin (some finish much quicker than others), rather than waiting for all items to be\\nprocessed. One more question to keep in mind is, what happens if one of the database\\nitems can’t be retrieved as expected? Let’s say there’s a deadlock somewhere in the\\ndatabase connection. The driver would simply hang and never retrieve all items. For\\nthat reason it’s a good idea to work with reasonable timeouts. Let’s say we don’t want\\nto wait longer than 10 times the longest data retrieval task before stopping the task.\\nHere’s how you can do that with Ray by using wait :\\nstart = time.time()\\nobject_references  = [\\n    retrieve_task .remote(item, db_object_ref ) for item in range(8)  \\n]\\nall_data  = []\\nwhile len(object_references ) > 0:  \\n    finished , object_references  = ray.wait(  \\n        object_references , num_returns =2, timeout=7.0\\n    )\\n    data = ray.get(finished )\\n    print_runtime (data, start)  \\n    all_data .extend(data)  \\nRun remote  on our retrieve_task  and pass the respective item  we want to\\nretrieve and the object reference to our database.\\nInstead of blocking, loop through unfinished object_references .\\nWe asynchronously wait  for finished data with a reasonable timeout .\\nobject_references  gets overridden here, to prevent an infinite loop.\\n30 | Chapter 2: Getting Started with Ray Core\\n\\nPrint results as they come in, namely in blocks of two.\\nappend  new data  to the all_data  until finished.\\nAs you can see, ray.wait  returns two arguments: finished values and futures that still\\nneed to be processed. We use the num_returns  argument, which defaults to 1, to let\\nwait  return whenever a new pair of database items is available. This results in the\\nfollowing output for us:\\nRuntime: 0.11 seconds, data:\\n(0, \\'Learning\\')\\n(1, \\'Ray\\')\\nRuntime: 0.31 seconds, data:\\n(2, \\'Flexible\\')\\n(3, \\'Distributed\\')\\nRuntime: 0.51 seconds, data:\\n(4, \\'Python\\')\\n(5, \\'for\\')\\nRuntime: 0.71 seconds, data:\\n(6, \\'Machine\\')\\n(7, \\'Learning\\')\\nNote how in the while  loop, instead of just printing results, we could have done many\\nother things, like starting entirely new tasks on other workers with the values already\\nretrieved up to this point.\\nHandling task dependencies\\nSo far our example program has been fairly easy on a conceptual level. It consists\\nof a single step: retrieving a bunch of database items. Now, imagine that once your\\ndata is loaded you want to run a follow-up processing task. To be more concrete, let’s\\nsay we want to use the result of our first retrieve task to query other, related data (pre‐\\ntend that you’re querying data from a different table in the same database). Exam‐\\nple 2-2  sets up such a task and runs both our retrieve_task  and follow_up_task\\nconsecutively.\\nExample 2-2. Running a follow-up task that depends on another Ray task\\n@ray.remote\\ndef follow_up_task (retrieve_result ):  \\n    original_item , _ = retrieve_result\\n    follow_up_result  = retrieve (original_item  + 1)  \\n    return retrieve_result , follow_up_result   \\nretrieve_refs  = [retrieve_task .remote(item, db_object_ref ) for item in [0, 2, 4, 6]]\\nfollow_up_refs  = [follow_up_task .remote(ref) for ref in retrieve_refs ]  \\nAn Introduction to Ray Core | 31\\n\\n8According to Clarke’s third law , any sufficiently advanced technology is indistinguishable from magic. For me,\\nthis example has a bit of magic to it.\\n9The same thing happened earlier, when we passed an object reference to the remote call of retrieve_task\\nand then directly accessed the respective items of the database db there. We didn’t want to distract you too\\nmuch from the main point of that example.result = [print(data) for data in ray.get(follow_up_refs )]\\nUsing the result of retrieve_task , compute another Ray task on top of it.\\nLeveraging the original_item  from the first task, retrieve  more data.\\nReturn both the original and the follow-up data.\\nPass the object references from the first task to the second task.\\nRunning this code results in the following output:\\n((0, \\'Learning\\'), (1, \\'Ray\\'))\\n((2, \\'Flexible\\'), (3, \\'Distributed\\'))\\n((4, \\'Python\\'), (5, \\'for\\'))\\n((6, \\'Machine\\'), (7, \\'Learning\\'))\\nIf you don’t have a lot of experience with asynchronous programming, you might\\nnot be impressed by Example 2-2 . But we hope to convince you that it’s at least a\\nbit surprising that this code snippet runs at all.8 So, what’s the big deal? After all, the\\ncode reads like regular Python: a function definition and a few list comprehensions.\\nThe point is that the function body of follow_up_task  expects a Python tuple  for its\\ninput argument retrieve_result , which we unpack in the first line of the function\\ndefinition.\\nBut by invoking [follow_up_task.remote(ref) for ref in retrieve_refs]  we\\ndo not pass in tuples to the follow-up task at all. Instead, we pass in Ray object\\nreferences  with retrieve_refs . What happens under the hood is that Ray knows that\\nfollow_up_task  requires actual values, so internally in this task it will call ray.get\\nto resolve the futures.9 Ray builds a dependency graph for all tasks and executes them\\nin an order that respects the dependencies. Y ou do not have to tell Ray explicitly\\nwhen to wait for a previous task to finish; it will infer that information for you. This\\nalso shows you a powerful feature of the Ray object store: if intermediate values are\\nlarge, you can avoid copying them back to the driver. Y ou can just pass your object\\nreferences to the next task and let Ray handle the rest.\\nThe follow-up tasks will be scheduled only once the individual retrieve tasks\\nhave finished. If you ask us, that’s an incredible feature. In fact, if we had called\\n32 | Chapter 2: Getting Started with Ray Core\\n\\n10The actor model is an established concept in computer science, which you can find implemented, e.g., in\\nAkka or Erlang. However, the history and specifics of actors are not relevant to our discussion.retrieve_refs  something like retrieve_result , you may not have even noticed this\\nimportant detail. That’s by design. Ray wants you to focus on your work, not on the\\ndetails of cluster computing. In Figure 2-1  you can see the dependency graph for the\\ntwo tasks visualized.\\nFigure 2-1. Running two dependent tasks asynchronously and in parallel with Ray\\nIf you feel like it, try to rewrite Example 2-2  so that it explicitly uses get on the first\\ntask before passing values into the follow-up task. Not only does this introduce more\\nboilerplate code, it’s also a bit less intuitive to write and understand.\\nFrom classes to actors\\nBefore wrapping up this example, let’s discuss one more important concept of Ray\\nCore.  Notice how everything is essentially a function in our example. We just used\\nthe ray.remote  decorator to make some of them remote functions, and other than\\nthat we used plain Python.\\nLet’s say we wanted to track how often our database has been queried. Sure, we could\\nsimply count the results of our retrieve tasks, but is there a better way to do this? We\\nwant to track this in a “distributed” way that will scale. For that, Ray has the concept\\nof actors . Actors allow you to run stateful  computations on your cluster. They can\\nalso communicate between each other.10 Much like Ray tasks were simply decorated\\nAn Introduction to Ray Core | 33\\n\\nfunctions, Ray actors are decorated Python classes. Let’s write a simple counter to\\ntrack our database calls:\\n@ray.remote  \\nclass DataTracker :\\n    def __init__ (self):\\n        self._counts = 0\\n    def increment (self):\\n        self._counts += 1\\n    def counts(self):\\n        return self._counts\\nMake any Python class a Ray actor by using the same ray.remote  decorator as\\nbefore.\\nThis DataTracker  class is already an actor, since we equipped it with the ray.remote\\ndecorator. This actor can track state, here just a simple counter, and its methods\\nare Ray tasks that get invoked precisely like we did with functions before, namely,\\nusing .remote() . Let’s see how we can modify our existing retrieve_task  to incor‐\\nporate this new actor:\\n@ray.remote\\ndef retrieve_tracker_task (item, tracker, db):  \\n    time.sleep(item / 10.)\\n    tracker.increment .remote()  \\n    return item, db[item]\\ntracker = DataTracker .remote()  \\nobject_references  = [  \\n    retrieve_tracker_task .remote(item, tracker, db_object_ref ) \\n    for item in range(8)\\n]\\ndata = ray.get(object_references )\\nprint(data) \\nprint(ray.get(tracker.counts.remote()))  \\nPasses in the tracker  actor into this task.\\nThe tracker  receives an increment  for each call.\\nInstantiates our DataTracker  actor by calling .remote()  on the class.\\nThe actor gets passed into the retrieve task.\\n34 | Chapter 2: Getting Started with Ray Core\\n\\n11To paraphrase Alan Kay , to get simplicity, you need to find slightly more sophisticated building blocks. The\\nRay API does just that for distributed Python.\\n12Check out the API reference  to see that there are in fact quite a few more methods available. At some point\\nyou should invest in understanding the arguments of init , but all other methods likely won’t be of interest to\\nyou, if you’re not an administrator of your Ray Cluster.\\nAftwerward, we can get the counts  state from our tracker  from another remote\\ninvocation.\\nNot surprisingly, the result of this computation is in fact 8. We didn’t need actors\\nto compute this, but it can be useful to have a mechanism to track state across the\\ncluster, potentially spanning multiple tasks. In fact, we could pass our actor into\\nany dependent task, or even into the constructor of yet another actor. There is no\\nlimitation to what you can do, and it’s this flexibility that makes the Ray API so\\npowerful. It’s not very common for distributed Python tools to allow for stateful\\ncomputations like this. This feature can come in handy, especially when running\\ncomplex distributed algorithms, for instance when using reinforcement learning.\\nThis completes our extensive first Ray API example. We’ll concisely summarize the\\nRay API next.\\nIn this introduction by example we focused a lot on Ray tasks and\\nactors as distributed versions of Python functions and classes. But\\nobjects  are also first-class citizens in Ray Core and should be seen\\nas equal in status to tasks and actors. The object store is a central\\ncomponent of Ray.\\nAn Overview of the Ray Core API\\nIf you recall what we did in the previous example, you’ll notice that we used a total\\nof just six API methods.11 We used ray.init()  to start the cluster and @ray.remote\\nto turn functions and classes into tasks and actors. Then we used ray.put()  to pass\\nvalues into Ray’s object store and ray.get()  to retrieve objects from the cluster.\\nFinally, we used .remote()  on actor methods or tasks to run code on our cluster, and\\nray.wait  to avoid blocking calls.\\nWhile six API methods might not seem like much, those are the only ones you’ll\\nlikely ever care about when using the Ray API.12 We briefly summarize them in\\nTable 2-1  so you can easily reference them in the future.\\nAn Introduction to Ray Core | 35\\n\\nTable 2-1. The six major API methods of Ray Core\\nAPI call Description\\nray.init() Initializes your Ray Cluster. Pass in an address  to connect to an existing cluster.\\n@ray.remote Turns functions into tasks and classes into actors.\\nray.put() Puts values into Ray’s object store.\\nray.get() Gets values from the object store. Returns the values you’ve put  there or that were computed by a task or\\nactor.\\n.remote() Runs actor methods or tasks on your Ray Cluster and is used to instantiate actors.\\nray.wait() Returns two lists of object references, one with finished  tasks we’re waiting for and one with unfinished\\ntasks.\\nNow that you’ve seen the Ray API in action, let’s spend some time on its system\\narchitecture.\\nUnderstanding Ray System Components\\nY ou’ve seen how the Ray API can be used and understand the design philosophy\\nbehind Ray. Now it’s time to get a better understanding of the underlying system\\ncomponents. In other words, how does Ray work and how does it achieve what it\\ndoes?\\nScheduling and Executing Work on a Node\\nY ou know that Ray Clusters consist of nodes. We’ll first look at what happens on indi‐\\nvidual nodes, before we zoom out and discuss how the whole cluster interoperates.\\nAs we’ve already discussed, a worker node consists of several worker processes or\\nsimply workers. Each worker has a unique ID, an IP address, and a port by which\\nthey can be referenced. Workers are called “workers” for a reason; they’re compo‐\\nnents that blindly execute the work you give them. But who tells them what to do and\\nwhen? A worker might be busy already, it may not have the proper resources to run\\na task (e.g., access to a GPU), and it might not even have the values it needs to run\\na given task. On top of that, workers have no knowledge of what happens before or\\nafter they’ve executed their workload; there’s no coordination.\\nTo address these issues, each worker node has a component called Raylet . Think of\\nRaylets as the smart components of a node that manage the worker processes.  Raylets\\nare shared between jobs and consist of two components, a task scheduler  and an object\\nstore .\\nLet’s talk about object stores first. In the running example in this chapter, we’ve\\nalready used the concept of an object store loosely, without explicitly specifying it.\\nEach node of a Ray Cluster is equipped with an object store, within that node’s\\n36 | Chapter 2: Getting Started with Ray Core\\n\\nRaylet, and all objects stored collectively form the distributed object store of a cluster.\\nThe object store manages a shared pool of memory  across workers on the same\\nnode and ensures that workers can access objects that were created on a different\\nnode. The object store is implemented in Plasma , which now belongs to the Apache\\nArrow project. Functionally, the object store takes care of memory management and\\nultimately makes sure workers have access to the objects they need.\\nThe second component of a Raylet is its scheduler. The scheduler takes care of\\nresource management , among other things. For instance, if a task requires access to\\nfour CPUs, the scheduler needs to make sure it can find a free worker process that\\ncan grant access to said resources. By default, the scheduler knows about and acquires\\ninformation about the number of CPUs and GPUs, as well as the amount of memory\\navailable on its node. If a scheduler can’t provide the required resources, it simply\\ncan’t schedule execution of a task right away and needs to queue it. The scheduler\\nlimits which tasks are running concurrently to make sure that you don’t run out of\\nphysical resources.\\nApart from resources, the other requirement the scheduler takes care of is dependency\\nresolution . That means it needs to ensure that each worker has all the objects it needs\\nto execute a task in the local object store. For that to work, the scheduler will first\\nresolve local dependencies by looking up values in its object store. If the required\\nvalue is not available on this node’s object store, the scheduler will communicate with\\nother nodes (we’ll tell you how in a bit) and pull in remote dependencies. Once the\\nscheduler has ensured enough resources for a task, resolved all needed dependencies,\\nand found a worker for a task, it can schedule the task for execution.\\nTask scheduling is a very difficult topic, even if we’re talking only about single\\nnodes. Y ou can easily imagine scenarios in which an incorrectly or naively planned\\ntask execution can “block” downstream tasks because not enough resources remain.\\nEspecially in a distributed context, assigning work like this can be become tricky very\\nquickly.\\nNow that you know about Raylets, let’s briefly come back to worker processes and\\nwrap up the discussion by explaining how Ray can recover from failures and the\\nconcepts needed to do so.\\nIn short, workers store metadata for all the tasks they invoke and the object refer‐\\nences returned by those tasks. This concept, called ownership , means the process that\\ngenerates an object reference is also responsible for its resolution. In other words,\\neach worker process “owns” the tasks it submits, which includes proper execution\\nand ensuring availability of results. Worker processes need to track what they own,\\nfor instance in case of failures, which is why they have a so-called ownership table .\\nThis way, if a task fails and needs to be recomputed, the worker already owns all\\nUnderstanding Ray System Components | 37\\n\\n13This is an extremely limited description of how Ray handles failures in general. After all, just having all the\\ninformation to recover does not tell you how to do so. We refer you to the architecture whitepaper  for an\\nin-depth discussion on this topic.the information it needs to do so.13 To give you a concrete example of an ownership\\nrelationship, as opposed to the concept of dependency discussed earlier, let’s say we\\nhave a program that starts a simple task and internally calls another task:\\n@ray.remote\\ndef task_owned ():\\n    return\\n@ray.remote\\ndef task(dependency ):\\n    res_owned  = task_owned .remote()\\n    return\\nval = ray.put(\"value\")\\nres = task.remote(dependency =val)\\nLet’s quickly analyze ownership and dependency for this example. We defined two\\ntasks in task  and task_owned , and we have three variables in total: val, res, and\\nres_owned . Our main program defines both val (which puts \"value\"  into the object\\nstore) and res, and it also calls task . In other words, the driver owns  task , val, and\\nres according to Ray’s ownership definition. In contrast, res depends on task , but\\nthere’s no ownership relationship between the two. When task  gets called, it takes\\nval as a dependency. It then calls task_owned  and assigns res_owned  and hence owns\\nthem both. Lastly, task_owned  itself does not own anything, but certainly res_owned\\ndepends on it. Figure 2-2  sums up this discussion about worker nodes, showing all\\ninvolved components.\\nFigure 2-2. The system components comprising a Ray worker node\\n38 | Chapter 2: Getting Started with Ray Core\\n\\n14In fact, it could have multiple drivers, but this is not essential for our discussion. Starting a single driver on\\nthe head node is the most common, but driver processes also can be started on any node in the cluster, and\\nmultiple drivers can be on a single cluster.The Head Node\\nWe’ve already indicated in Chapter 1  that each Ray Cluster has one special node\\ncalled a head node . So far you know that this node has a driver process.14 Drivers can\\nsubmit tasks themselves but can’t execute them. Y ou also know that the head node\\ncan have some worker processes, which is important to be able to run local clusters\\nconsisting of a single node.\\nThe head node is identical to other worker nodes, but it additionally runs processes\\nresponsible for cluster management such as the autoscaler (that we cover in Chap‐\\nter 9 ) and a component called Global Control Service  (GCS). This is an important\\ncomponent that carries global information about the cluster.  The GCS is a key-value\\nstore that stores information such as system-level metadata. For instance, it has a\\ntable with heartbeat signals for each Raylet to ensure they are still reachable. Raylets,\\nin turn, send heartbeat signals to the GCS to indicate that they are alive. The GCS also\\nstores the locations of Ray actors.  The ownership model just discussed tells us that all\\nobject information is stored at their owner worker process, which avoids making the\\nGCS a bottleneck.\\nDistributed Scheduling and Execution\\nLet’s briefly talk about cluster orchestration and how nodes manage, plan, and exe‐\\ncute tasks. When talking about worker nodes, we’ve indicated that there are several\\ncomponents to distributing workloads with Ray. Here’s an overview of the steps and\\nintricacies involved in this process:\\nDistributed memory\\nThe object stores of individual Raylets manage memory on a node. But some‐\\ntimes objects need to be transferred between nodes, which is called distributed\\nobject transfer . This is needed for remote dependency resolution so that workers\\nhave the objects they need to run tasks.\\nCommunication\\nMost of the communication in a Ray Cluster, such as object transfer, takes place\\nvia gRPC .\\nResource management and fulfillment\\nOn a node, Raylets are responsible for granting resources and leasing  worker\\nprocesses to task owners.  All schedulers across nodes form the distributed sched‐\\nuler, which effectively means that nodes can schedule tasks on other nodes.\\nUnderstanding Ray System Components | 39\\n\\nThrough communication with the GCS, local schedulers know about other\\nnodes’ resources.\\nTask execution\\nOnce a task has been submitted for execution, all its dependencies (local and\\nremote data) need to be resolved, e.g., by retrieving large data from the object\\nstore, before execution can begin.\\nIf the past few sections seem a bit involved technically, that’s because they are. It’s\\nimportant to understand the basic patterns and ideas of the software you’re using, but\\nwe’ll admit that the details of Ray’s architecture can be a bit tough to wrap your head\\naround in the beginning. In fact, it’s one of Ray’s design principles to trade usability\\nfor architectural complexity. If you want to delve deeper into Ray’s architecture, a\\ngood place to start is their architecture whitepaper .\\nFigure 2-3  summarizes what we know about Ray’s architecture.\\nNow that you’ve learned the basics of the Ray Core API and know the fundamentals\\nof Ray’s Cluster architecture, let’s compute one more complex example.\\nFigure 2-3. An overview of Ray’s architectural components\\n40 | Chapter 2: Getting Started with Ray Core\\n\\n15It’s a drosophila melanogaster  of sorts, not unlike computing a classifier on the ubiquitous MNIST dataset.Systems Related to Ray\\nWith the architecture and functionality of it in mind, how does Ray relate to other\\nsystems? Here are the basics:\\n•Ray can be used as a parallelization framework for Python and shares properties•\\nwith tools like celery  or multiprocessing . In fact, there’s a drop-in replacement\\nfor the latter implemented in Ray.\\n•Ray is also related to data processing frameworks such as Spark, Dask, Flink, and•\\nMARS. We’ll explore these relationships in Chapter 11 , when talking about Ray’s\\necosystem.\\n•As a distributed computing tool, Ray also deals with the problems of cluster•\\nmanagement and orchestration, and we’ll see how Ray does that in relation to\\ntools like Kubernetes in Chapter 9 .\\n•Since Ray is implementing the actor model of concurrency, it’s also interesting to•\\nexplore its relationship with frameworks like Akka.\\n•Lastly, since Ray banks on a performant, low-level API for communication,•\\nthere’s a certain relationship with high-performance computing (HPC) frame‐\\nworks and communication protocols like the message passing interface (MPI).\\nA Simple MapReduce Example with Ray\\nWe can’t let you go without discussing an example of one of the most important\\nmilestones in distributed computing in recent decades, namely, MapReduce . Many\\nsuccessful big data technologies like Hadoop are based on this programming model,\\nand it’s worth revisiting in the context of Ray. To keep things simple, we’ll restrict\\nour MapReduce implementation to a single use case, the task of counting word\\noccurrences across several documents. This is an almost trivial task in single process‐\\ning, but it becomes an interesting challenge once a massive corpus of documents is\\ninvolved and you need multiple compute nodes to crunch the numbers.\\nImplementing a MapReduce word-count example might be the most well-known\\nexample we have in distributed computing,15 so it’s worth knowing. If you don’t know\\nabout this classic paradigm, it’s based on three straightforward steps:\\n1.Take a set of documents and transform or “map” its elements (for instance1.\\nthe words contained in them) according to a function you provide. This map\\nphase  produces key-value pairs by design, in which a key represents a document\\nelement and a value  is simply a metric you want to compute for that element.\\nA Simple MapReduce Example with Ray | 41\\n\\n16In general, a shuffle is any operation that requires redistributing data across its partitions. Shuffles can be\\nquite costly. If your map phase operates on N partitions, it will produce N × N results that need to be shuffled.Since we’re interested in counting words, whenever we encounter a word  in a\\ndocument, our map function  will simply emit the pair (word, 1)  to indicate that\\nwe found one occurrence of it.\\n2.Collect and group all the outputs of the map phase according to their key. Since2.\\nwe work in a distributed setup and the same key might be present on several\\ncompute nodes, this might require shuffling of data between nodes. For that\\nreason this step is often referred to as the shuffle  phase .16 To give you an idea of\\nwhat grouping might mean in our concrete use case, let’s say we have a total of\\nfour (word, 1)  occurrences produced in the map phase. The shuffle would then\\nco-locate all occurrences of the same word on the same node.\\n3.Aggregate or “reduce” the elements from the shuffle step, which is why we refer3.\\nto it as the reduce phase . Continuing with the example we laid out, we simply sum\\nup all word occurrences on each node to get the final count. For instance, four\\noccurrences of (word, 1)  would be reduced to word: 4 .\\nEvidently, MapReduce gets its name from the first and last of these three phases, but\\nthe second one is arguably just as important. While schematically these phases may\\nlook simple, their power lies in the fact that they can be massively parallelized across\\nhundreds of machines.\\nIn Figure 2-4  we illustrate an example of applying the three MapReduce phases to\\na corpus of documents that has been distributed across three partitions. To run\\nMapReduce on a distributed corpus of documents, we first map each document to a\\nset of key-value pairs, then shuffle the results to ensure that all key-value pairs with\\nthe same key are on the same node, and finally reduce the key-value pairs to compute\\nthe final word counts.\\nLet’s implement the MapReduce algorithm for our word-count use case in Python\\nand parallelize the computation using Ray. First, load example data so that you get a\\nbetter idea of what we’re operating on:\\nimport subprocess\\nzen_of_python  = subprocess .check_output ([\"python\" , \"-c\", \"import this\" ])\\ncorpus = zen_of_python .split()  \\nnum_partitions  = 3\\nchunk = len(corpus) // num_partitions\\npartitions  = [  \\n    corpus[i * chunk: (i + 1) * chunk] for i in range(num_partitions )\\n]\\n42 | Chapter 2: Getting Started with Ray Core\\n\\n17Note the usage of yield  in the map function. This is the quickest way of building a generator with the data we\\nneed in Python. Y ou could also build and return  a list of pairs, if that’s clearer to you.\\nOur text corpus is the content of the Zen of Python.\\nSplit the corpus into three partitions.\\nFigure 2-4. Running the MapReduce algorithm on a distributed corpus of documents\\nThe data we’re using is the so-called Zen of Python, a small set of guidelines by the\\nPython community. The Zen is hidden in an “Easter egg” and gets printed when\\nyou type import this  in a Python session. It’s worth reading these guidelines as a\\nPython programmer, but for this exercise we’re only interested in counting the words\\nthey contain. Put simply, we load the Zen of Python, treat each line as a separate\\n“document, ” and split it into three partitions.\\nTo start our implementation of MapReduce, we’ll first cover the map phase and\\ndiscuss how Ray can help us take care of shuffling the results.\\nMapping and Shuffling  Document Data\\nTo define the map phase, we need a map function that we apply to each document.\\nIn our case, we want to emit the pair (word, 1)  for each word  we find in a document .\\nFor simple text documents loaded as Python strings, it looks like this:17\\nA Simple MapReduce Example with Ray | 43\\n\\ndef map_function (document ):\\n    for word in document .lower().split():\\n        yield word, 1\\nNext, we want to apply this map function to a whole corpus of documents. We do this\\nby making the following apply_map  function a Ray task via the @ray.remote  decora‐\\ntor. When we call apply_map , we’ll apply it to three partitions ( num_partitions=3 ) of\\ndocument data, just like we indicated in Figure 2-4 . Note that apply_map  will return\\nthree lists, one for each partition. As you will see in a moment, we do this so that Ray\\ncan automatically shuffle the results of the map phase to the right nodes for us:\\nimport ray\\n@ray.remote\\ndef apply_map (corpus, num_partitions =3):\\n    map_results  = [list() for _ in range(num_partitions )]  \\n    for document  in corpus:\\n        for result in map_function (document ):\\n            first_letter  = result[0].decode(\"utf-8\")[0]\\n            word_index  = ord(first_letter ) % num_partitions   \\n            map_results [word_index ].append(result)  \\n    return map_results\\nThe Ray task apply_map  returns one result for each data partition.\\nAssign each (word, 1)  pair to a partition by using the ord function to generate\\na word_index . This ensures that each occurrence of a word  gets shuffled to the\\nsame partition.\\nThe pairs are then successively appended to the correct list.\\nFor a text corpus that can be loaded on a single machine, this is overkill, and we could\\ncount the words instead. But in a distributed setting, in which we have to partition\\nthe data across several nodes, this map phase makes perfect sense.\\nTo apply the map phase to our corpus of documents in parallel, we use a remote  call\\non apply_map  as we’ve done many times before in this chapter. The notable difference\\nis that now we also instruct Ray to return three results (one for each partition) via the\\nnum_returns  argument:\\nmap_results  = [\\n    apply_map .options(num_returns =num_partitions )  \\n    .remote(data, num_partitions )  \\n    for data in partitions   \\n]\\nfor i in range(num_partitions ):\\n    mapper_results  = ray.get(map_results [i])  \\n44 | Chapter 2: Getting Started with Ray Core\\n\\n18By construction, all same-key pairs will end up on the same node this way. For instance, note how in the\\nsample output we printed the word is that appears in the 0-th return value of two of the mappers. All\\noccurrences of is will end up on the same partition for the reduce phase.    for j, result in enumerate (mapper_results ):\\n        print(f\"Mapper {i}, return value {j}: {result[:2]}\")\\nUse options  to tell Ray to return num_partitions  values.\\nExecute apply_map  remotely.\\nIterate over each of the partitions  we defined.\\nInspect the results for illustration purposes only. Normally you would not call\\nray.get  yet.\\nIf you run this code, you will see that each map phase result consists of three lists, of\\nwhich we print the first two elements of each:\\nMapper 0, return value 0: [(b\\'of\\', 1), (b\\'is\\', 1)]\\nMapper 0, return value 1: [(b\\'python,\\', 1), (b\\'peters\\', 1)]\\nMapper 0, return value 2: [(b\\'the\\', 1), (b\\'zen\\', 1)]\\nMapper 1, return value 0: [(b\\'unless\\', 1), (b\\'in\\', 1)]\\nMapper 1, return value 1: [(b\\'although\\', 1), (b\\'practicality\\', 1)]\\nMapper 1, return value 2: [(b\\'beats\\', 1), (b\\'errors\\', 1)]\\nMapper 2, return value 0: [(b\\'is\\', 1), (b\\'is\\', 1)]\\nMapper 2, return value 1: [(b\\'although\\', 1), (b\\'a\\', 1)]\\nMapper 2, return value 2: [(b\\'better\\', 1), (b\\'than\\', 1)]\\nAs you will see, we can make it so that all pairs from the j-th return value end up on\\nthe same node for the reduce phase.18 Let’s discuss this phase next.\\nReducing Word Counts\\nIn the reduce phase, we can now simply create a dictionary that sums up all word\\noccurrences on each partition:\\n@ray.remote\\ndef apply_reduce (*results):  \\n    reduce_results  = dict()\\n    for res in results:\\n        for key, value in res:\\n            if key not in reduce_results :\\n                reduce_results [key] = 0\\n            reduce_results [key] += value  \\n    return reduce_results\\nA Simple MapReduce Example with Ray | 45\\n\\nReduce the list of shuffled map results.\\nIterate over each result  obtained from the map phase and increase word counts\\nby one for each occurrence of a word.\\nWe can now collect the j-th return value from each mapper and pass it to the j-th\\nreducer as follows. Note that we use a toy dataset here, but this code would scale to\\ndatasets that don’t fit on a single machine. That’s because we’re passing Ray object\\nreferences to the reducers, not the actual data. The map and reduce phases are Ray\\ntasks that can be executed on any Ray Cluster, and the shuffling of the data is handled\\nby Ray as well:\\noutputs = []\\nfor i in range(num_partitions ):\\n    outputs.append(  \\n        apply_reduce .remote(*[partition [i] for partition  in map_results ])\\n    )\\ncounts = {k: v for output in ray.get(outputs) for k, v in output.items()}  \\nsorted_counts  = sorted(counts.items(), key=lambda item: item[1], reverse=True)  \\nfor count in sorted_counts :\\n    print(f\"{count[0].decode(\\'utf-8\\')}: {count[1]}\")\\nGather one output from each map task and supply it to apply_reduce .\\nCollect all reduce-phase results in a single Python count  dictionary.\\nPrint the sorted word count over the full corpus.\\nRunning this example will yield the following output:\\nis: 10\\nthan: 8\\nbetter: 8\\nthe: 6\\nto: 5\\nalthough: 3\\n...\\nIf you want a deep dive into making MapReduce tasks scale to multiple nodes\\nwith Ray, including detailed memory considerations, we recommend studying the\\nexcellent blog post  on this topic.\\nThe important part about this MapReduce example is how flexible  Ray’s programming\\nmodel really is . Surely, a production-grade MapReduce implementation takes a bit\\nmore effort. But being able to reproduce common algorithms like this one quickly\\ngoes a long way. Keep in mind that in the earlier phases of MapReduce, say around\\n46 | Chapter 2: Getting Started with Ray Core\\n\\n19We encourage you to check out Ray’s in-depth patterns and anti-patterns for both tasks  and actors .2010, this paradigm was often the only thing you had to express your workloads.\\nWith Ray, a whole range of interesting distributed computing patterns become acces‐\\nsible to any intermediate Python programmer.19\\nSummary\\nY ou’ve seen the basics of the Ray API in action in this chapter. Y ou know how to\\nput values into the object store and how to get them back. Also, you’re familiar with\\ndeclaring Python functions as Ray tasks with the @ray.remote  decorator, and you\\nknow how to run them on a Ray Cluster with the .remote()  call. In much the same\\nway, you understand how to declare a Ray actor from a Python class and how to\\ninstantiate it and leverage it for stateful, distributed computations.\\nOn top of that, you also know the basics of Ray Clusters. After starting them with\\nray.init(...) , you know that you can submit jobs consisting of tasks to your clus‐\\nter. The driver process, sitting on the head node, will then distribute the tasks to the\\nworker nodes. Raylets on each node will schedule the tasks, and worker processes will\\nexecute them. Y ou’ve also seen a quick implementation of the MapReduce paradigm\\nwith Ray as an example of a common pattern of building Ray applications.\\nThis quick tour through Ray Core should get you started with writing your own\\ndistributed programs. In Chapter 3  we’ll test your knowledge by implementing a basic\\nmachine learning application.\\nSummary | 47\\n\\n\\n\\nCHAPTER 3\\nBuilding Your First Distributed Application\\nNow that you’ve seen the basics of the Ray API in action, let’s build something\\nmore realistic with it. By the end of this chapter, you will have built a reinforcement\\nlearning (RL) problem from scratch, implemented your first algorithm to tackle it,\\nand used Ray tasks and actors to parallelize this solution to a local cluster—all in less\\nthan 250 lines of code.\\nThis chapter is designed to work for readers who don’t have any experience with RL.\\nWe’ll work on a straightforward problem and develop the necessary skills to tackle\\nit hands-on. Since Chapter 4  is devoted entirely to this topic, we’ll skip all advanced\\nRL topics and language and just focus on the problem at hand. But even if you’re a\\nquite advanced RL user, you’ll likely benefit from implementing a classic algorithm in\\na distributed setting.\\nThis is the last chapter working only with Ray Core. We hope you learn to appreciate\\nhow powerful and flexible it is and how quickly you can implement distributed\\nexperiments that would otherwise take considerable efforts to scale.\\nBefore we jump into any implementation, let’s quickly talk about the paradigm of RL\\nin a bit more detail. Feel free to skip this section if you’ve worked with RL before.\\nIntroducing Reinforcement Learning\\nOne of my (Max’s) favorite mobile apps can automatically classify or “label” individ‐\\nual plants in our garden. The app works by simply showing it a picture of the plant\\nin question. That’s immensely helpful; I’m terrible at distinguishing them. (I’m not\\nbragging about the size of my garden; I’m just bad at it.) In the last couple of years\\nwe’ve seen a surge of impressive applications similar to this one.\\n49\\n\\n1We don’t yet have gardening robots, and we don’t know which AI paradigm will get us there. RL isn’t\\nnecessarily the answer; it is just a paradigm that naturally fits into this specific discussion of AI goals.Ultimately, the promise of AI is to build intelligent agents that go far beyond classify‐\\ning objects. Imagine an AI application that not only knows your plants but can take\\ncare of them too. Such an application would have to do the following:\\n•Operate in dynamic environments (like the change of seasons)•\\n•React to changes in the environment (like a heavy storm or pests)•\\n•Take sequences of actions (like watering and fertilizing plants)•\\n•Accomplish long-term goals (like prioritizing plant health)•\\nBy observing its environment, such an AI would also learn to explore the possible\\nactions it could take and come up with better solutions over time. If you feel like\\nthis example is artificial or too far out, it’s not difficult to come up with examples\\non your own that share all these requirements. Think of managing and optimizing a\\nsupply chain, strategically restocking a warehouse considering fluctuating demands,\\nor orchestrating the processing steps in an assembly line. Another famous example of\\nwhat you could expect from AI is Stephen Wozniak’s famous “Coffee Test”: if you’re\\ninvited to a friend’s house, you can navigate to the kitchen, spot the coffee machine\\nand all necessary ingredients, figure out how to brew a cup of coffee, and sit down to\\nenjoy it. A machine should be able to do the same, except the last part might be a bit\\nof a stretch. What other examples can you think of?\\nY ou can frame all the requirements naturally in RL, a subfield of machine learning .1\\nFor now, it’s enough to understand that RL is about agents interacting with their\\nenvironment by observing it and emitting actions. In RL, agents evaluate their envi‐\\nronments by attributing a reward (e.g., how healthy is my plant on a linear scale).\\nThe term “reinforcement” comes from the fact that agents will ideally learn to seek\\nbehavior that leads to good outcomes (high reward) and shy away from punishing\\nsituations (low or negative reward).\\nThe interaction of agents with their environment is usually modeled by creating a\\ncomputer simulation of it (although sometimes that’s not feasible). So, let’s build an\\nexample of such a simulation with agents acting in their environments to give you an\\nidea of what this looks like in practice.\\nSetting Up a Simple Maze Problem\\nAs with the previous chapters, we encourage you to code this chapter and build this\\napplication as we go.  If you don’t want to do that, you can simply follow the notebook\\nfor this chapter .\\n50 | Chapter 3: Building Your First Distributed Application\\n\\nTo give you an idea, the app we’re building is structured as follows:\\n•Implement a simple 2D-maze game in which a single player can move around in•\\nthe four major directions.\\n•Initialize the maze as a 5 × 5 grid to which the player is confined. One of the 25•\\ngrid cells is the “goal” that a player called the seeker  must reach.\\n•Employ an RL algorithm instead of hard-coding a solution so that the seeker•\\nlearns to find the goal.\\n•Run simulations of the maze repeatedly, rewarding the seeker for finding the goal•\\nand smartly keeping track of which of the seeker’s decisions worked and which\\ndidn’t. Because running simulations can be parallelized and our RL algorithm can\\nalso be trained in parallel, we use the Ray API to parallelize the whole process.\\nWe’re not quite ready to deploy this application on an actual Ray Cluster composed of\\nmultiple nodes just yet, so for now we’ll continue to work with local clusters. If you’re\\ninterested in infrastructure topics and want to learn how to set up Ray Clusters, jump\\nahead to Chapter 9 . In any case, make sure you have Ray installed with pip install\\nray.\\nLet’s start by implementing the 2D maze we just sketched. The idea is to implement\\na simple grid in Python that spans a 5 × 5 grid starting at (0, 0) and ending at (4,\\n4) and properly define how a player can move around the grid. To do this, we first\\nneed an abstraction for moving in the four cardinal directions. These four actions,\\nnamely, moving up, down, left, and right, can be encoded in Python as a class we call\\nDiscrete . The abstraction of moving in several discrete actions is so useful that we’ll\\ngeneralize it to n directions, instead of just four. In case you’re worried, this is not\\npremature—we’ll actually need a general Discrete  class in a moment:\\nimport random\\nclass Discrete :\\n    def __init__ (self, num_actions : int):\\n        \"\"\" Discrete action space for num_actions.\\n        Discrete(4) can be used as encoding moving in\\n        one of the cardinal directions.\\n        \"\"\"\\n        self.n = num_actions\\n    def sample(self):\\n        return random.randint(0, self.n - 1)  \\nspace = Discrete (4)\\nprint(space.sample())  \\nSetting Up a Simple Maze Problem | 51\\n\\nA discrete action can be uniformly sampled between 0 and n – 1.\\nFor instance, a Discrete(4)  sample will give you 0, 1, 2, or 3.\\nSampling from a Discrete(4)  like in this example will randomly return 0, 1, 2, or 3.\\nHow we interpret these numbers is up to us, so let’s say we go for “down, ” “left, ” “up, ”\\nand “right” in that order.\\nNow that we know how to encode moving around the maze, let’s code the maze itself,\\nincluding the goal  cell and the position of the seeker  player that tries to find the\\ngoal. To this end we’re going to implement a Python class called Environment . It’s\\ncalled that because the maze is the environment in which the player “lives. ” To make\\nmatters easy, we’ll always put the seeker  at (0, 0) and the goal  at (4, 4).  To make the\\nseeker  move and find the goal, we initialize the Environment  with an action_space\\nof Discrete(4) .\\nWe need to set up one last bit of information for our maze environment: an encoding\\nof the seeker  position. The reason is that we’re going to implement an algorithm later\\nthat keeps track of which actions led to good results for which seeker  positions. By\\nencoding the seeker position as a Discrete(5*5) , it becomes a single number that’s\\nmuch easier to work with. In RL lingo it is common to call the information of the\\ngame that is accessible to the player an observation . So, in an analogy to the actions\\nwe can carry out for our seeker , we can also define an observation_space  for it.\\nHere’s the implementation of what we’ve just discussed:\\nimport os\\nclass Environment :\\n    def __init__ (self,  *args, **kwargs):\\n        self.seeker, self.goal = (0, 0), (4, 4)  \\n        self.info = {\\'seeker\\' : self.seeker, \\'goal\\': self.goal}\\n        self.action_space  = Discrete (4)  \\n        self.observation_space  = Discrete (5*5)  \\nThe seeker  gets initialized in the top left, the goal  in the bottom right of the\\nmaze.\\nOur seeker  can move down, left, up, and right.\\nIt can be in a total of 25 states, one for each position on the grid.\\n52 | Chapter 3: Building Your First Distributed Application\\n\\nNote that we defined an info  variable as well, which can be used to print information\\nabout the current state of the maze, for instance for debugging purposes. To play an\\nactual game of find-the-goal from the perspective of the seeker, we have to define a\\nfew helper methods. Clearly, the game should be considered “done” when the seeker\\nfinds the goal. Also, we should reward the seeker for finding the goal. And when\\nthe game is over, we should be able to reset it to its initial state, to play again.\\nTo round things off, we also define a get_observation  method that returns the\\nencoded seeker  position. Continuing our implementation of the Environment  class,\\nthis translates into the following four methods:\\n    def reset(self):  \\n        \"\"\"Reset seeker position and return observations.\"\"\"\\n        self.seeker = (0, 0)\\n        return self.get_observation ()\\n    def get_observation (self):\\n        \"\"\"Encode the seeker position as integer\"\"\"\\n        return 5 * self.seeker[0] + self.seeker[1]  \\n    def get_reward (self):\\n        \"\"\"Reward finding the goal\"\"\"\\n        return 1 if self.seeker == self.goal else 0  \\n    def is_done(self):\\n        \"\"\"We\\'re done if we found the goal\"\"\"\\n        return self.seeker == self.goal  \\nTo play a new game, reset  the grid to its original state.\\nConvert the seeker tuple to a value from the environment’s observation_space .\\nThe seeker is rewarded only upon reaching the goal.\\nIf the seeker is at the goal, the game is over.\\nThe last essential method to implement is the step  method.  Imagine you’re playing\\nour maze game and decide to go right as your next move. The step  method will\\ntake this action (namely, 3, the encoding of “right”) and apply it to the internal state\\nof the game. To reflect what changed, the step  method will then return the seeker’s\\nobservations, its reward, whether the game is over, and the info  value of the game.\\nHere’s how the step  method works:\\n    def step(self, action):\\n        \"\"\"Take a step in a direction and return all available information.\"\"\"\\n        if action == 0:  # move down\\n            self.seeker = (min(self.seeker[0] + 1, 4), self.seeker[1])\\nSetting Up a Simple Maze Problem | 53\\n\\n        elif action == 1:  # move left\\n            self.seeker = (self.seeker[0], max(self.seeker[1] - 1, 0))\\n        elif action == 2:  # move up\\n            self.seeker = (max(self.seeker[0] - 1, 0), self.seeker[1])\\n        elif action == 3:  # move right\\n            self.seeker = (self.seeker[0], min(self.seeker[1] + 1, 4))\\n        else:\\n            raise ValueError (\"Invalid action\" )\\n        obs = self.get_observation ()\\n        rew = self.get_reward ()\\n        done = self.is_done()\\n        return obs, rew, done, self.info  \\nReturns the observation, reward, whether we’re done, and any additional infor‐\\nmation we might find useful after taking a step in the specified direction.\\nWe said the step  method was the last essential method, but we actually want to define\\none more helper method that’s extremely useful for visualizing the game and helping\\nus understand it. This render  method will print the current state of the game to the\\ncommand line:\\n    def render(self, *args, **kwargs):\\n        \"\"\"Render the environment, e.g., by printing its representation.\"\"\"\\n        os.system(\\'cls\\' if os.name == \\'nt\\' else \\'clear\\')  \\n        grid = [[\\'| \\' for _ in range(5)] + [\"|\\\\n\"] for _ in range(5)]\\n        grid[self.goal[0]][self.goal[1]] = \\'|G\\'\\n        grid[self.seeker[0]][self.seeker[1]] = \\'|S\\'  \\n        print(\\'\\'.join([\\'\\'.join(grid_row ) for grid_row  in grid]))  \\nClear the screen.\\nDraw the grid and mark the goal as G and the seeker as S on it.\\nThe grid then gets rendered by printing it to your screen.\\nGreat, now we have completed the implementation of our Environment  class that’s\\ndefining our 2D-maze game. We can step  through this game, know when it’s done ,\\nand reset  it again. The player of the game, the seeker , can also observe its environ‐\\nment and get rewarded for finding the goal.\\nLet’s use this implementation to play a game of find-the-goal for a seeker that simply\\ntakes random actions. This can be done by creating a new Environment , sampling\\nand applying actions to it, and rendering the environment until the game is over:\\nimport time\\nenvironment  = Environment ()\\n54 | Chapter 3: Building Your First Distributed Application\\n\\nwhile not environment .is_done():\\n    random_action  = environment .action_space .sample()  \\n    environment .step(random_action )\\n    time.sleep(0.1)\\n    environment .render()  \\nWe can test our environment by applying sampled actions until we’re finished.\\nTo visualize the environment, render it after waiting for a tenth of a second\\n(otherwise the code runs too fast to follow).\\nIf you run this on your computer, eventually you’ll see that the game is over and the\\nseeker has found the goal. It might take a while if you’re unlucky.\\nIn case you’re objecting that this is an extremely simple problem, and to solve it,\\nall you have to do is take a total of eight steps, namely, going right and down four\\ntimes each in arbitrary order, we’re not arguing with you. The point is that we want\\nto tackle this problem using machine learning, so that we can take on much harder\\nproblems later. Specifically, we want to implement an algorithm that figures out on its\\nown how to play the game, merely by playing the game repeatedly: observing what’s\\nhappening, deciding what to do next, and getting rewarded for its actions.\\nIf you want, now is a good time to make the game more complex. As long as you do\\nnot change the interface we defined for the Environment  class, you could modify this\\ngame in many ways. Here are a few suggestions:\\n•Make it a 10 × 10 grid or randomize the initial position of the seeker.•\\n•Make the outer walls of the grid dangerous. Whenever you touch them, you’ll•\\nincur a reward of –100, i.e., a steep penalty.\\n•Introduce obstacles in the grid that the seeker cannot pass through.•\\nIf you’re feeling really adventurous, you could also randomize the goal position. This\\nrequires extra care, as currently the seeker has no information about the goal position\\nin terms of the get_observation  method. Maybe come back to tackling this last\\nexercise after you’ve finished reading this chapter.\\nBuilding a Simulation\\nWith the Environment  class implemented, what does it take to tackle the problem of\\n“teaching” the seeker to play the game well? How can it find the goal consistently\\nin the minimum number of eight steps necessary? We’ve equipped the maze environ‐\\nment with reward information so that the seeker can use this signal to learn to play\\nthe game. In RL, you play games repeatedly and learn from the experience you gain in\\nthe process. The player of the game is often referred to as an agent  that takes actions\\nBuilding a Simulation | 55\\n\\n2As we’ll see in Chapter 4 , you can run RL on multiplayer games too. Making the maze environment a\\nso-called multi-agent environment, in which multiple seekers compete for the goal, is an interesting exercise.in the environment, observes its state , and receives a reward .2 The better an agent\\nlearns, the better it becomes at interpreting the current game state (observations) and\\nfinding actions that lead to more rewarding outcomes.\\nRegardless of the RL algorithm you want to use, you need to have a way of simulating\\nthe game repeatedly to collect experience data. For this reason, we’re going to imple‐\\nment a simple Simulation  class.\\nThe other useful abstraction we need to proceed is that of a Policy , a way of\\nspecifying actions. Right now the only thing we can do to play the game is sample\\nrandom actions for our seeker. What a Policy  allows us to do is to get better actions\\nfor the current state of the game. In fact, we define a Policy  to be a class with a\\nget_action  method that takes a game state and returns an action.\\nRemember that in our game the seeker has a total of 25 possible states on the grid\\nand can carry out four actions. A simple idea would be to look at pairs of states and\\nactions and assign a high value to a pair if carrying out this action in this state will\\nlead to a high reward, and a low value otherwise. For instance, from your intuition of\\nthe game it should be clear that going down or right is always a good idea, whereas\\ngoing left or up is not. Then, create a 25 × 4 lookup table of all possible state-action\\npairs and store it in our Policy . Then we could simply ask our policy to return the\\nhighest value of any action, given a state. Of course, implementing an algorithm that\\nfinds good values for these state-action pairs is the challenging part. Let’s implement\\nthis idea of a Policy  first and worry about a suitable algorithm later:\\nimport numpy as np\\nclass Policy:\\n    def __init__ (self, env):\\n        \"\"\"A Policy suggests actions based on the current state.\\n        We do this by tracking the value of each state-action pair.\\n        \"\"\"\\n        self.state_action_table  = [\\n            [0 for _ in range(env.action_space .n)]\\n            for _ in range(env.observation_space .n)  \\n        ]\\n        self.action_space  = env.action_space\\n    def get_action (self, state, explore=True, epsilon=0.1):  \\n        \"\"\"Explore randomly or exploit the best value currently available.\"\"\"\\n        if explore and random.uniform(0, 1) < epsilon:  \\n            return self.action_space .sample()\\n        return np.argmax(self.state_action_table [state])  \\n56 | Chapter 3: Building Your First Distributed Application\\n\\nDefine a nested list of values for each state-action pair, initialized to zero.\\nexplore  random actions on demand so that we don’t get stuck in suboptimal\\nbehavior.\\nIntroduce an explore  parameter to the get_action  method because we might\\nwant to explore actions randomly in the game. By default, this happens 10% of\\nthe time.\\nReturn the action with the highest value in the lookup table, given the current\\nstate.\\nWe’ve snuck a little implementation detail into the Policy  definition that might be\\na bit confusing. The get_action  method has an explore  parameter. Without it, if\\nyou learn an extremely poor policy (e.g., one that always wants you to move left),\\nyou have no chance of ever finding better solutions. In other words, sometimes you\\nneed to explore new ways and not “exploit” your current understanding of the game.\\nAs indicated before, we haven’t discussed how to learn to improve the values in the\\nstate_action_table  of our policy. For now, just keep in mind that the policy gives us\\nthe actions we want to follow when simulating the maze game.\\nMoving on to the Simulation  class we spoke about earlier, a simulation should take\\nan Environment  and compute actions of a given Policy  until the goal is reached and\\nthe game ends. The data we observe when “rolling out” a full game like this is what\\nwe call the experience  we gained. Accordingly, our Simulation  class has a rollout\\nmethod that computes experiences  for a full game and returns them. Here’s what the\\nimplementation of the Simulation  class looks like:\\nclass Simulation (object):\\n    def __init__ (self, env):\\n        \"\"\"Simulates rollouts of an environment, given a policy to follow.\"\"\"\\n        self.env = env\\n    def rollout(self, policy, render=False, explore=True, epsilon=0.1):  \\n        \"\"\"Returns experiences for a policy rollout.\"\"\"\\n        experiences  = []\\n        state = self.env.reset()  \\n        done = False\\n        while not done:\\n            action = policy.get_action (state, explore, epsilon)  \\n            next_state , reward, done, info = self.env.step(action)  \\n            experiences .append([state, action, reward, next_state ])  \\n            state = next_state\\n            if render:  \\n                time.sleep(0.05)\\n                self.env.render()\\nBuilding a Simulation | 57\\n\\n        return experiences\\nCompute a game “rollout” by following the actions of a policy , and optionally\\nrender the simulation.\\nTo be sure, reset the environment before each rollout.\\nThe passed-in policy  drives the actions we take. The explore  and epsilon\\nparameters are passed through.\\nStep through the environment by applying the policy’s action .\\nDefine an experience as a (state, action, reward, next_state)  quadruple.\\nOptionally render the environment at each step.\\nNote that each entry of the experiences  we collect in a rollout  consists of four\\nvalues: the current state, the action taken, the reward received, and the next state. The\\nalgorithm we’re going to implement in a moment will use these experiences to learn\\nfrom them. Other algorithms might use other experience values, but those are the\\nones we need to proceed.\\nWe now have a policy that hasn’t learned anything just yet, but we can already\\ntest its interface to see if it works. Let’s try it out by initializing a Simulation\\nobject, calling its rollout  method on a not-so-smart Policy , and then printing the\\nstate_action_table  of it:\\nuntrained_policy  = Policy(environment )\\nsim = Simulation (environment )\\nexp = sim.rollout(untrained_policy , render=True, epsilon=1.0)  \\nfor row in untrained_policy .state_action_table :\\n    print(row)  \\nRoll out one full game with an “untrained” policy that we render.\\nThe state-action values are currently all zero.\\nIf you feel like we haven’t made much progress since the previous section, rest\\nassured that things will come together in the next one. The prep work of setting up\\na Simulation  and a Policy  were necessary to frame the problem correctly. Now the\\nonly thing that’s left is to devise a smart way to update the internal state of the Policy\\nbased on the experiences we’ve collected so that it actually learns to play the maze\\ngame.\\n58 | Chapter 3: Building Your First Distributed Application\\n\\nTraining a Reinforcement Learning Model\\nImagine we have a set of experiences that we’ve collected from a couple of games.\\nWhat would be a smart way to update the values in the state_action_table  of our\\nPolicy ? Here’s one idea. Let’s say you’re sitting at position (3,5), and you’ve decided\\nto go right, which puts you at (4,5), just one step away from the goal. Clearly you\\ncould then just go right and collect a reward of 1. That must mean the current state\\nyou’re in, combined with an action of going “right, ” should have a high value. In\\nother words, the value of this particular state-action pair should be high. In contrast,\\nmoving left in the same situation does not lead to anything, and the corresponding\\nstate-action pair should have a low value.\\nMore generally, let’s say you were in a given state , you decided to take an action ,\\nleading to a reward, and you’re then in next_state . Remember that this is how we\\ndefined an experience. With our policy.state_action_table  we can peek a little\\nahead and see if we can expect to gain anything from actions taken from next_state .\\nThat is, we can compute:\\nnext_max  = np.max(policy.state_action_table [next_state ])\\nHow should we compare the knowledge of this value to the current state-action value,\\nwhich is value = policy.state_action_table[state][action] ? There are many\\nways to go about this, but we clearly can’t completely discard the current value  and\\nput too much trust in next_max . After all, this is just a single piece of experience we’re\\nusing here. So as a first approximation, why don’t we simply compute a weighted sum\\nof the old and the expected value and go with new_value = 0.9 * value + 0.1 *\\nnext_max ? Here, the values 0.9 and 0.1 have been chosen somewhat arbitrarily; the\\nonly important pieces are that the first value is high enough to reflect our preference\\nto keep the old value and that both weights sum to 1. That formula is a good starting\\npoint, but the problem is that we’re not at all factoring in the crucial information that\\nwe’re getting from the reward . In fact, we should put more trust in the current reward\\nvalue than in the projected next_max  value, so it’s a good idea to discount the latter a\\nlittle, let’s say by 10%. Updating the state-action value would then look like this:\\nnew_value  = 0.9 * value + 0.1 * (reward + 0.9 * next_max )\\nDepending on your level of experience with this kind of reasoning, the last few para‐\\ngraphs might be a lot to digest. If you’ve understood the explanations up to this point,\\nthe remainder of this chapter will likely come easily to you. Mathematically, this\\nwas the last (and only) hard part of this example. If you’ve worked with RL before,\\nyou will have noticed that this is an implementation of the so-called Q-Learning\\nalgorithm. It’s called that because the state-action table can be described as a function\\nQ(state, action)  that returns values for these pairs.\\nTraining a Reinforcement Learning Model | 59\\n\\nWe’re almost there, so let’s formalize the procedure with an update_policy  function\\nfor a policy and collected experiences:\\ndef update_policy (policy, experiences , weight=0.1, discount_factor =0.9):\\n    \"\"\"Updates a given policy with a list of (state, action, reward, state)\\n    experiences.\"\"\"\\n    for state, action, reward, next_state  in experiences :  \\n        next_max  = np.max(policy.state_action_table [next_state ])  \\n        value = policy.state_action_table [state][action]  \\n        new_value  = (1 - weight) * value + weight * \\\\\\n                    (reward + discount_factor  * next_max )  \\n        policy.state_action_table [state][action] = new_value   \\nLoop through all experiences in order.\\nChoose the maximum value among all possible actions in the next state.\\nExtract the current state-action value.\\nThe new value is the weighted sum of the old value and the expected value, which\\nis the sum of the current reward and the discounted next_max .\\nAfter updating, set the new state_action_table  value.\\nHaving this function in place now makes it simple to train a policy to make better\\ndecisions. We can use the following procedure:\\n1.Initialize a policy and a simulation.1.\\n2.Run the simulation many times, let’s say for a total of 10,000 runs.2.\\n3.For each game, first collect the experiences by running a rollout . 3.\\n4.Then update the policy by calling update_policy  on the collected experiences. 4.\\nThat’s it! The following train_policy  function implements this procedure:\\ndef train_policy (env, num_episodes =10000, weight=0.1, discount_factor =0.9):\\n    \"\"\"Training a policy by updating it with rollout experiences.\"\"\"\\n    policy = Policy(env)\\n    sim = Simulation (env)\\n    for _ in range(num_episodes ):\\n        experiences  = sim.rollout(policy)  \\n        update_policy (policy, experiences , weight, discount_factor )  \\n    return policy\\ntrained_policy  = train_policy (environment )  \\n60 | Chapter 3: Building Your First Distributed Application\\n\\nCollect experiences for each game.\\nUpdate our policy with those experiences.\\nFinally, train and return a policy for our enviroment  from before.\\nNote that in the RL literature, the high-brow way of referring to a full play-through\\nof the maze game is an episode . That’s why we call the argument num_episodes  in the\\ntrain_policy  function, rather than num_games .\\nQ-Learning\\nThe Q-Learning algorithm we just implemented is often the first algorithm taught in\\nRL classes, mostly because it is relatively easy to reason with.  Y ou collect and tabulate\\nexperience data that shows you how well state-action pairs work, and then you update\\nthe table according to the Q-Learning update rule.\\nFor RL problems that have a huge number of either states or actions, the Q-table can\\nbecome excessively large. The algorithm then becomes inefficient, because it would\\ntake too much time to collect enough experience data for all (relevant) state-action\\npairs.\\nOne way to address this issue is to use a neural network to approximate the Q-table.\\nBy this we mean that you can employ a deep neural network to learn a function that\\nmaps states to actions. This approach is called Deep Q-Learning, and the networks\\nused for learning are called Deep Q-Networks (DQN). From Chapter 4  on, we will\\nexclusively use deep learning to tackle RL problems in this book.\\nNow that we have a trained policy, let’s see how well it performs. We’ve run random\\npolicies twice before in this chapter, just to get an idea of how well they work for the\\nmaze problem. But let’s now properly evaluate our trained policy on several games\\nand see how it does on average. Specifically, we’ll run our simulation for a couple of\\nepisodes and count how many steps it took per episode to reach the goal. So, let’s\\nimplement an evaluate_policy  function that does precisely that:\\ndef evaluate_policy (env, policy, num_episodes =10):\\n    \"\"\"Evaluate a trained policy through rollouts.\"\"\"\\n    simulation  = Simulation (env)\\n    steps = 0\\n    for _ in range(num_episodes ):\\n        experiences  = simulation .rollout(policy, render=True, explore=False)  \\n        steps += len(experiences )  \\n    print(f\"{steps / num_episodes } steps on average \"\\n          f \"for a total of {num_episodes } episodes.\" )\\nTraining a Reinforcement Learning Model | 61\\n\\n    return steps / num_episodes\\nevaluate_policy (environment , trained_policy )\\nThis time, set explore  to False  to fully exploit the trained policy’s learnings.\\nThe length of the experiences  is the number of steps we took to finish the game.\\nApart from seeing the trained policy crush the maze problem 10 times in a row, as we\\nhoped it would, you should also see the following prompt:\\n8.0 steps on average for a total of 10 episodes.\\nIn other words, the trained policy is able to find optimal solutions for the maze game.\\nThat means you’ve successfully implemented your first RL algorithm from scratch!\\nWith the understanding you’ve built, do you think placing the seeker  into random‐\\nized starting positions and then running this evaluation function would still work?\\nWhy don’t you go ahead and make the changes necessary for that?\\nAnother interesting question to ask yourself is what assumptions went into the\\nalgorithm we used. For instance, it’s clearly a prerequisite for the algorithm that all\\nstate-action pairs can be tabulated. Do you think this would still work well if we had\\nmillions of states and thousands of actions?\\nBuilding a Distributed Ray App\\nWe hope you have enjoyed the example so far, but you might be wondering how what\\nwe’ve done until now relates to Ray (which is a great question).  As you’ll see shortly,\\nall we need to make the RL experiment a distributed Ray app is writing three short\\ncode snippets. This is what we’re going to do:\\n1.Make the Simulation  a Ray actor using just a few lines of code. 1.\\n2.Define a parallel version of train_policy  that’s structurally similar to its original. 2.\\nFor simplicity, we will parallelize only the rollouts, not the policy updates.\\n3.Train and evaluate the policy as before but using train_policy_parallel . 3.\\nLet’s tackle the first step of this plan by implementing a Ray actor called Simulation\\nActor :\\nimport ray\\nray.init()\\n@ray.remote\\n62 | Chapter 3: Building Your First Distributed Application\\n\\nclass SimulationActor (Simulation ):  \\n    \"\"\"Ray actor for a Simulation.\"\"\"\\n    def __init__ (self):\\n        env = Environment ()\\n        super().__init__ (env)\\nThis Ray actor wraps our Simulation  class in a straightforward way.\\nWith the foundations on Ray Core you’ve developed in Chapter 2 , you should have\\nno problems reading this code. It might take some practice to be able to write it\\nyourself, but conceptually you should be on top of this example.\\nMoving on, let’s define a train_policy_parallel  function that distributes this RL\\nworkload on your local Ray Cluster. To do so, we create a policy  on the driver and\\na total of four SimulationActor  instances that we can use for distributed rollouts.\\nWe then put the policy  into the object store with ray.put  and pass it to the remote\\nrollout  calls as an argument to collect experiences for a given number of training\\nepisodes.  We then use ray.wait  to get the finished rollouts (and account for the fact\\nthat some rollouts might finish earlier than others) and update our policy with the\\ncollected experiences. Finally, we return the trained policy:\\ndef train_policy_parallel (env, num_episodes =1000, num_simulations =4):\\n    \"\"\"Parallel policy training function.\"\"\"\\n    policy = Policy(env)  \\n    simulations  = [SimulationActor .remote() for _ in range(num_simulations )]  \\n    policy_ref  = ray.put(policy)  \\n    for _ in range(num_episodes ):\\n        experiences  = [sim.rollout.remote(policy_ref ) for sim in simulations ]  \\n        while len(experiences ) > 0:\\n            finished , experiences  = ray.wait(experiences )  \\n            for xp in ray.get(finished ):\\n                update_policy (policy, xp)\\n    return policy\\nInitialize a policy for the given environment.\\nInstead of one simulation, create four simulation actors.\\nPut the policy into the object store.\\nFor each of the 1,000 episodes, collect experience data in parallel using our\\nsimulation actors.\\nBuilding a Distributed Ray App | 63\\n\\nFinished rollouts can be retrieved from the object store and used to update the\\npolicy.\\nThis allows us to take the last step and run the training procedure in parallel and  then\\nevaluate the result as before:\\nparallel_policy  = train_policy_parallel (environment )\\nevaluate_policy (environment , parallel_policy )\\nThe result of those two lines is the same as before, when we ran the serial version of\\nthe RL training for the maze. We hope you appreciate how train_policy_parallel\\nhas the same high-level structure as train_policy . It’s a good exercise to compare the\\ntwo line-by-line.\\nEssentially, all it took to parallelize the training process was to use the ray.remote\\ndecorator on a class in a suitable way and then use the right remote  calls. Of course,\\nyou need some experience to get this right. But notice how little time we spent on\\nthinking about distributed computing and how much time we could spend on the\\nactual application code. We didn’t need to adopt an entirely new programming para‐\\ndigm and could simply approach the problem in the most natural way. Ultimately,\\nthat’s what you want—and Ray is great at giving you this kind of flexibility.\\nTo wrap things up, let’s have a quick look at the execution graph of the Ray applica‐\\ntion that we’ve just built. Figure 3-1  summarizes this task graph in a compact way.\\nThe running example in this chapter is an implementation of the\\npseudocode example used to illustrate the flexibility of Ray in the\\ninitial paper  by its creators. That paper has a figure similar to\\nFigure 3-1  and is worth reading for context.\\n64 | Chapter 3: Building Your First Distributed Application\\n\\nFigure 3-1. Parallel training of a reinforcement learning policy with Ray\\nBuilding a Distributed Ray App | 65\\n\\nRecapping RL Terminology\\nBefore we wrap up this chapter, let’s discuss the concepts we’ve encountered in the\\nmaze example in a broader context. Doing so will prepare you for more complex RL\\nsettings in the next chapter and show you where we simplified things a little for this\\nchapter’s running example. If you know RL well enough, you can skip this section.\\nEvery RL problem starts with the formulation of an environment , which describes the\\ndynamics of the “game” you want to play. The environment hosts a player or agent\\nthat interacts with its environment through a simple interface. The agent can request\\ninformation from the environment, namely, its current state  within the environment,\\nthe reward  it has received in this state, and whether the game is done  or not. In\\nobserving states and rewards, the agent can learn to make decisions based on the\\ninformation it receives.  Specifically, the agent will emit an action  that can be executed\\nby the environment by taking the next step .\\nThe mechanism used by an agent to produce actions for a given state is called a\\npolicy , and we sometimes say that the agent follows a given policy. Given a policy,\\nwe can simulate or roll out  a few steps or an entire game using that policy. During a\\nrollout we can collect experiences , which collect information about the current state\\nand reward, the next action, and the resulting state.  An entire sequence of steps from\\nstart to finish is referred to as an episode , and the environment can be reset  to its\\ninitial state to start a new episode.\\nThe policy we used in this chapter was based on the simple idea of tabulating\\nstate-action values  (also called Q-values ), and the algorithm used to update the policy\\nfrom the experiences collected during rollouts is called Q-Learning . More generally,\\nyou can consider the state-action table we implemented as the model  used by the\\npolicy.  In the next chapter you will see examples of more complex models, such as a\\nneural network to learn state-action values. The policy can decide to exploit  what it\\nhas learned about the environment by choosing the best available value of its model\\nor explore  the environment by choosing a random action.\\nMany of the basic concepts introduced here hold for any RL problem, but we’ve made\\na few simplifying assumptions. For instance, there could be multiple agents  acting in\\nthe environment (imagine having multiple seekers competing for reaching the goal\\nfirst), and we’ll look into so-called  multi-agent environments and multi-agent RL and\\nin the next chapter. Also, we assumed that the action space  of an agent was discrete ,\\nmeaning that the agent could take only a fixed set of actions.  Y ou can, of course,\\nalso have continuous  action spaces, and the cart–pole example from Chapter 1  is one\\nexample of this. Especially when you have multiple agents, action spaces can be more\\ncomplicated, and you might need tuples of actions or even to nest them accordingly.\\nThe observation space  we’ve considered for the maze game was also quite simple and\\nwas modeled as a discrete set of states. Y ou can easily imagine that complex agents\\n66 | Chapter 3: Building Your First Distributed Application\\n\\nlike robots interacting with their environments might work with image or video data\\nas observations, which would require a more complex observation space too.\\nAnother crucial assumption we made is that the environment is deterministic , mean‐\\ning that when our agent chose to take an action, the resulting state would always\\nreflect that choice. In general environments this is not the case, and there can be\\nelements of randomness at play in the environment. For instance, we could have\\nimplemented a coin flip in the maze game, and whenever tails came up, the agent\\nwould get pushed in a random direction. In that scenario, we couldn’t have planned\\nahead like we did in this chapter because actions would not deterministically lead to\\nthe same next state every time. To reflect this probabilistic behavior, in general we\\nhave to account for state transition probabilities  in our RL experiments.\\nThe last simplifying assumption I’ d like to talk about here is that we’ve been treating\\nthe environment and its dynamics as a game that can be perfectly simulated.  But the\\nfact is that some physical systems can’t be faithfully simulated. In that case you might\\nstill interact with this physical environment through an interface like the one we\\ndefined in our Environment  class, but there would be some communication overhead\\ninvolved. In practice, reasoning  about RL problems as if they were games takes very\\nlittle away from the experience.\\nSummary\\nTo recap, we’ve implemented a simple maze problem in plain Python and then solved\\nthe task of finding the goal in that maze using a straightforward reinforcement\\nlearning algorithm. We then took this solution and ported it to a distributed Ray\\napplication in roughly 25 lines of code. We did so without having to plan up front\\nhow to work with Ray—we simply used the Ray API to parallelize our Python code.\\nThis example shows how Ray gets out of your way and lets you focus on your\\napplication code. It also demonstrates how custom workloads that use advanced\\ntechniques like RL can be efficiently implemented and distributed with Ray.\\nIn Chapter 4 , you’ll build on what you’ve learned here and see how easy it is to solve\\nour maze problem directly with the higher-level Ray RLlib library.\\nSummary | 67\\n\\n\\n\\n1We’re using a simple game to illustrate the process of RL. There is a multitude of interesting industry\\napplications of RL that are not games.CHAPTER 4\\nReinforcement Learning with Ray RLlib\\nIn Chapter 3  you built an RL environment, a simulation to play out some games,\\nan RL algorithm, and the code to parallelize the training of the algorithm—all\\ncompletely from scratch. It’s good to know how to do all that, but in practice the\\nonly thing you really want to do when training RL algorithms is the first part,\\nnamely, specifying your custom environment, the “game” you want to play.1 Most of\\nyour efforts will go into selecting the right algorithm, setting it up, finding the best\\nparameters for the problem, and generally focusing on training a well-performing\\npolicy.\\nRay RLlib is an industry-grade library for building RL algorithms at scale. Y ou’ve\\nalready seen a first example of RLlib in Chapter 1 , but in this chapter we’ll go\\ninto much more depth. The great thing about RLlib is that it’s a mature library for\\ndevelopers that comes with good abstractions to work with. As you will see, many of\\nthese abstractions you already know from the previous chapter.\\nWe start out by giving you an overview of RLlib’s capabilities. Then we quickly revisit\\nthe maze game from Chapter 3  and show you how to tackle it both with the RLlib\\nCLI and the RLlib Python API in a few lines of code. Y ou’ll see how easy RLlib is to\\nget started before learning about its key concepts, such as RLlib environments and\\nalgorithms.\\nWe’ll also take a closer look at some advanced RL topics that are extremely useful in\\npractice but are not often properly supported in other RL libraries.  For instance, you\\nwill learn how to create a curriculum for your RL agents so that they can learn simple\\nscenarios before moving on to more complex ones. Y ou will also see how RLlib deals\\n69\\n\\n2We don’t cover this integration in this book, but you can learn more about deploying RLlib models in the\\n“Serving RLlib Models” tutorial  in the Ray documentation.with having multiple agents in a single environment and how to leverage experience\\ndata that you’ve collected outside your current application to improve your agent’s\\nperformance.\\nAn Overview of RLlib\\nBefore we dive into any examples, let’s quickly discuss what RLlib is and what it can\\ndo. As part of the Ray ecosystem, RLlib inherits all the performance and scalability\\nbenefits of Ray. In particular, RLlib is distributed by default, so you can scale your RL\\ntraining to as many nodes as you want.\\nAnother benefit of being built on top of Ray is that RLlib integrates tightly with other\\nRay libraries. For instance, the hyperparameters of any RLlib algorithm can be tuned\\nwith Ray Tune, as we will see in Chapter 5 . Y ou can also seamlessly deploy your  RLlib\\nmodels with Ray Serve.2\\nWhat’s extremely useful is that RLlib works with both of the predominant deep\\nlearning frameworks at the time of this writing: PyTorch and TensorFlow. Y ou can\\nuse either one of them as your backend and can easily switch between them, often by\\nchanging just one line of code. That’s a huge benefit, as companies are often locked\\ninto their underlying deep learning framework and can’t afford to switch to another\\nsystem and rewrite their code.\\nRLlib also has a track record of solving real-world problems and is a mature library\\nused by many companies to bring their RL workloads to production. The RLlib\\nAPI appeals to many engineers, as it offers the right level of abstraction for many\\napplications while still being flexible enough to be extended.\\nApart from these more general benefits, RLlib has a lot of RL-specific features that\\nwe will cover in this chapter. In fact, RLlib is so feature rich that it would deserve\\na book on its own, which means we can touch on just some aspects of it here. For\\ninstance, RLlib has a rich library of advanced RL algorithms to choose from. In this\\nchapter we will focus on a few select ones, but you can track the growing list of\\noptions on the RLlib algorithms page . RLlib also has many options for specifying RL\\nenvironments and is very flexible in handling them during training; for an overview\\nof RLlib environments see the documentation .\\n70 | Chapter 4: Reinforcement Learning with Ray RLlib\\n\\nGetting Started with RLlib\\nTo use RLlib, make sure you have installed it on your computer:\\npip install \"ray[rllib]==2.2.0\"\\nCheck out the accompanying notebook for this chapter  if you don’t\\nfeel like typing the code yourself.\\nEvery RL problem starts with having an interesting environment to investigate. In\\nChapter 1  we looked at the classical cart–pole balancing problem. Recall that we\\ndidn’t implement this cart–pole environment; it came out of the box with RLlib.\\nIn contrast, in Chapter 3  we implemented a simple maze game on our own. The\\nproblem with this implementation is that we can’t directly use it with RLlib or any\\nother RL library for that matter. The reason is that in RL you have ubiquitous\\nstandards, and our environments need to implement certain interfaces.  The best\\nknown and most widely used library for RL environments is gym, an open source\\nPython project  from OpenAI.\\nLet’s have a look at what Gym is and how to convert our maze Environment  from the\\nprevious chapter to a Gym environment compatible with RLlib.\\nBuilding a Gym Environment\\nIf you look at the well-documented and easy-to-read gym.Env  environment interface\\non GitHub , you’ll notice that an implementation of this interface has two mandatory\\nclass variables and three methods that subclasses need to implement. Y ou don’t have\\nto check the source code, but we do encourage you to have a look. Y ou might just be\\nsurprised by how much you already know about these environments.\\nIn short, the interface of a Gym environment looks like the following pseudocode:\\nimport gym\\nclass Env:\\n    action_space : gym.spaces.Space\\n    observation_space : gym.spaces.Space  \\n    def step(self, action):  \\n        ...\\n    def reset(self):  \\nGetting Started with RLlib | 71\\n\\n        ...\\n    def render(self, mode=\"human\"):  \\n        ...\\nThe gym.Env  interface has an action and an observation space.\\nThe Env can run a step  and returns a tuple of observations, reward, done\\ncondition, and further info.\\nAn Env can reset  itself, which will return the initial observations of a new\\nepisode.\\nWe can render  an Env for different purposes, such as for human display or as a\\nstring representation.\\nY ou’ll recall from Chapter 3  that this is very similar to the interface of the maze\\nEnvironment  we built there. In fact, Gym has a so-called Discrete  space imple‐\\nmented in gym.spaces , which means we can make our maze Environment  a gym.Env\\nas follows. We assume that you store this code in a file called maze_gym_env.py  and\\nthat the code for the Environment  from Chapter 3  is located at the top of that file (or\\nis imported there):\\n# maze_gym_env.py  | Original definition of Environment goes at the top.\\nimport gym\\nfrom gym.spaces  import Discrete   \\nclass GymEnvironment (Environment , gym.Env):  \\n    def __init__ (self, *args, **kwargs):\\n        \"\"\"Make our original Environment a gym `Env`.\"\"\"\\n        super().__init__ (*args, **kwargs)\\ngym_env = GymEnvironment ()\\nReplace our own Discrete  implementation with that of Gym.\\nMake the GymEnvironment  implement a gym.Env . The interface is essentially the\\nsame as before.\\nOf course, we could have made our original Environment  implement gym.Env  by\\nsimply inheriting from it in the first place. But the point is that the gym.Env  interface\\n72 | Chapter 4: Reinforcement Learning with Ray RLlib\\n\\n3From Ray 2.3.0 onward, RLlib will be using the Gymnasium library as drop-in replacement for Gym. This will\\nlikely introduce some breaking changes, so it’s best to stick with Ray 2.2.0 to follow this chapter.\\n4Gym comes with a variety of interesting environments that are worth exploring. For instance, you can find\\nmany of the Atari environments that were used in the famous “Playing Atari with Deep Reinforcement\\nLearning” paper  from DeepMind, or advanced physics simulations using the MuJoCo engine.comes up so naturally in the context of RL that it is a good exercise to implement it\\nwithout having to resort to external libraries.3\\nThe gym.Env  interface also comes with helpful utility functionality and many interest‐\\ning example implementations. For instance, the CartPole-v1  environment we used\\nin Chapter 1  is an example from Gym,4 and there are many other environments\\navailable to test your RL algorithms.\\nRunning the RLlib CLI\\nNow that we have our GymEnvironment  implemented as a gym.Env , here’s how you\\ncan use it with RLlib. Y ou’ve seen the RLlib CLI in action in Chapter 1 , but this time\\nthe situation is a bit different. In the first chapter we simply ran a tuned example\\nusing the rllib example  command.\\nThis time around we want to bring our own gym environment class, namely, the class\\nGymEnvironment  that we defined in maze_gym_env.py . To specify this class in Ray\\nRLlib, you use the full qualifying name of the class from where you’re referencing it,\\nso in our case that’s maze_gym_env.GymEnvironment . If you had a more complicated\\nPython project and your environment was stored in another module, you’ d simply\\nadd the module name accordingly.\\nThe following Python file specifies the minimal configuration needed to train an\\nRLlib algorithm on the GymEnvironment  class. To align as closely as possible with our\\nexperiment from Chapter 3 , in which we used Q-Learning, we use a DQNConfig  to\\ndefine a DQN algorithm and store it in a file called maze.py :\\nfrom ray.rllib.algorithms.dqn  import DQNConfig\\nconfig = DQNConfig ().environment (\"maze_gym_env.GymEnvironment\" )\\\\\\n    .rollouts (num_rollout_workers =2)\\nThis gives a quick preview of RLlib’s Python API, which we cover in the next section.\\nTo run this with RLlib, we’re using the rllib train  command. We do this by\\nspecifying the file  we want to run: maze.py . To make sure we can control the time of\\ntraining, we tell our algorithm to stop  after running for a total of 10,000 time steps\\n(timesteps_total ):\\n rllib train file maze.py --stop \\'{\"timesteps_total\": 10000}\\'\\nGetting Started with RLlib | 73\\n\\n5To be precise, RLlib uses a double and dueling DQN.\\n6In the GitHub repo for this book we’ve also included an equivalent maze.yml  file that you could use via rllib\\ntrain file maze.yml  (no --type  needed).This single line takes care of everything we did in Chapter 3 , but in a better way:\\n•It runs a more sophisticated version of Q-Learning for us (DQN).5•\\n•It takes care of scaling out to multiple workers under the hood (in this case two).•\\n•It even creates checkpoints of the algorithm automatically for us.•\\nFrom the output of that training script you should see that Ray will write training\\nresults to a directory located at ~/ray_results/maze_env . And if the training run\\nfinishes successfully,6 you’ll get a checkpoint and a copiable rllib evaluate  com‐\\nmand in the output, just as in the example from Chapter 1 . Using this reported\\n<checkpoint> , you can now evaluate the trained policy on our custom environment\\nby running the following command:\\nrllib evaluate  ~/ray_results/maze_env/<checkpoint> \\\\\\n  --algo DQN\\\\\\n  --env maze_gym_env.Environment \\\\\\n  --steps 100\\nThe algorithm used in --algo  and the environment specified with --env  have to\\nmatch the ones used in the training run, and we evaluate the trained algorithm for a\\ntotal of 100 steps. This should lead to output of the following form:\\nEpisode #1: reward: 1.0\\nEpisode #2: reward: 1.0\\nEpisode #3: reward: 1.0\\n...\\nEpisode #13: reward: 1.0\\nIt should not come as a big surprise that the DQN algorithm from RLlib gets the\\nmaximum reward of 1 for the simple maze environment we tasked it with every\\nsingle time.\\nBefore moving on to the Python API, we should mention that the RLlib CLI uses Ray\\nTune under the hood, for instance, to create the checkpoints of your algorithms.  Y ou\\nwill learn more about this integration in Chapter 5 .\\n74 | Chapter 4: Reinforcement Learning with Ray RLlib\\n\\n7Of course, configuring your models is a crucial part of RL experiments. We will discuss configuration of RLlib\\nalgorithms in more detail in the next section.Using the RLlib Python API\\nIn the end, the RLlib CLI is merely a wrapper around its underlying Python library.\\nAs you will likely spend most of your time coding your RL experiments in Python,\\nwe’ll focus the rest of this chapter on aspects of this API.\\nTo run RL workloads with RLlib from Python, the Algorithm  class is your main\\nentry point. Always start with a corresponding AlgorithmConfig  class to define an\\nalgorithm.  For instance, in the previous section we used a DQNConfig  as a starting\\npoint, and the rllib train  command took care of instantiating the DQN algorithm\\nfor us. All other RLlib algorithms follow the same pattern.\\nTraining RLlib algorithms\\nEvery RLlib Algorithm  comes with reasonable default parameters, meaning that you\\ncan initialize them without having to tweak any configuration parameters for these\\nalgorithms.7\\nThat said, it’s worth noting that RLlib algorithms are highly configurable, as you\\nwill see in the following example. We start by creating a DQNConfig  object. Then\\nwe specify its environment  and set the number of rollout workers to two by using\\nthe rollouts  method.  This means that the DQN algorithm will spawn two Ray\\nactors, each using a CPU by default, to run the algorithm in parallel. Also, for later\\nevaluation purposes, we set create_env_on_local_worker  to True :\\nfrom ray.tune.logger  import pretty_print\\nfrom maze_gym_env  import GymEnvironment\\nfrom ray.rllib.algorithms.dqn  import DQNConfig\\nconfig = (DQNConfig ().environment (GymEnvironment )  \\n          .rollouts (num_rollout_workers =2, create_env_on_local_worker =True))\\npretty_print (config.to_dict())\\nalgo = config.build()  \\nfor i in range(10):\\n    result = algo.train()  \\nprint(pretty_print (result))  \\nGetting Started with RLlib | 75\\n\\n8If you set num_rollout_workers  to 0, only the local worker on the head node will be created, and all sampling\\nfrom the env is done there. This is particularly useful for debugging, as no additional Ray actor processes are\\nspawned.\\nSet the environment  to our custom GymEnvironment  class and configure the\\nnumber of rollout workers and ensure that an environment instance is created on\\nthe local worker.\\nUse the DQNConfig  from RLlib to build  a DQN algorithm for training. This time\\nwe use two rollout workers.\\nCall the train  method to train the algorithm for 10 iterations.\\nWith the pretty_print  utility, we can generate human-readable output of the\\ntraining results.\\nNote that the number of training iterations has no special meaning, but it should be\\nenough for the algorithm to learn to solve the maze problem adequately. The example\\njust goes to show that you have full control over the training process.\\nFrom printing the config  dictionary, you can verify that the num_rollout_workers\\nparameter is set to 2.8 The result  contains detailed information about the state of\\nthe DQN algorithm and the training results, which are too verbose to show here.\\nThe part that’s most relevant for us right now is information about the reward of\\nthe algorithm, which ideally indicates that the algorithm learned to solve the maze\\nproblem. Y ou should see output of the following form (we’re showing only the most\\nrelevant information for clarity):\\n...\\nepisode_reward_max: 1.0\\nepisode_reward_mean: 1.0\\nepisode_reward_min: 1.0\\nepisodes_this_iter: 15\\nepisodes_total: 19\\n...\\ntraining_iteration: 10\\n...\\nIn particular, this output shows that the minimum reward attained on average per\\nepisode is 1.0, which in turn means that the agent always reached the goal and\\ncollected the maximum reward (1.0).\\n76 | Chapter 4: Reinforcement Learning with Ray RLlib\\n\\nSaving, loading, and evaluating RLlib models\\nReaching the goal for this simple example isn’t too difficult, but let’s see if evaluating\\nthe trained algorithm confirms that the agent can also do so in an optimal way,\\nnamely, by taking only the minimum number of eight steps to reach the goal.\\nTo do so, we utilize another mechanism that you’ve already seen from the RLlib CLI:\\ncheckpointing . Creating algorithm checkpoints is useful to ensure you can recover\\nyour work in case of a crash or simply to track training progress persistently. Y ou can\\ncreate a checkpoint of an RLlib algorithm at any point in the training process by call‐\\ning algo.save() . Once you have a checkpoint, you can easily restore your Algorithm\\nwith it. Evaluating a model is as simple as calling algo.evaluate(checkpoint)  with\\nthe checkpoint you created. Here’s how that looks if you put it all together:\\nfrom ray.rllib.algorithms.algorithm  import Algorithm\\ncheckpoint  = algo.save()  \\nprint(checkpoint )\\nevaluation  = algo.evaluate ()  \\nprint(pretty_print (evaluation ))\\nalgo.stop()  \\nrestored_algo  = Algorithm .from_checkpoint (checkpoint )  \\nSave algorithms to create checkpoints.\\nEvaluate RLlib algorithms at any point in time by calling evaluate .\\nStop an algo  to free all claimed resources.\\nRestore any Algorithm  from a given checkpoint with from_checkpoint .\\nLooking at the output of this example, we can now confirm that the trained RLlib\\nalgorithm did indeed converge to a good solution for the maze problem, as indicated\\nby episodes of length 8 in evaluation:\\n~/ray_results/DQN_GymEnvironment_2022-02-09_10-19-301o3m9r6d/checkpoint_000010/\\ncheckpoint-10 evaluation:\\n  ...\\n  episodes_this_iter: 5\\n  hist_stats:\\n    episode_lengths:\\n    - 8\\n    - 8\\n    ...\\nGetting Started with RLlib | 77\\n\\nComputing actions\\nRLlib algorithms have much more functionality than just the train , evaluate , save ,\\nand from_checkpoint  methods we’ve seen so far.  For example, you can directly com‐\\npute actions given the current state of an environment. In Chapter 3  we implemented\\nepisode rollouts by stepping through an environment and collecting rewards. We can\\neasily do the same with RLlib for our GymEnvironment :\\nenv = GymEnvironment ()\\ndone = False\\ntotal_reward  = 0\\nobservations  = env.reset()\\nwhile not done:\\n    action = algo.compute_single_action (observations )  \\n    observations , reward, done, info = env.step(action)\\n    total_reward  += reward\\nTo compute actions for given observations , use compute_single_action .\\nIn case you should need to compute many actions at once, not just a single one,\\nyou can use the compute_actions  method instead, which takes dictionaries of obser‐\\nvations as input and produces dictionaries of actions with the same dictionary  keys as\\noutput:\\naction = algo.compute_actions (  \\n    {\"obs_1\": observations , \"obs_2\": observations }\\n)\\nprint(action)\\n# {\\'obs_1\\': 0, \\'obs_2\\': 1}\\nFor multiple actions, use compute_actions .\\nAccessing policy and model states\\nRemember that each reinforcement learning algorithm is based on a policy  that\\nchooses  next actions given the agent’s current observations of the environment. Each\\npolicy is in turn based on an underlying model .\\nIn the case of vanilla Q-Learning that we discussed in Chapter 3 , the model was a\\nsimple lookup table of state-action values, also called Q-values. And that policy used\\nthis model for predicting next actions in case it decided to exploit  what the model had\\nlearned so far or to explore  the environment with random actions otherwise.\\nWhen using Deep Q-Learning, the underlying model of the policy is a neural net‐\\nwork that, loosely speaking, maps observations to actions.  Note that for choosing\\nnext actions in an environment, we’re ultimately not interested in the concrete values\\nof the approximated Q-values, but rather in the probabilities  of taking each action.\\nThe probability distribution over all possible actions is called an action distribution .\\n78 | Chapter 4: Reinforcement Learning with Ray RLlib\\n\\n9The Policy  class in RLlib today will be replaced in a future release. The new Policy  class will likely be a\\ndrop-in replacement for the most part and exhibit some minor differences. The idea of the class remains the\\nsame, though: a policy  is a class that encapsulates the logic of choosing actions given observations, and it gives\\nyou access to the underlying models used.\\n10Technically speaking, only the local  model is used for actual training. The two worker models are used for\\naction computation and data collection (rollouts). After each training step, the local model sends its current\\nweights to the workers for synchronization. Fully distributed training, as opposed to distributed sampling,\\nwill be available across all RLlib algorithms in future Ray versions.\\n11This is true by default, since we’re using TensorFlow and Keras under the hood. Should you opt to change\\nthe framework  specification of your algorithm to work with PyTorch directly, do print(model) , in which case\\nmodel  is-a torch.nn.Module . Access to the underlying model will be unified across all frameworks the future.In the maze we’re using as a running example, we can move up, right, down, or left.\\nSo, in our case an action distribution is a vector of four probabilities, one for each\\naction. In the case of Q-Learning, the algorithm will always greedily  choose the action\\nwith the highest probability of this distribution, while other algorithms will sample\\nfrom it.\\nTo make things concrete, let’s look at how you access policies and models in RLlib:9\\npolicy = algo.get_policy ()\\nprint(policy.get_weights ())\\nmodel = policy.model\\nBoth policy  and model  have many useful methods to explore. In this example we use\\nget_weights  to inspect the parameters of the model underlying the policy (which are\\ncalled weights  by standard convention).\\nTo convince you that not just one model is at play here but in fact a collection of\\nmodels,10 we can access all the workers we used in training and then ask each worker’s\\npolicy for their weights using foreach_worker :\\nworkers = algo.workers\\nworkers.foreach_worker (\\n    lambda remote_trainer : remote_trainer .get_policy ().get_weights ()\\n)\\nIn this way, you can access every method available on an Algorithm  instance on each\\nof your workers. In principle, you can use this to set model parameters as well, or\\notherwise configure your workers. RLlib workers are ultimately Ray actors, so you\\ncan alter and manipulate them in almost any way you like.\\nWe haven’t talked about the specific implementation of Deep Q-Learning used in\\nDQN, but the model used is a bit more complex than what we’ve described so far.\\nEvery RLlib model obtained from a policy has a base_model  that has a neat summary\\nmethod to describe itself:11\\nmodel.base_model .summary()\\nGetting Started with RLlib | 79\\n\\n12The “value” output of this network represents the Q-value of state-action pairs.\\n13To learn more about customizing your RLlib models, check out the guide to custom models in the Ray\\ndocumentation .As you can see from the following output, this model takes in our observations .\\nThe shape of these observations  is a bit strangely annotated as [(None, 25)] , but\\nessentially this means we have the expected 5 × 5 maze grid values correctly encoded.\\nThe model follows with two so-called Dense  layers and predicts a single value at the\\nend:12\\nModel: \"model\"\\n________________________________________________________________________________\\nLayer (type)                  Output Shape       Param #     Connected to\\n================================================================================\\nobservations (InputLayer)     [(None, 25)]       0\\n________________________________________________________________________________\\nfc_1 (Dense)                  (None, 256)        6656        observations[0][0]\\n________________________________________________________________________________\\nfc_out (Dense)                (None, 256)        65792       fc_1[0][0]\\n________________________________________________________________________________\\nvalue_out (Dense)             (None, 1)          257         fc_1[0][0]\\n================================================================================\\nTotal params: 72,705\\nTrainable params: 72,705\\nNon-trainable params: 0\\n________________________________________________________________________________\\nNote that it’s perfectly possible to customize this model for your RLlib experiments.\\nIf your environment is complex and has a big observation space, for instance, you\\nmight need a bigger model to capture that complexity. However, doing so requires in-\\ndepth knowledge of the underlying neural network framework (in this case Tensor‐\\nFlow), which we don’t assume you have.13\\nState-Action Values and State-Value Functions\\nSo far we’ve been mostly concerned with the concept of state-action values, since this\\nconcept takes center stage in the formulation of Q-Learning that we used extensively\\nso far. The model we just had a look at has a dedicated output (in deep learning terms\\ncalled a head ) for predicting Q-values. Y ou can access and summarize this part of the\\nmodel through model.q_value_head.summary() .\\nIn contrast it’s also possible to ask how valuable a particular state  is, without specify‐\\ning an action that pairs with it. This leads to the concept of state-value functions ,\\nor simply value functions , that are very important in the RL literature.  We can’t go\\ninto more detail in this RLlib introduction, but note that you have access to a value\\nfunction head  as well through model.state_value_head.summary() .\\n80 | Chapter 4: Reinforcement Learning with Ray RLlib\\n\\nNext, let’s see if we can take some observations from our environment and pass them\\nto the model we just extracted from our policy . This part is a bit technically involved\\nbecause models are a bit more difficult to access directly in RLlib. Normally you\\nwould only interface with a model through your policy , which takes care of prepro‐\\ncessing the observations, among other things. Luckily, we can access the preprocessor\\nused by the policy, transform  the observations from our environment, and then pass\\nthem to the model:\\nfrom ray.rllib.models.preprocessors  import get_preprocessor\\nenv = GymEnvironment ()\\nobs_space  = env.observation_space\\npreprocessor  = get_preprocessor (obs_space )(obs_space )  \\nobservations  = env.reset()\\ntransformed  = preprocessor .transform (observations ).reshape(1, -1)  \\nmodel_output , _ = model({\"obs\": transformed })  \\nUse get_preprocessor  to access the preprocessor used by the policy.\\nY ou can use transform  on any observations  obtained from your env to the\\nformat expected by the model. Note that we need to reshape the observations too.\\nGet the model output by calling the model on a preprocessed observation\\ndictionary.\\nHaving computed our model_output , we can now access the Q-values and the action\\ndistribution of the model for this output:\\nq_values  = model.get_q_value_distributions (model_output )  \\nprint(q_values )\\naction_distribution  = policy.dist_class (model_output , model)  \\nsample = action_distribution .sample()  \\nprint(sample)\\nThe get_q_value_distributions  method is specific to DQN models only.\\nBy accessing dist_class  we get the policy’s action distribution class.\\nAction distributions can be sampled from.\\nGetting Started with RLlib | 81\\n\\n14We list only the methods we introduce in this chapter. Apart from those we mention, you also find options\\nfor evaluation  of your algorithms, reporting , debugging , checkpointing , adding callbacks , altering your\\ndeep learning framework , requesting resources , and accessing experimental  features.Configuring  RLlib Experiments\\nNow that you’ve seen the basic Python training API of RLlib in an example, let’s\\ntake a step back and discuss in more depth how to configure and run RLlib experi‐\\nments.  By now you know that to define an Algorithm , you start with the respective\\nAlgorithmConfig  and then build  your algorithm from it. So far we’ve used only the\\nrollout  method of an AlgorithmConfig  to set the number of rollout workers to two,\\nand set our environment  accordingly.\\nIf you want to alter the behavior of your RLlib training run, chain more utility meth‐\\nods onto the AlgorithmConfig  instance and then call build  on it at the end. As RLlib\\nalgorithms are fairly complex, they come with many configuration options. To make\\nthings easier, the common properties of algorithms are naturally grouped into useful\\ncategories.14 Each such category comes with its own respective AlgorithmConfig\\nmethod:\\ntraining()\\nTakes care of all training-related configuration options of your algorithm. The\\ntraining  method is the one place that RLlib algorithms differ in their configura‐\\ntion. All the following methods are algorithm-agnostic.\\nenvironment()\\nConfigures all aspects of your environment.\\nrollouts()\\nModifies the setup and behavior of your rollout workers.\\nexploration()\\nAlters the behavior of your exploration strategy.\\nresources()\\nConfigures the compute resources used by your algorithm.\\noffline_data()\\nDefines options for training with so-called offline data, a topic we cover in\\n“Working with Offline Data” on page 97 .\\nmulti_agent()\\nSpecifies options for training algorithms using multiple agents . We discuss an\\nexplicit example of this in the next section.\\n82 | Chapter 4: Reinforcement Learning with Ray RLlib\\n\\nThe algorithm-specific configuration in training()  becomes even more relevant\\nonce you’ve settled on an algorithm and want to squeeze it for performance. In\\npractice, RLlib provides you with good defaults to get started.\\nFor more details on configuring RLlib experiments, look up configuration arguments\\nin the API reference for RLlib algorithms . But before we move on to examples, you\\nshould learn about the most common configuration options in practice.\\nResource Configuration\\nWhether you use Ray RLlib locally or on a cluster, you can specify the resources\\nused for the training process. Here are the most important options to consider. We\\ncontinue using the DQN algorithm as an example, but this would apply to any other\\nRLlib algorithm as well:\\nfrom ray.rllib.algorithms.dqn  import DQNConfig\\nconfig = DQNConfig ().resources (\\n    num_gpus =1,  \\n    num_cpus_per_worker =2, \\n    num_gpus_per_worker =0, \\n)\\nSpecify the number of GPUs to use for training. It’s important to check whether\\nyour algorithm of choice supports GPUs first. This value can also be fractional.\\nFor example, if using four rollout workers in DQN ( num_rollout_workers=4 ),\\nyou can set num_gpus=0.25  to pack all four workers on the same GPU so that\\nall rollout workers benefit from the potential speedup. This affects only the local\\nlearner process, not the rollout workers.\\nSet the number of CPUs to use for each rollout worker.\\nSet the number of GPUs used per worker.\\nRollout Worker Configuration\\nRLlib lets you configure how your rollouts are computed and how to distribute them:\\nfrom ray.rllib.algorithms.dqn  import DQNConfig\\nconfig = DQNConfig ().rollouts (\\n    num_rollout_workers =4,  \\n    num_envs_per_worker =1, \\n    create_env_on_local_worker =True, \\n)\\nY ou’ve seen this already. It specifies the number of Ray workers to use.\\nConfiguring  RLlib Experiments | 83\\n\\n15There’s also a way to register  your environments so that you can refer to them by name, but this requires using\\nRay Tune. Y ou will learn about this feature in Chapter 5 .\\nSpecify the number of environments to evaluate per worker. This setting allows\\nyou to “batch” evaluation of environments. In particular, if your models take a\\nlong time to evaluate, grouping environments like this can speed up training.\\nWhen num_rollout_workers  > 0, the driver (“local worker”) does not need an\\nenvironment. That’s because sampling and evaluation is done by the rollout\\nworkers. If you still want an environment on the driver, you can set this option to\\nTrue .\\nEnvironment Configuration\\nfrom ray.rllib.algorithms.dqn  import DQNConfig\\nconfig = DQNConfig ().environment (\\n    env=\"CartPole-v1\" ,  \\n    env_config ={\"my_config\" : \"value\"}, \\n    observation_space =None,\\n    action_space =None, \\n    render_env =True, \\n)\\nSpecify the environment you want to use for training. This can be either a string\\nof an environment known to Ray RLlib, such as any Gym environment, or the\\nclass name of a custom environment you’ve implemented.15\\nOptionally specify a dictionary of configuration options for your environment\\nthat will be passed to the environment constructor.\\nY ou can specify the observation and action spaces of your environment too. If\\nyou don’t specify them, they will be inferred from the environment.\\nFalse  by default, this property allows you to turn on rendering of the environ‐\\nment, which requires you to implement the render  method of your environment.\\nNote that we left out many available configuration options for each of the types we\\nlisted.  On top of that, we can’t touch on aspects here that alter the behavior of the\\nRL training procedure in this introduction (like modifying the underlying model to\\nuse).  But the good news is that you’ll find all the information you need in the RLlib\\nTraining API documentation .\\n84 | Chapter 4: Reinforcement Learning with Ray RLlib\\n\\nWorking with RLlib Environments\\nSo far we’ve introduced you to just Gym environments, but RLlib supports a wide\\nvariety of environments. After giving you a quick overview of all available options\\n(see Figure 4-1 ), we’ll show you two concrete examples of advanced RLlib environ‐\\nments in action.\\nAn Overview of RLlib Environments\\nAll available RLlib environments extend a common BaseEnv  class. If you want to\\nwork with several copies of the same gym.Env  environment, you can use RLlib’s\\nVectorEnv  wrapper. Vectorized environments are useful, but they are straightforward\\ngeneralizations of what you’ve seen already. The two other types of environments\\navailable in RLlib are more interesting and deserve more attention.\\nFigure 4-1. An overview of all available RLlib environments\\nThe first is called MultiAgentEnv , which allows you to train a model with multiple\\nagents . Working with multiple agents can be tricky. That’s because you have to take\\ncare to define your agents within your environment with a suitable interface and\\naccount for the fact that each agent might have a completely different way of interact‐\\ning with its environment.\\nWhat’s more is that agents might interact with each other, and they have to respect\\neach other’s actions. In more advanced settings, there might even be a hierarchy\\nof agents that explicitly depend on each other. In short, running multi-agent RL\\nexperiments is difficult, and we’ll see how RLlib handles this in the next example.\\nThe other type of environment we will look at is called ExternalEnv , which can\\nbe used to connect external simulators to RLlib. For instance, imagine our simple\\nmaze problem from earlier was a simulation of an actual robot navigating a maze.\\nIt might not be suitable in such scenarios to co-locate the robot (or its simulation,\\nimplemented in a different software stack) with RLlib’s learning agents. To account\\nfor that, RLlib provides you with a simple client-server architecture for communicat‐\\ning with external simulators, which allows communication over a REST API. In case\\nWorking with RLlib Environments | 85\\n\\nyou want to work both in a multi-agent and external environment setting, RLlib\\noffers a MultiAgentExternalEnv  environment that combines both.\\nWorking with Multiple Agents\\nThe basic idea of defining multi-agent environments in RLlib is simple. Y ou first\\nassign each agent an agent ID. Then, whatever you previously defined as a single\\nvalue in a Gym environment (observations, rewards, etc.), you now define as a dictio‐\\nnary with agent IDs as keys and values per agent. Of course, the details are a little\\nmore complicated than that in practice. But once you have defined an environment\\nhosting several agents, you have to define how these agents should learn.\\nIn a single-agent environment there’s one agent and one policy to learn. In a multi-\\nagent environment there are multiple agents that might map to one or several\\npolicies.  For instance, if you have a group of homogenous agents in your environ‐\\nment, then you could define a single policy for all of them. If they all act the same\\nway, then their behavior can be learned the same way. In contrast, you might have\\nsituations with heterogeneous agents in which each of them has to learn a separate\\npolicy. Between these two extremes, there’s a spectrum of possibilities, as shown in\\nFigure 4-2 .\\nWe continue to use our maze game as a running example for this chapter. This way\\nyou can check for yourself how the interfaces differ in practice. So, to put the ideas we\\njust outlined into code, let’s define a multi-agent version of the GymEnvironment  class.\\nOur MultiAgentEnv  class will have precisely two agents, which we encode in a Python\\ndictionary called agents , but in principle this works with any number of agents.\\nFigure 4-2. Mapping agents to policies in multi-agent reinforcement learning problems\\n86 | Chapter 4: Reinforcement Learning with Ray RLlib\\n\\n16Y ou can find a good example that defines different observation and action spaces for multiple agents in the\\nRLlib documentation .We start by initializing and resetting our new environment:\\nfrom ray.rllib.env.multi_agent_env  import MultiAgentEnv\\nfrom gym.spaces  import Discrete\\nimport os\\nclass MultiAgentMaze (MultiAgentEnv ):\\n    def __init__ (self,  *args, **kwargs):  \\n        self.action_space  = Discrete (4)\\n        self.observation_space  = Discrete (5*5)\\n        self.agents = {1: (4, 0), 2: (0, 4)}  \\n        self.goal = (4, 4)\\n        self.info = {1: {\\'obs\\': self.agents[1]}, 2: {\\'obs\\': self.agents[2]}}  \\n    def reset(self):\\n        self.agents = {1: (4, 0), 2: (0, 4)}\\n        return {1: self.get_observation (1), 2: self.get_observation (2)}  \\nAction and observation spaces stay exactly the same as before.\\nWe now have two seekers with (0, 4)  and (4, 0)  starting positions in an\\nagents  dictionary.\\nFor the info  object, we’re using agent IDs as keys.\\nObservations are now per-agent dictionaries.\\nNotice that we didn’t touch the action and observation spaces at all.  That’s because\\nwe’re using two essentially identical agents here that can reuse the same spaces. In\\nmore complex situations you’ d have to account for the fact that the actions and\\nobservations might look different for some agents.16\\nTo continue, let’s generalize our helper methods get_observation , get_reward , and\\nis_done  to work with multiple agents.  We do this by passing in an action_id  to their\\nsignatures and handling each agent the same way as before:\\n    def get_observation (self, agent_id ):\\n        seeker = self.agents[agent_id ]\\n        return 5 * seeker[0] + seeker[1]\\n    def get_reward (self, agent_id ):\\n        return 1 if self.agents[agent_id ] == self.goal else 0\\nWorking with RLlib Environments | 87\\n\\n17Note how this can lead to issues like deciding which agent gets to act first. In our simple maze problem the\\norder of actions is irrelevant, but in more complex scenarios this becomes a crucial part of modeling the RL\\nproblem correctly.    def is_done(self, agent_id ):\\n        return self.agents[agent_id ] == self.goal\\nNext, to port the step  method to our multi-agent setup, you have to know that\\nMultiAgentEnv  now expects the action  passed to a step  to be a dictionary with keys\\ncorresponding to the agent IDs, too.  We define a step by looping through all available\\nagents and acting on their behalf:17\\n    def step(self, action):  \\n        agent_ids  = action.keys()\\n        for agent_id  in agent_ids :\\n            seeker = self.agents[agent_id ]\\n            if action[agent_id ] == 0:  # move down\\n                seeker = (min(seeker[0] + 1, 4), seeker[1])\\n            elif action[agent_id ] == 1:  # move left\\n                seeker = (seeker[0], max(seeker[1] - 1, 0))\\n            elif action[agent_id ] == 2:  # move up\\n                seeker = (max(seeker[0] - 1, 0), seeker[1])\\n            elif action[agent_id ] == 3:  # move right\\n                seeker = (seeker[0], min(seeker[1] + 1, 4))\\n            else:\\n                raise ValueError (\"Invalid action\" )\\n            self.agents[agent_id ] = seeker  \\n        observations  = {i: self.get_observation (i) for i in agent_ids }  \\n        rewards = {i: self.get_reward (i) for i in agent_ids }\\n        done = {i: self.is_done(i) for i in agent_ids }\\n        done[\"__all__\" ] = all(done.values())  \\n        return observations , rewards, done, self.info\\nActions in a step  are now per-agent dictionaries.\\nAfter applying the correct action for each seeker, set the correct states of all\\nagents .\\nobservations , rewards , and dones  are also dictionaries with agent IDs as keys.\\nAdditionally, RLlib needs to know when all agents are done.\\n88 | Chapter 4: Reinforcement Learning with Ray RLlib\\n\\n18Deciding when an episode is done is a crucial part of multi-agent RL, and it depends entirely on the problem\\nat hand and what you want to achieve.The last step is to modify rendering the environment, which we do by denoting each\\nagent by its ID when printing the maze to the screen:\\n    def render(self, *args, **kwargs):\\n        os.system(\\'cls\\' if os.name == \\'nt\\' else \\'clear\\')\\n        grid = [[\\'| \\' for _ in range(5)] + [\"|\\\\n\"] for _ in range(5)]\\n        grid[self.goal[0]][self.goal[1]] = \\'|G\\'\\n        grid[self.agents[1][0]][self.agents[1][1]] = \\'|1\\'\\n        grid[self.agents[2][0]][self.agents[2][1]] = \\'|2\\'\\n        grid[self.agents[2][0]][self.agents[2][1]] = \\'|2\\'\\n        print(\\'\\'.join([\\'\\'.join(grid_row ) for grid_row  in grid]))\\nRandomly rolling out an episode until one of the agents reaches the goal can, for\\ninstance, be done by the following code:18\\nimport time\\nenv = MultiAgentMaze ()\\nwhile True:\\n    obs, rew, done, info = env.step(\\n        {1: env.action_space .sample(), 2: env.action_space .sample()}\\n    )\\n    time.sleep(0.1)\\n    env.render()\\n    if any(done.values()):\\n        break\\nNote how we have to make sure to pass two random samples by means of a Python\\ndictionary into the step  method, and how we check if any of the agents are done\\nyet. We use this break  condition for simplicity because it’s highly unlikely that both\\nseekers find their way to the goal at the same time by chance. But of course we’ d like\\nboth agents to complete the maze eventually.\\nIn any case, equipped with our MultiAgentMaze , training an RLlib Algorithm  works\\nexactly  the same way as before:\\nfrom ray.rllib.algorithms.dqn  import DQNConfig\\nsimple_trainer  = DQNConfig ().environment (env=MultiAgentMaze ).build()\\nsimple_trainer .train()\\nThis covers the simplest case of training a multi-agent reinforcement learning\\n(MARL) problem. But if you remember what we said earlier, when using multiple\\nagents, there’s always a mapping between agents and policies. By not specifying such\\na mapping, both of our seekers were implicitly assigned to the same policy.  This can\\nWorking with RLlib Environments | 89\\n\\nbe changed by calling the .multi_agent  method on our DQNConfig  and setting the\\npolicies  and policy_mapping_fn  arguments accordingly:\\nalgo = DQNConfig ()\\\\\\n    .environment (env=MultiAgentMaze )\\\\\\n    .multi_agent (\\n        policies ={  \\n            \"policy_1\" : (\\n                None, env.observation_space , env.action_space , {\"gamma\": 0.80}\\n            ),\\n            \"policy_2\" : (\\n                None, env.observation_space , env.action_space , {\"gamma\": 0.95}\\n            ),\\n        },\\n        policy_mapping_fn  = lambda agent_id : f\"policy_ {agent_id }\",  \\n    ).build()\\nprint(algo.train())\\nDefine multiple policies  for our agents, each with a different \"gamma\"  value.\\nEach agent can then be mapped to a policy with a custom policy_mapping_fn .\\nAs you can see, running multi-agent RL experiments is a first-class citizen of RLlib,\\nand there’s a lot more that could be said about it. The support of MARL problems is\\nprobably one of RLlib’s strongest features.\\nWorking with Policy Servers and Clients\\nFor the last example in this section, let’s assume our original GymEnvironment  can be\\nsimulated only on a machine that can’t run RLlib, for instance because it doesn’t have\\nenough resources available. We can run the environment on a PolicyClient  that\\ncan ask a respective server  for suitable next actions to apply to the environment. The\\nserver, in turn, does not know about the environment. It only knows how to ingest\\ninput data from a PolicyClient , and it is responsible for running all RL-related code;\\nin particular, it defines an RLlib AlgorithmConfig  object and trains an Algorithm .\\nTypically, you want to run the server that trains your algorithm on a powerful Ray\\nCluster, and then the respective client runs outside that cluster. Figure 4-3  schemati‐\\ncally illustrates this setup.\\n90 | Chapter 4: Reinforcement Learning with Ray RLlib\\n\\nFigure 4-3. Working with policy servers and clients in RLlib\\nDefining  a server\\nLet’s start by defining the server side of such an application first. We define a so-called\\nPolicyServerInput  that runs on localhost on port 9900. This policy input is what the\\nclient will provide later. With this policy_input  defined as input  to our algorithm\\nconfiguration, we can define yet another DQN to run on the server:\\n# policy_server.py\\nimport ray\\nfrom ray.rllib.agents.dqn  import DQNConfig\\nfrom ray.rllib.env.policy_server_input  import PolicyServerInput\\nimport gym\\nray.init()\\ndef policy_input (context):\\n    return PolicyServerInput (context, \"localhost\" , 9900)  \\nconfig = DQNConfig ()\\\\\\n    .environment (\\n        env=None,  \\n        action_space =gym.spaces.Discrete (4),  \\n        observation_space =gym.spaces.Discrete (5*5))\\\\\\n    .debugging (log_level =\"INFO\")\\\\\\n    .rollouts (num_rollout_workers =0)\\\\\\n    .offline_data (  \\n        input=policy_input ,\\n        input_evaluation =[])\\nalgo = config.build()\\nThe policy_input  function returns a PolicyServerInput  object running on\\nlocalhost on port 9900.\\nWorking with RLlib Environments | 91\\n\\n19For technical reasons, we have to specify observation and action spaces here, which might not be necessary in\\nfuture releases of RLlib, as it leaks environment information. Also note that we need to set input_evaluation\\nto an empty list to make this server work.\\nWe explicitly set the env to None  because this server does not need one.\\nWe therefore need to define both an observation_space  and an action_space ,\\nas the server is not able to infer them from the environment.\\nTo make this work, we need to feed our policy_input  into the experiment’s\\ninput .\\nWith this algo  defined,19 we can now start a training session on the server like so:\\n# policy_server.py\\nif __name__ == \"__main__\" :\\n    time_steps  = 0\\n    for _ in range(100):\\n        results = algo.train()\\n        checkpoint  = algo.save()  \\n        if time_steps  >= 1000:  \\n            break\\n        time_steps  += results[\"timesteps_total\" ]\\nTrain for a maximum of 100 iterations and store checkpoints after each iteration.\\nIf training surpasses 1,000 time steps, we stop the training.\\nIn what follows we assume that you store the last two code snippets in a file called\\npolicy_server.py . If you want to, you can now start this policy server on your local\\nmachine by running python policy_server.py  in a terminal.\\nDefining  a client\\nNext, to define the corresponding client side of the application, we define a Policy\\nClient  that connects to the server we just started. Since we can’t assume that you\\nhave several computers at home (or available in the cloud), contrary to what we\\nsaid prior, we will start this client on the same machine. In other words, the client\\nwill connect to http://localhost:9900 , but if you can run the server on a different\\nmachine, you could replace localhost  with the IP address of that machine, provided\\nit’s available in the network.\\nPolicy clients have a fairly lean interface. They can trigger the server to start or end\\nan episode, get next actions from it, and log reward information to it (that it would\\notherwise not have). With that said, here’s how you define such a client:\\n92 | Chapter 4: Reinforcement Learning with Ray RLlib\\n\\n# policy_client.py\\nimport gym\\nfrom ray.rllib.env.policy_client  import PolicyClient\\nfrom maze_gym_env  import GymEnvironment\\nif __name__ == \"__main__\" :\\n    env = GymEnvironment ()\\n    client = PolicyClient (\"http://localhost:9900\" , inference_mode =\"remote\" )  \\n    obs = env.reset()\\n    episode_id  = client.start_episode (training_enabled =True)  \\n    while True:\\n        action = client.get_action (episode_id , obs)  \\n        obs, reward, done, info = env.step(action)\\n        client.log_returns (episode_id , reward, info=info)  \\n        if done:\\n            client.end_episode (episode_id , obs)  \\n            exit(0)  \\nStart a policy client on the server address with remote  inference mode.\\nTell the server to start an episode.\\nFor given environment observations, we can get the next action from the server.\\nIt’s mandatory for the client  to log reward information to the server.\\nIf a certain condition is reached, we can stop the client process.\\nIf the environment is done , we have to inform the server about episode\\ncompletion.\\nAssuming you store this code under policy_client.py  and start it by running python\\npolicy_client.py , then the server that we started earlier will start learning with\\nenvironment information solely obtained from the client.\\nAdvanced Concepts\\nSo far we’ve been working with simple environments that were easy enough to tackle\\nwith the most basic RL algorithm settings in RLlib. Of course, in practice you’re not\\nalways that lucky and might have to come up with other ideas to tackle more difficult\\nenvironments. In this section we’re going to introduce a slightly harder version of the\\nmaze environment and discuss some advanced concepts to help you solve it.\\nAdvanced Concepts | 93\\n\\n20In the definition of reset , we allow the seeker to reset on top of the goal to keep the definition simpler.\\nAllowing this trivial edge case does not affect learning.Building an Advanced Environment\\nLet’s make our maze GymEnvironment  a bit more challenging. First, we increase its\\nsize from a 5 × 5 to an 11 × 11 grid. Then we introduce obstacles in the maze that the\\nagent can pass through but only by incurring a penalty, a negative reward of –1. This\\nway our seeker agent will have to learn to avoid obstacles while still finding the goal.\\nAlso, we randomize the agent’s starting position. All of this makes the RL problem\\nharder to solve. Let’s look at the initialization of this new AdvancedEnv  first:\\nfrom gym.spaces  import Discrete\\nimport random\\nimport os\\nclass AdvancedEnv (GymEnvironment ):\\n    def __init__ (self, seeker=None, *args, **kwargs):\\n        super().__init__ (*args, **kwargs)\\n        self.maze_len  = 11\\n        self.action_space  = Discrete (4)\\n        self.observation_space  = Discrete (self.maze_len  * self.maze_len )\\n        if seeker:  \\n            assert 0 <= seeker[0] < self.maze_len  and \\\\\\n                   0 <= seeker[1] < self.maze_len\\n            self.seeker = seeker\\n        else:\\n            self.reset()\\n        self.goal = (self.maze_len -1, self.maze_len -1)\\n        self.info = {\\'seeker\\' : self.seeker, \\'goal\\': self.goal}\\n        self.punish_states  = [  \\n            (i, j) for i in range(self.maze_len ) for j in range(self.maze_len )\\n            if i % 2 == 1 and j % 2 == 0\\n        ]\\nSet the seeker  position upon initialization.\\nIntroduce punish_states  as obstacles for the agent.\\nNext, when resetting the environment, we want to make sure to reset the agent’s\\nposition to a random state.20 We also increase the positive reward for reaching the\\ngoal to 5 to offset the negative reward for passing through an obstacle (which will\\n94 | Chapter 4: Reinforcement Learning with Ray RLlib\\n\\nhappen a lot before the RL algorithm picks up on the obstacle locations). Balancing\\nrewards like this is a crucial task in calibrating your RL experiments:\\n    def reset(self):\\n        \"\"\"Reset seeker position randomly, return observations.\"\"\"\\n        self.seeker = (\\n            random.randint(0, self.maze_len  - 1),\\n            random.randint(0, self.maze_len  - 1)\\n        )\\n        return self.get_observation ()\\n    def get_observation (self):\\n        \"\"\"Encode the seeker position as integer\"\"\"\\n        return self.maze_len  * self.seeker[0] + self.seeker[1]\\n    def get_reward (self):\\n        \"\"\"Reward finding the goal and punish forbidden states\"\"\"\\n        reward = -1 if self.seeker in self.punish_states  else 0\\n        reward += 5 if self.seeker == self.goal else 0\\n        return reward\\n    def render(self, *args, **kwargs):\\n        \"\"\"Render the environment, e.g. by printing its representation.\"\"\"\\n        os.system(\\'cls\\' if os.name == \\'nt\\' else \\'clear\\')\\n        grid = [[\\'| \\' for _ in range(self.maze_len )] +\\n                [\"|\\\\n\"] for _ in range(self.maze_len )]\\n        for punish in self.punish_states :\\n            grid[punish[0]][punish[1]] = \\'|X\\'\\n        grid[self.goal[0]][self.goal[1]] = \\'|G\\'\\n        grid[self.seeker[0]][self.seeker[1]] = \\'|S\\'\\n        print(\\'\\'.join([\\'\\'.join(grid_row ) for grid_row  in grid]))\\nThere are many other ways you could make this environment more difficult, like\\nmaking it much bigger, introducing a negative reward for every step the agent takes\\nin a certain direction, or punishing the agent for trying to walk off the grid. By\\nnow you should understand the problem setting well enough to customize the maze\\nfurther.\\nWhile you might have success training this environment, this is a good opportunity\\nto introduce some advanced concepts that you can apply to other RL problems.\\nApplying Curriculum Learning\\nOne of the most interesting features of RLlib is providing an Algorithm  with a curric‐\\nulum  to learn from. Instead of letting the algorithm learn from arbitrary environment\\nsetups, we cherry-pick states that are much easier to learn from and then slowly but\\nsurely introduce more difficult states. Building a learning curriculum is a great way to\\nmake your experiments converge to solutions quicker. To apply curriculum learning,\\nthe only thing you need is a view on which starting states are easier than others. This\\ncan be a challenge for many environments, but it’s easy to come up with a simple\\nAdvanced Concepts | 95\\n\\ncurriculum for our advanced maze. Namely, the distance of the seeker from the goal\\ncan be used as a measure of difficulty.  The distance measure we’ll use for simplicity is\\nthe sum of the absolute distance of both seeker coordinates from the goal to define a\\ndifficulty .\\nTo run curriculum learning with RLlib, we define a CurriculumEnv  that extends\\nboth our AdvancedEnv  and a so-called TaskSettableEnv  from RLLib. The interface\\nof TaskSettableEnv  is very simple in that you have to define only how to get the\\ncurrent difficulty ( get_task ) and how to set a required difficulty ( set_task ). Here’s\\nthe full definition of this CurriculumEnv :\\nfrom ray.rllib.env.apis.task_settable_env  import TaskSettableEnv\\nclass CurriculumEnv (AdvancedEnv , TaskSettableEnv ):\\n    def __init__ (self, *args, **kwargs):\\n        AdvancedEnv .__init__ (self)\\n    def difficulty (self):  \\n        return abs(self.seeker[0] - self.goal[0]) + \\\\\\n               abs(self.seeker[1] - self.goal[1])\\n    def get_task (self):  \\n        return self.difficulty ()\\n    def set_task (self, task_difficulty ):  \\n        while not self.difficulty () <= task_difficulty :\\n            self.reset()\\nDefine the difficulty  of the current state as the sum of the absolute distance of\\nboth seeker coordinates from the goal.\\nTo define get_task  we can then simply return the current difficulty .\\nTo set a task difficulty, we reset  the environment until its difficulty  is at most\\nthe specified task_difficulty .\\nTo use this environment for curriculum learning, we need to define a curriculum\\nfunction that tells the algorithm when and how to set the task difficulty. We have\\nmany options here, but we use a schedule that simply increases the difficulty by one\\nevery 1,000 time steps trained:\\ndef curriculum_fn (train_results , task_settable_env , env_ctx):\\n    time_steps  = train_results .get(\"timesteps_total\" )\\n    difficulty  = time_steps  // 1000\\n    print(f\"Current difficulty: {difficulty }\")\\n    return difficulty\\n96 | Chapter 4: Reinforcement Learning with Ray RLlib\\n\\n21Note that if you run the notebook for this chapter  on the cloud, the training process could take a while to\\nfinish.To test this curriculum function, we need to add it to our RLlib algorithm config\\nby setting the env_task_fn  property to our curriculum_fn . Note that before training\\na DQN for 15 iterations, we also set an output  folder in our config. This will store\\nexperience data of our training run to the specified temp  folder:21\\nfrom ray.rllib.algorithms.dqn  import DQNConfig\\nimport tempfile\\ntemp = tempfile .mkdtemp()  \\ntrainer = (\\n    DQNConfig ()\\n    .environment (env=CurriculumEnv , env_task_fn =curriculum_fn )  \\n    .offline_data (output=temp)  \\n    .build()\\n)\\nfor i in range(15):\\n    trainer.train()\\nCreate a temp  file to store our training data for later use.\\nSet the CurriculumEnv  as our environment in the environment  part of our config\\nand assign our curriculum_fn  to the env_task_fn  property.\\nUse the offline_data  method to store output  in our temp  folder.\\nRunning this algorithm, you should see how the task difficulty increases over time,\\nthereby giving the algorithm easy examples to start with so that it can learn from\\nthem and progress to more difficult tasks.\\nCurriculum learning is a great technique to be aware of and RLlib allows you to easily\\nincorporate it into your experiments through the curriculum API we just discussed.\\nWorking with Offline  Data\\nIn our previous curriculum learning example we stored training data to a temporary\\nfolder. What’s interesting is that you already know from Chapter 3  that in Q-Learning\\nyou can collect experience data first and decide when to use it in a training step\\nlater. This separation of data collection and training opens up many possibilities.\\nFor instance, maybe you have a good heuristic that can solve your problem in an\\nAdvanced Concepts | 97\\n\\n22Note that RLlib has a wide range of on-policy algorithms like PPO as well.imperfect yet reasonable manner. Or you have records of human interaction with\\nyour environment, demonstrating how to solve the problem by example.\\nThe topic of collecting experience data for later training is often discussed as working\\nwith offline  data . It’s called “offline” because it’s not directly generated by a policy\\ninteracting online with the environment. Algorithms that don’t rely on training on\\ntheir own policy output are called off-policy  algorithms , and Q-Learning, particularly\\nDQN, is just one such example. Algorithms that don’t share this property are called\\non-policy algorithms. In other words, off-policy algorithms can be used to train on\\noffline data.22\\nTo use the data we stored in the temp  folder, we can create a new DQNConfig  that\\ntakes this folder as input . We will also set explore  to False , since we simply want\\nto exploit the data previously collected for training—the algorithm will not explore\\naccording to its own policy.\\nUsing the resulting RLlib algorithm works exactly as before, which we demonstrate\\nby training it for 10 iterations and then evaluating it:\\nimitation_algo  = (\\n    DQNConfig ()\\n    .environment (env=AdvancedEnv )\\n    .evaluation (off_policy_estimation_methods ={})\\n    .offline_data (input_=temp)\\n    .exploration (explore=False)\\n    .build())\\nfor i in range(10):\\n    imitation_algo .train()\\nimitation_algo .evaluate ()\\nNote that we called the algorithm imitation_algo . That’s because this training pro‐\\ncedure intends to imitate  the behavior reflected in the data we collected before.  This\\ntype of learning by demonstration in RL is therefore often called imitation learning  or\\nbehavior cloning .\\nOther Advanced Topics\\nBefore concluding this chapter, let’s have a look at a few other advanced topics that\\nRLlib has to offer. Y ou’ve already seen how flexible RLlib is: working with a range of\\ndifferent environments, configuring your experiments, training on a curriculum, or\\nrunning imitation learning. This section gives you a taste of what else is possible.\\n98 | Chapter 4: Reinforcement Learning with Ray RLlib\\n\\nWith RLlib, you can completely customize the models and policies used under the\\nhood. If you’ve worked with deep learning before, you know how important it can\\nbe to have a good model architecture in place. In RL this is often not as crucial as\\nin supervised learning, but it is still a vital part of successfully running advanced\\nexperiments.\\nY ou can also change the way your observations are preprocessed by providing custom\\npreprocessors. For our simple maze examples, there was nothing to preprocess, but\\nwhen working with image or video data, preprocessing is often a crucial step.\\nIn our AdvancedEnv  we introduced states to avoid. Our agents had to learn to do\\nthis, but RLlib has a feature to automatically avoid them through so-called parametric\\naction spaces . Loosely speaking, what you can do is “mask out” all undesired actions\\nfrom the action space for each point in time. In some cases it can also be necessary to\\nhave variable observation spaces, which is also fully supported by RLlib.\\nWe briefly touched on the topic of offline data. RLlib has a full-fledged Python API\\nfor reading and writing experience data that can be used in various situations.\\nWe have worked solely with DQN here for simplicity, but RLlib has an impressive\\nrange of training algorithms.  To name just one, the MARWIL algorithm is a complex\\nhybrid algorithm with which you can run imitation learning from offline data, while\\nalso mixing in regular training on data generated “online. ”\\nSummary\\nY ou’ve seen a selection of interesting RLlib features in this chapter. We covered\\ntraining multi-agent environments, working with offline data generated by another\\nagent, setting up a client-server architecture to split simulations from RL training,\\nand using curriculum learning to specify increasingly difficult tasks.\\nWe’ve also given you a quick overview of the main concepts underlying RLlib and\\nhow to use its CLI and Python API. In particular, we’ve shown how to configure your\\nRLlib algorithms and environments to your needs. As we’ve covered only a small part\\nof RLlib’s possibilities, we encourage you to read its documentation and explore its\\nAPI.\\nIn the next chapter you’ll learn how to tune the hyperparameters of your RLlib\\nmodels and policies with Ray Tune.\\nSummary | 99\\n\\n\\n\\nCHAPTER 5\\nHyperparameter Optimization\\nwith Ray Tune\\nIn Chapter 4  you learned how to build and run various reinforcement learning\\nexperiments. Running such experiments can be expensive, in terms of both compute\\nresources and the time it takes to run them. This expense only gets amplified as you\\nmove on to more challenging tasks, since it is unlikely that you can just pick an\\nalgorithm out of the box and run it to get a good result. In other words, at some point\\nyou’ll need to tune the hyperparameters of your algorithms to get the best results. As\\nwe’ll see in this chapter, tuning machine learning models is hard, but Ray Tune is an\\nexcellent choice to help you tackle this task.\\nRay Tune is a powerful tool for hyperparameter optimization (HPO).  Not only does\\nit work in a distributed manner by default (and works in any other Ray library\\ndiscussed in this book), but it’s also one of the most feature-rich HPO libraries\\navailable. To top this off, Tune integrates with some of the most prominent HPO\\nlibraries out there, such as Hyperopt, Optuna, and many more. This makes Tune an\\nideal candidate for distributed HPO experiments, whether you’re coming from other\\nlibraries or starting from scratch.\\nIn this chapter we’ll first revisit in a bit more depth why HPO is hard to do and\\nhow you could naively implement it yourself with Ray. We then teach you the\\ncore concepts of Ray Tune and how you can use it to tune the RLlib models built\\nin the previous chapter. To wrap things up, we’ll also have a look at how to use\\nTune for supervised learning tasks, using frameworks like Keras. Along the way, we\\ndemonstrate how Tune integrates with other HPO libraries and introduce you to\\nsome of its more advanced features.\\n101\\n\\nTuning Hyperparameters\\nLet’s briefly recap the basics of hyperparameter optimization. If you’re familiar with\\nHPO, you can skip this section, but since we’re also discussing aspects of distributed\\nHPO, you might still benefit from following along. As always, you can find a note‐\\nbook for this chapter in the book’s GitHub repository .\\nIn our first RL experiment introduced in Chapter 3 , we defined a very basic Q-\\nLearning algorithm whose internal state-action values  were updated according to an\\nexplicit update rule. After initialization, we never touched these model parameters\\ndirectly; they were learned by the algorithm. By contrast, in setting up the algorithm,\\nwe explicitly chose a weight  and a discount_factor  parameter prior to training. We\\ndidn’t tell you how we chose to set these parameters back then; we simply accepted\\nthat they were good enough to crack the problem at hand.\\nIn the same way, in Chapter 4  we initialized an RLlib algorithm with a config  that\\nused two rollout workers for our DQN algorithm by setting num_rollout_workers=2 .\\nParameters like these are called hyperparameters , and finding good choices for them\\ncan be crucial for successful experiments. The field of hyperparameter optimization is\\ndevoted to efficiently finding such good choices.\\nBuilding a Random Search Example with Ray\\nHyperparameters like the weight  or the discount_factor  of our Q-Learning algo‐\\nrithm are continuous  parameters, so we can’t possibly test all combinations of them.\\nWhat’s more, these parameter choices may not be independent of each other. If we\\nwant them to be selected for us, we also need to specify a value range  for each of\\nthem (both hyperparameters need to be between 0 and 1 in this case). So, how do we\\ndetermine good or even optimal hyperparameters?\\nLet’s look at an example that implements a naive yet effective approach to tuning\\nhyperparameters. This example will also allow us to introduce some terminology that\\nwe’ll use later. The core idea is that we can attempt to randomly sample  hyperparame‐\\nters, run the algorithm for each sample, and then select the best run based on the\\nresults.  But to do the theme of this book justice, we don’t just want to run this in a\\nsequential loop; we want to compute our runs in parallel using Ray.\\nTo keep things simple we’ll revisit our simple Q-Learning algorithm from Chapter 3 .\\nWe defined the signature of the main training function as train_policy(env,\\nnum_episodes=10000, weight=0.1, discount_factor=0.9) . That means we can\\ntune the weight  and discount_factor  parameters of our algorithm by passing in\\ndifferent values to the train_policy  function and see how the algorithm performs.\\nTo do that, let’s define a so-called search space  for our hyperparameters. For both\\n102 | Chapter 5: Hyperparameter Optimization with Ray Tune\\n\\nparameters in question we uniformly sample values between 0 and 1, for a total of 10\\nchoices.\\nHere’s what that looks like:\\nimport random\\nsearch_space  = []\\nfor i in range(10):\\n    random_choice  = {\\n        \\'weight\\' : random.uniform(0, 1),\\n        \\'discount_factor\\' : random.uniform(0, 1)\\n    }\\n    search_space .append(random_choice )\\nNext, we define an objective function , or simply objective . The role of an objective\\nfunction is to evaluate the performance of a given set of hyperparameters for a\\ndesired task. In our case, we want to train our RL algorithm and evaluate the trained\\npolicy. Recall that in Chapter 3  we also defined an evaluate_policy  function for\\nprecisely this purpose. The evaluate_policy  function was defined to return the\\naverage number of steps it took for an agent to reach the goal in the underlying\\nmaze environment. In other words, we want to find a set of hyperparameters that\\nminimizes the result of our objective function. To parallelize the objective function,\\nwe’ll use the ray.remote  decorator to make our objective  a Ray task:\\nimport ray\\n@ray.remote\\ndef objective (config):  \\n    environment  = Environment ()\\n    policy = train_policy (  \\n        environment ,\\n        weight=config[\"weight\" ],\\n        discount_factor =config[\"discount_factor\" ]\\n    )\\n    score = evaluate_policy (environment , policy)  \\n    return [score, config]  \\nPass in a dictionary with a hyperparameter sample into our objective.\\nTrain our RL policy using the chosen hyperparameters.\\nAfterward we can evaluate the policy to retrieve the score we want to minimize.\\nReturn both score and hyperparameter choice for later analysis.\\nTuning Hyperparameters | 103\\n\\nFinally, we can run the objective function in parallel using Ray by iterating over the\\nsearch space and collecting the results:\\nresult_objects  = [objective .remote(choice) for choice in search_space ]\\nresults = ray.get(result_objects )\\nresults.sort(key=lambda x: x[0])\\nprint(results[-1])\\nThe actual results of this hyperparameter run are not very interesting because the\\nproblem is easy to solve (most runs will return the optimum of eight steps, regardless\\nof the hyperparameters chosen). What’s more interesting here is how easy it is to\\nparallelize the objective function with Ray. In fact, we’ d like to encourage you to\\nrewrite the preceding example to simply loop through the search space and call the\\nobjective function for each sample, just to confirm how painfully slow such a serial\\nloop can be.\\nConceptually, the three steps we took to run that example are representative of\\nhow hyperparameter tuning works in general. First, you define a search space, then\\nyou define an objective function, and finally you run an analysis to find the best\\nhyperparameters.  In HPO it is common to speak of one evaluation of the objective\\nfunction per hyperparameter sample as a trial, and all trials form the basis for your\\nanalysis.  How parameters are sampled from the search space (in our case, randomly)\\nis up to a search algorithm  to decide. In practice, finding good hyperparameters is\\neasier said than done, so let’s have a closer look at why this problem is so hard.\\nWhy Is HPO Hard?\\nIf you zoom out from the previous example, you can see several intricacies involved\\nin making the process of hyperparameter tuning work well. Here’s a quick overview\\nof the most important ones:\\n•Y our search space can be composed of a large number of hyperparameters.•\\nThese parameters might have different data types and ranges. Some parameters\\nmight be correlated or even depend on others. Sampling good candidates from\\ncomplex, high-dimensional spaces is a difficult task.\\n•Picking parameters at random can work surprisingly well, but it’s not always the•\\nbest option. In general, you need to test more complex search algorithms to find\\nthe best parameters.\\n•In particular, even if you parallelize your hyperparameter search like we just did,•\\na single run of your objective function can take a long time to complete. That\\nmeans you can’t afford to run too many searches overall. For instance, training\\nneural networks can take hours to complete, so your hyperparameter search\\nneeds to be efficient.\\n104 | Chapter 5: Hyperparameter Optimization with Ray Tune\\n\\n•When distributing search, you need to have enough compute resources available•\\nto run searches over the objective function effectively. For instance, you might\\nneed a GPU to compute your objective function fast enough, so all your search\\nruns need to have access to a GPU. Allocating the necessary resources for each\\ntrial is critical for speeding up your search.\\n•Y ou need convenient tooling for your HPO experiments, like stopping bad runs•\\nearly, saving intermediate results, restarting from previous trials, or pausing and\\nresuming runs.\\nAs a mature, distributed HPO framework, Ray Tune addresses all these topics and\\nprovides a simple interface for running hyperparameter tuning experiments. Before\\nwe look into how Tune works, let’s rewrite our previous example to use Tune.\\nAn Introduction to Tune\\nTo get your first taste of Tune, porting over our naive Ray Core implementation of\\nrandom search to Tune is straightforward and follows the same three steps as before.\\nFirst, we define a search space, but this time using tune.uniform , instead of the\\nrandom library:\\nfrom ray import tune\\nsearch_space  = {\\n    \"weight\" : tune.uniform(0, 1),\\n    \"discount_factor\" : tune.uniform(0, 1),\\n}\\nNext, we can define an objective function that looks almost the same as before. We\\ndesigned it like that. The only differences are that this time we return the score as a\\ndictionary, and we don’t need a ray.remote  decorator because Tune will take care of\\ndistributing this objective function for us internally:\\ndef tune_objective (config):\\n    environment  = Environment ()\\n    policy = train_policy (\\n        environment ,\\n        weight=config[\"weight\" ],\\n        discount_factor =config[\"discount_factor\" ]\\n    )\\n    score = evaluate_policy (environment , policy)\\n    return {\"score\": score}\\nWith this tune_objective  function defined, we can pass it to a tune.run  call,\\ntogether with the search space we defined. By default, Tune will run a random search\\nAn Introduction to Tune | 105\\n\\n1Tune uses the same resource model as Ray Core. Each tune_objective  run will be executed on a different\\nCPU core by default. If you want, you can also specify a (fractional) GPU to be used for each trial.for you, but you can also specify other search algorithms, as you will see soon.1\\nCalling tune.run  generates random search trials for your objective and returns an\\nanalysis  object that contains information about the hyperparameter search. We can\\nget the best hyperparameters found by calling get_best_config  and specifying the\\nmetric  and mode  arguments (we want to minimize our score):\\nanalysis  = tune.run(tune_objective , config=search_space )\\nprint(analysis .get_best_config (metric=\"score\", mode=\"min\"))\\nThis quick example covers the basics of Tune, but there’s a lot more to unpack. The\\ntune.run  function is quite powerful and takes a lot of arguments for you to configure\\nyour runs. To understand these different configuration options, we first need to\\nintroduce you to the key concepts of Tune.\\nHow Does Tune Work?\\nTo effectively work with Tune, you must understand six key concepts, four of which\\nyou used in the previous example. Here’s an overview of Ray Tune’s components and\\nhow you should think about them:\\nSearch spaces\\nThese spaces determine which parameters to select. Search spaces define the\\nrange of values for each parameter and how they should be sampled. They\\nare defined as dictionaries and use Tune’s sampling functions to specify valid\\nhyperparameter values. Y ou have already seen tune.uniform , but there are many\\nmore options to choose from .\\nTrainables\\nA Trainable  is Tune’s formal representation of an objective you want to “tune. ”\\nTune has a class-based API as well, but we will use only the function-based API\\nin this book. For us, a Trainable  is a function with a single argument: a search\\nspace, which reports scores to Tune. The easiest way to report a score is by\\nreturning a dictionary with the value you’re interested in.\\nTrials\\nBy triggering tune.run(...) , Tune will set up trials and schedule them for\\nexecution on your cluster. A trial contains all the necessary information about a\\nsingle run of your objective, given a set of hyperparameters.\\n106 | Chapter 5: Hyperparameter Optimization with Ray Tune\\n\\nAnalyses\\nCompleting a tune.run  call returns an ExperimentAnalysis  object, with the\\nresults of all trials. Y ou can use this object to drill down into the results of your\\ntrials.\\nSearch algorithms\\nTune supports a large variety of search algorithms, which are at the core of\\nhow to tune your hyperparameters. So far you’ve implicitly encountered Tune’s\\ndefault search algorithm, which randomly selects hyperparameters from the\\nsearch space.\\nSchedulers\\nThe last, crucial component of a Tune experiment is that of a scheduler . Schedu‐\\nlers plan and execute what the search algorithm selects. By default, Tune sched‐\\nules trials selected by your search algorithm on a first-in-first-out (FIFO) basis.\\nIn practice, you can think of schedulers as a way to speed up your experiments,\\nfor instance by stopping unsuccessful trials early.\\nFigure 5-1  sums up these major Tune components and their relationships.\\nFigure 5-1. The core components of Ray Tune\\nIn this chapter we exclusively use tune.run  to illustrate Tune’s\\nfunctionality.  Tune also had an API called Tuner  added in Ray 2.0\\nas part of Ray AIR, which you will learn more about in Chapter 7\\nand use within Ray AIR in Chapter 10 .\\nAt the time of this writing, tune.run  is still the more mature\\nAPI. For instance, experiments using tune.run(...)  return an\\nExperimentAnalysis  object, a powerful tool for analyzing your\\nresults. Analogous calls using the Tuner  API return a so-called\\nResultGrid  instead. In the long run ResultGrid  will succeed\\nExperimentAnalysis , but it is not yet at feature parity.\\nTo learn more, see the Tune API documentation on this topic .\\nAn Introduction to Tune | 107\\n\\n2In open source software, it’s important to determine who is responsible for maintaining an integration. We\\nwill discuss this more in Chapter 11 , which covers Ray’s ecosystem as a whole. In the case of Tune, among the\\nintegrations listed here, the Hyperopt and Optuna integrations are maintained by the Ray Tune team; the rest\\nare community-sponsored.Note that internally Tune runs are started on the driver process of your Ray Cluster,\\nwhich spawns several worker processes (using Ray actors) that execute individual\\ntrials of your HPO experiment. Y our trainables, defined on the driver, have to be\\nsent to the workers, and trial results need to be communicated to the driver running\\ntune.run(...) .\\nSearch spaces, trainables, trials, and analyses don’t need much additional explana‐\\ntion, and we’ll see more examples of each of those components in the rest of this\\nchapter.  But search algorithms, searchers  for short, and schedulers need a bit more\\nelaboration.\\nSearch algorithms\\nAll advanced search algorithms provided by Tune, and the many third-party HPO\\nlibraries it integrates with, fall under the umbrella of Bayesian optimization . Unfortu‐\\nnately, going into the details of specific Bayesian search algorithms is far beyond\\nthe scope of this book. The basic idea is that you update your beliefs about which\\nhyperparameter ranges are worth exploring based on the results of your previous\\ntrials. Techniques using this principle make more informed decisions and, hence,\\ntend to be more efficient than independently sampling parameters (e.g., at random).\\nApart from the basic random search we’ve seen already, and grid search , which\\npicks hyperparameters from a predefined “grid” of choices, Tune integrates with a\\nwide range of Bayesian optimization searchers. For instance, Tune integrates with\\nthe popular Hyperopt and Optuna libraries,2 and you can use the popular TPE (Tree-\\nstructured Parzen Estimator) searcher with Tune through both of these libraries.\\nNot only that, Tune also integrates with tools such as Ax, BlendSearch, FLAML,\\nDragonfly, scikit-Optimize, Bayesian optimization, HpBandSter, Nevergrad, ZOOpt,\\nSigOpt, and HEBO. If you need to run HPO experiments with any of these tools on a\\ncluster or want to easily switch between them, Tune is the way to go.\\nTo make things more concrete, let’s rewrite our basic random search Tune example\\nfrom earlier to use the Bayesian optimization library. To do so, make sure you install\\nthis library in your Python environment first, e.g., with pip install bayesian-\\noptimization :\\nfrom ray.tune.suggest.bayesopt  import BayesOptSearch\\nalgo = BayesOptSearch (random_search_steps =4)\\n108 | Chapter 5: Hyperparameter Optimization with Ray Tune\\n\\ntune.run(\\n    tune_objective ,\\n    config=search_space ,\\n    metric=\"score\",\\n    mode=\"min\",\\n    search_alg =algo,\\n    stop={\"training_iteration\" : 10},\\n)\\nNote that we “warm start” our Bayesian optimization with four random steps at the\\nbeginning, and we explicitly stop  the trial runs after 10 training iterations.\\nBecause we’re not just randomly selecting parameters with BayesOptSearch : the\\nsearch_alg  we use in our Tune run needs to know which metric  to optimize for\\nand whether to minimize or optimize this metric.  As we’ve argued before, we want to\\nachieve a \"min\"  \"score\" .\\nSchedulers\\nNext, let’s discuss how to use trial schedulers  in Tune to make your runs more\\nefficient.  We also use this section to introduce a slightly different way to report your\\nmetrics to Tune within an objective function.\\nSo let’s say that instead of computing a score straight-up, like we did in the previous\\nexamples, we compute an intermediate score  in a loop. This is a situation that often\\noccurs in supervised machine learning scenarios, when training a model for several\\niterations (we’ll see concrete applications of this in “Machine Learning with Tune”  on\\npage 115). With good hyperparameter choices selected, this immediate score might\\nstagnate way before the loop in which it is computed. In other words, if we’re not\\nseeing enough incremental changes, why not stop the trial early? This is exactly one\\nof the cases Tune’s schedulers are built for.\\nHere’s a quick example of such an objective function. This is a toy example, but it will\\nhelp us think about the optimal hyperparameters we want Tune to find more easily\\nthan if we started with a complex example:\\ndef objective (config):\\n    for step in range(30):  \\n        score = config[\"weight\" ] * (step ** 0.5) + config[\"bias\"]\\n        tune.report(score=score)  \\nsearch_space  = {\"weight\" : tune.uniform(0, 1), \"bias\": tune.uniform(0, 1)}\\nOften you may want to compute intermediate scores, e.g., in a “training loop. ”\\nY ou can use tune.report  to let Tune know about these intermediate scores.\\nAn Introduction to Tune | 109\\n\\nThe score we want to minimize here is the square root of a positive number times a\\nweight , plus adding a bias  term. It’s clear that both of these hyperparameters need to\\nbe as small as possible to minimize the score  for any positive x. Given that the square\\nroot function “flattens out, ” we might not have to compute all 30 passes through\\nthe loop to find sufficiently good values for our two hyperparameters. If each score\\ncomputation took an hour, stopping early could be a huge boost in making your\\nexperiments run quicker.\\nLet’s illustrate this idea by using the popular Hyperband algorithm as our trial sched‐\\nuler.  This scheduler needs to be passed a metric and mode (again, we min-imize our\\nscore ). We also make sure to run for 10 samples so as not to stop prematurely:\\nfrom ray.tune.schedulers  import HyperBandScheduler\\nscheduler  = HyperBandScheduler (metric=\"score\", mode=\"min\")\\nanalysis  = tune.run(\\n    objective ,\\n    config=search_space ,\\n    scheduler =scheduler ,\\n    num_samples =10,\\n)\\nprint(analysis .get_best_config (metric=\"score\", mode=\"min\"))\\nNote that in this case we did not specify a search algorithm, which means that\\nHyperband will run on parameters selected by random search. We also could have\\ncombined  this scheduler with another search algorithm instead. This would have\\nallowed us to pick better trial hyperparameters and stop bad trials early.  However,\\nnote that not every scheduler can be combined with search algorithms. Check Tune’s\\nscheduler compatibility matrix  for more information.\\nTo wrap up this discussion, apart from Hyperband, Tune includes distributed imple‐\\nmentations of early stopping algorithms such as the Median Stopping Rule, ASHA,\\nPopulation Based Training (PBT), and Population Based Bandits (PB2).\\nConfiguring  and Running Tune\\nBefore looking into more concrete machine learning examples using Ray Tune, let’s\\ndive into some useful topics that help you get more out of your Tune experiments,\\nsuch as properly utilizing resources, stopping and resuming trials, adding callbacks to\\nyour Tune runs, or defining custom and conditional search spaces.\\n110 | Chapter 5: Hyperparameter Optimization with Ray Tune\\n\\nSpecifying resources\\nBy default, each Tune trial will run on one CPU and leverage as many CPUs as\\navailable for concurrent trials.  For instance, if you run Tune on a laptop with 8 CPUs,\\nany of the experiments computed so far in this chapter will spawn eight concurrent\\ntrials and allocate one CPU for each trial. Changing this behavior can be controlled\\nusing the resources_per_trial  argument of a Tune run.\\nY ou can also determine the number of GPUs used per trial. Plus, Tune allows you\\nto use fractional resources ; i.e., you can share resources between trials.  So, let’s say\\nthat you have a machine with 12 CPUs and 2 GPUs and you request the following\\nresources for your objective :\\nfrom ray import tune\\ntune.run(\\n    objective ,\\n    config=search_space ,\\n    num_samples =10,\\n    resources_per_trial ={\"cpu\": 2, \"gpu\": 0.5}\\n)\\nThat means Tune can schedule and execute up to four concurrent trials on your\\nmachine, as this would max out GPU utilization on this machine (while you’ d still\\nhave four idle CPUs for other tasks). If you want, you can also specify the amount of\\n\"memory\"  used by a trial by passing the number of bytes into resources_per_trial .\\nAlso note that should you need to explicitly restrict  the number of concurrent\\ntrials, you can do so by passing in the max_concurrent_trials  parameter to your\\ntune.run(...) . In the preceding example, if you want to always keep one GPU\\navailable for other tasks, you can limit the number of concurrent trials to two by\\nsetting max_concurrent_trials = 2 .\\nNote that everything we just exemplified for resources on a single machine naturally\\nextends to any Ray Cluster and its available resources. In any case, Ray will always try\\nto schedule the next trials, but it will wait and ensure enough resources are available\\nbefore executing them.\\nCallbacks and metrics\\nIf you’ve spent some time investigating the outputs of the Tune runs we’ve started\\nin this chapter so far, you’ll have noticed that each trial comes equipped with a lot\\nof information by default, such as the trial ID, its execution date, and much more.\\nWhat’s interesting is that Tune not only allows you to customize the metrics you want\\nto report, you can also hook into a tune.run  by providing callbacks . Let’s compute a\\nquick, representative example that does both.\\nAn Introduction to Tune | 111\\n\\n3If you want to learn more about how to use callbacks in Tune or create your own callbacks, check out the user\\nguide on callbacks and metrics in Tune .Slightly modifying a previous example, let’s say we want to log a specific message\\nwhenever a trial returns a result. To do so, all you need to do is implement the\\non_trial_result  method on a Callback  object from the ray.tune  package.3 Here’s\\nhow that would look for an objective function that reports a score :\\nfrom ray import tune\\nfrom ray.tune  import Callback\\nfrom ray.tune.logger  import pretty_print\\nclass PrintResultCallback (Callback ):\\n    def on_trial_result (self, iteration , trials, trial, result, **info):\\n        print(f\"Trial {trial} in iteration {iteration }, \"\\n              f \"got result: {result[\\'score\\']}\")\\ndef objective (config):\\n    for step in range(30):\\n        score = config[\"weight\" ] * (step ** 0.5) + config[\"bias\"]\\n        tune.report(score=score, step=step, more_metrics ={})\\nNote that, apart from the score, we also report step  and more_metrics  to Tune. In\\nfact, you could expose any other metric you’ d like to track there, and Tune would\\nadd it to its trial metrics. Here’s how you’ d run a Tune experiment with our custom\\ncallback and print the custom metrics we just defined:\\nsearch_space  = {\"weight\" : tune.uniform(0, 1), \"bias\": tune.uniform(0, 1)}\\nanalysis  = tune.run(\\n    objective ,\\n    config=search_space ,\\n    mode=\"min\",\\n    metric=\"score\",\\n    callbacks =[PrintResultCallback ()])\\nbest = analysis .best_trial\\nprint(pretty_print (best.last_result ))\\nRunning this code will result in the following outputs (additional to what you’ll see in\\nany other Tune run). Note that we need to specify mode  and metric  explicitly here so\\nthat Tune knows what we mean by best_result . First, you should see the output of\\nour callback, while the trials are running:\\n...\\nTrial objective_85955_00000 in iteration 57, got result: 1.5379782083952644\\nTrial objective_85955_00000 in iteration 58, got result: 1.5539087627537493\\nTrial objective_85955_00000 in iteration 59, got result: 1.569535794562848\\n112 | Chapter 5: Hyperparameter Optimization with Ray Tune\\n\\nTrial objective_85955_00000 in iteration 60, got result: 1.5848760187255326\\nTrial objective_85955_00000 in iteration 61, got result: 1.5999446700996236\\n...\\nThen, at the very end of the program, we print the metrics of the best available trial,\\nwhich includes the three custom metrics we defined. The following output omits\\nsome default metrics to make it more readable. We recommend that you run an\\nexample like this on your own, in particular to get used to reading the outputs of\\nTune trials (which can be a bit overwhelming due to their concurrent nature):\\nResult logdir: /Users/maxpumperla/ray_results/objective_2022-05-23_15-52-01\\n...\\ndone: true\\nexperiment_id: ea5d89c2018f483183a005a1b5d47302\\nexperiment_tag: 0_bias=0.73356,weight=0.16088\\nhostname: mac\\niterations_since_restore: 30\\nmore_metrics: {}\\nscore: 1.5999446700996236\\nstep: 29\\ntrial_id: \\'85955_00000\\'\\n...\\nWe used on_trial_result  as an example of a method to implement a custom Tune\\nCallback , but you have many other useful options that are relatively self-explanatory.\\nIt’s not very helpful to list them all here, but some particularly useful callback meth‐\\nods are on_trial_start , on_trial_error , on_experiment_end , and on_checkpoint .\\nThe latter hints at an important aspect of Tune runs that we’ll discuss next.\\nCheckpoints, stopping, and resuming\\nThe more Tune trials you kick off and the longer they each run individually, espe‐\\ncially in a distributed setting, the more you need a mechanism to protect you against\\nfailures, stop a run, or pick a run up again from previous results. Tune makes\\nthis possible by periodically creating checkpoints  for you. The checkpoint cadence is\\ndynamically adjusted by Tune to ensure at least 95% of the time is spent on running\\ntrials, and not too many resources are devoted to storing checkpoints.\\nIn the example we just computed, the checkpoint directory, or logdir , used by default\\nis of the form ~/ray_results/<your-objective>_<date>_<time> . If you know this\\ncheckpoint directory of your experiment, you can easily resume  it like so:\\nanalysis  = tune.run(\\n    objective ,\\n    name=\"<your-logdir>\" ,\\n    resume=True,\\n    config=search_space )\\nSimilarly, you can stop your trials by defining stopping conditions and explicitly\\npassing them to your tune.run . The easiest option for doing that is by providing a\\nAn Introduction to Tune | 113\\n\\ndictionary with a stopping condition. Here’s how you stop running our objective\\nanalysis after reaching a training_iteration  count of 10, a built-in metric of all\\nTune runs:\\ntune.run(\\n    objective ,\\n    config=search_space ,\\n    stop={\"training_iteration\" : 10})\\nOne of the drawbacks of specifying a stopping condition this way is that it assumes\\nthe metric in question is increasing . For instance, the score  we compute starts high\\nand is something we want to minimize.  To formulate a flexible stopping condition for\\nour score , the best way is to provide a stopping function as follows:\\ndef stopper(trial_id , result):\\n    return result[\"score\"] < 2\\ntune.run(\\n    objective ,\\n    config=search_space ,\\n    stop=stopper)\\nIn situations that require a stopping condition with more context or explicit state,\\nyou can also define a custom Stopper  class  to pass into the stop  argument of your\\nTune run, but we won’t cover this case here.\\nCustom and conditional search spaces\\nThe last more advanced topic we’ll cover here is that of complex search spaces. So\\nfar, we’ve looked only at hyperparameters that were independent of each other, but in\\npractice, some often depend on others. Also, while Tune’s built-in search spaces have\\nquite a lot to offer, sometimes you want to sample parameters from a more exotic\\ndistribution or your own modules.\\nHere’s how you can handle both situations in Tune. Continuing with our simple\\nobjective  example, let’s say that instead of Tune’s tune.uniform  you want to use the\\nrandom.uniform  sampler from the numpy  package for your weight  parameter.  And\\nthen your bias  parameter should be weight  times a standard normal variable. Using\\ntune.sample_from  you can tackle this situation (or more complex and nested ones)\\nlike this:\\nfrom ray import tune\\nimport numpy as np\\nsearch_space  = {\\n    \"weight\" : tune.sample_from (\\n        lambda context: np.random.uniform(low=0.0, high=1.0)\\n    ),\\n114 | Chapter 5: Hyperparameter Optimization with Ray Tune\\n\\n4In case you were wondering why the “config” argument in tune.run  was not called search_space , the\\nhistorical reason lies in this interoperability with RLlib config  objects.    \"bias\": tune.sample_from (\\n        lambda context: context.config.weight * np.random.normal()\\n    )}\\ntune.run(objective , config=search_space )\\nThere are many more interesting features to explore in Ray Tune, but let’s switch\\ngears here and look into some machine learning applications using Tune.\\nMachine Learning with Tune\\nAs we’ve seen, Tune is versatile and allows you to tune hyperparameters for any\\nobjective you give it. In particular, you can use it with any machine learning frame‐\\nwork you’re interested in. This section provides two examples. First, we’re going to\\nuse Tune to optimize parameters of an RLlib experiment, and then we’ll tune a Keras\\nmodel using Optuna through Tune.\\nUsing RLlib with Tune\\nRLlib and Tune have been designed to work together, so you can quite easily set\\nup an HPO experiment for your existing RLlib code. In fact, RLlib trainers can be\\npassed into the first argument of tune.run , as Trainable . Y ou can choose between\\nthe actual trainer class, like DQNTrainer , or its string representation, like \"DQN\" . As\\nTune metric  you can pass any metric tracked by your RLlib experiment, for instance\\n\"episode_reward_mean\" . And the config  argument to tune.run  is just your RLlib\\ntrainer  configuration, but you can use the full power of Tune’s search space API\\nto sample hyperparameters like the learning rate or training batch size.4 Here’s a\\nfull example of what we just described, running a tuned RLlib experiment on the\\nCartPole-v0  Gym environment:\\nfrom ray import tune\\nanalysis  = tune.run(\\n    \"DQN\",\\n    metric=\"episode_reward_mean\" ,\\n    mode=\"max\",\\n    config={\\n        \"env\": \"CartPole-v1\" ,\\n        \"lr\": tune.uniform(1e-5, 1e-4),\\n        \"train_batch_size\" : tune.choice([10000, 20000, 40000]),\\n    },\\n)\\nMachine Learning with Tune | 115\\n\\nTuning Keras Models\\nTo wrap up this chapter, let’s look at a slightly more involved example. As we men‐\\ntioned, this is not primarily a machine learning book but rather an introduction\\nto Ray and its libraries. Thus we can neither introduce you to the basics of ML\\nnor spend much time on introducing ML frameworks in detail. So, in this section\\nwe assume familiarity with Keras and its API and some basic knowledge about\\nsupervised learning. If you do not have these prerequisites, you should still be able\\nto follow along and focus on the Ray Tune–specific parts. Y ou can view the following\\nexample as a more realistic scenario of applying Tune to machine learning workloads.\\nFrom a bird’s-eye view, we’ll take the following steps:\\n1.Load a common dataset.1.\\n2.Prepare it for an ML task.2.\\n3.Define a Tune objective by creating a deep learning model with Keras that reports3.\\nan accuracy metric to Tune.\\n4.Use Tune’s Hyperopt integration to define a search algorithm that tunes a set of4.\\nhyperparameters of our Keras model.\\nThe Tune workflow remains the same: we define an objective and a search space and\\nthen use tune.run  with the configuration we want. On a high level, the process of\\nusing Tune with any ML framework works as shown in Figure 5-2 .\\nFigure 5-2. Tune sets up a distributed HPO for your ML models by executing trials on\\nRay workers in your cluster and reporting metrics back to the driver\\nTo define a dataset to train on, let’s write a simple load_data  utility function that\\nloads the famous MNIST data that ships with Keras. MNIST consists of 28 × 28\\npixel images of handwritten digits. We normalize the pixel values to be between 0\\nand 1 and make the labels for those 10 digits categorical variables . Here’s how you\\ncan do this purely with Keras’s built-in functionality (make sure to use pip install\\ntensorflow  before running this):\\n116 | Chapter 5: Hyperparameter Optimization with Ray Tune\\n\\n5This can even lead to a rare condition in which one worker starts downloading the data and another one\\nchecks for and sees a local copy. But since the download is not complete, the second worker will try to open\\na potentially corrupted file. This goes to show that, while Ray takes care of a lot of things in the background,\\nyou still need to be mindful about how you write your code.from tensorflow.keras.datasets  import mnist\\nfrom tensorflow.keras.utils  import to_categorical\\ndef load_data ():\\n    (x_train, y_train), (x_test, y_test) = mnist.load_data ()\\n    num_classes  = 10\\n    x_train, x_test = x_train / 255.0, x_test / 255.0\\n    y_train = to_categorical (y_train, num_classes )\\n    y_test = to_categorical (y_test, num_classes )\\n    return (x_train, y_train), (x_test, y_test)\\nload_data ()\\nNote that after defining load_data , we call it once so that the data gets downloaded\\nlocally. That’s because when you call mnist.load_data() , it first looks for a locally\\ncached copy. If we didn’t load the data first, several Tune workers would try to\\ndownload the data in parallel, which can lead to problems.5\\nNext, we define a Tune objective  function, or trainable, by loading the data we just\\ndefined, setting up a sequential Keras model with hyperparameters selected from the\\nconfig  we pass into our objective , and then compile and fit the model. To define\\nour deep learning model, we first flatten the MNIST input images to vectors and\\nthen add two fully connected layers (called Dense  in Keras) and a Dropout  layer in\\nbetween.\\nThe hyperparameters we want to tune are the activation function of the first Dense\\nlayer, the Dropout  rate, and the number of “hidden” output units of the first layer. We\\ncould tune any other hyperparameter of this model the same way; this selection is just\\nan example.\\nWe could manually report a metric of interest in the same way we did in other\\nexamples in this chapter (e.g., by returning a dictionary in our objective  or using\\ntune.report(...) ). But since Tune comes with a proper Keras integration, we can\\nuse the so-called TuneReportCallback  as a custom Keras callback that we pass into\\nour model’s fit method. This is what our Keras objective  function looks like:\\nfrom tensorflow.keras.models  import Sequential\\nfrom tensorflow.keras.layers  import Flatten, Dense, Dropout\\nfrom ray.tune.integration.keras  import TuneReportCallback\\nMachine Learning with Tune | 117\\n\\ndef objective (config):\\n    (x_train, y_train), (x_test, y_test) = load_data ()\\n    model = Sequential ()\\n    model.add(Flatten(input_shape =(28, 28)))\\n    model.add(Dense(config[\"hidden\" ], activation =config[\"activation\" ]))\\n    model.add(Dropout(config[\"rate\"]))\\n    model.add(Dense(10, activation =\"softmax\" ))\\n    model.compile(loss=\"categorical_crossentropy\" , metrics=[\"accuracy\" ])\\n    model.fit(x_train, y_train, batch_size =128, epochs=10,\\n              validation_data =(x_test, y_test),\\n              callbacks =[TuneReportCallback ({\"mean_accuracy\" : \"accuracy\" })])\\nNext, let’s use a custom search algorithm to tune this objective. Specifically, we’re\\nusing the HyperOptSearch  algorithm, which gives us access to Hyperopt’s TPE algo‐\\nrithm through Tune. To use this integration, make sure to install Hyperopt on your\\nmachine (for instance with pip install hyperopt==0.2.7 ). HyperOptSearch  allows\\nus to define a list of promising initial hyperparameter choices to investigate.\\nThis is entirely optional, but sometimes you might have good guesses to start from.\\nIn our case, we go with a dropout \"rate\"  of 0.2, 128 \"hidden\"  units, and a rectified\\nlinear unit (ReLU) \"activation\"  function initially. Other than that, we can define\\na search space with the tune  utility just as we did before. Finally, we can get an\\nanalysis  object to determine the best hyperparameters found by passing everything\\ninto a tune.run  call:\\nfrom ray import tune\\nfrom ray.tune.suggest.hyperopt  import HyperOptSearch\\ninitial_params  = [{\"rate\": 0.2, \"hidden\" : 128, \"activation\" : \"relu\"}]\\nalgo = HyperOptSearch (points_to_evaluate =initial_params )\\nsearch_space  = {\\n    \"rate\": tune.uniform(0.1, 0.5),\\n    \"hidden\" : tune.randint(32, 512),\\n    \"activation\" : tune.choice([\"relu\", \"tanh\"])\\n}\\nanalysis  = tune.run(\\n    objective ,\\n    name=\"keras_hyperopt_exp\" ,\\n    search_alg =algo,\\n    metric=\"mean_accuracy\" ,\\n    mode=\"max\",\\n    stop={\"mean_accuracy\" : 0.99},\\n    num_samples =10,\\n    config=search_space ,\\n)\\nprint(\"Best hyperparameters found were: \" , analysis .best_config )\\n118 | Chapter 5: Hyperparameter Optimization with Ray Tune\\n\\nNote that we’re using the full power of Hyperopt here, without having to learn any\\nof its specifics. Hyperopt itself is not distributed (by default). By using Hyperopt\\nthrough the Tune API, we can leverage it for distributed HPO on a Ray Cluster.\\nWe chose the combination of Keras and Hyperopt as example of using Tune with an\\nadvanced ML framework and a third-party HPO library. But we could have chosen\\nany other machine learning library and practically any other HPO library supported\\nby Tune. If you’re interested in diving deeper into any of the many integrations Tune\\nhas to offer, check out the Ray Tune documentation examples .\\nSummary\\nTune is arguably one of the most versatile HPO tools you can choose today. It’s\\nfeature-rich, offering many search algorithms, advanced schedulers, complex search\\nspaces, custom stoppers, and many other features that we couldn’t cover in this chap‐\\nter. Also, it seamlessly integrates with most notable HPO tools, such as Optuna or\\nHyperopt, making it easy to migrate from these tools or simply leverage their features\\nthrough Tune. Y ou can view Ray Tune as a flexible, distributed HPO framework that\\nextends  others that might work only on single machines.\\nSummary | 119\\n\\n\\n\\nCHAPTER 6\\nData Processing with Ray\\nEdward Oakes\\nIn Chapter 5  you learned how to tune hyperparameters for your machine learning\\nexperiments. Of course, the key component to applying machine learning in practice\\nis data. In this chapter we’ll explore the core set of data processing capabilities on Ray:\\nRay Data.\\nWhile not meant to replace more general data processing systems such as Apache\\nSpark or Apache Hadoop, Ray Data offers basic data processing capabilities and a\\nstandard way to load, transform, and pass data to different parts of a Ray application.\\nThis enables an ecosystem of libraries on Ray to speak the same language so users can\\nmix and match functionality in a framework-agnostic way to meet their needs.\\nThe central component of the Ray Data ecosystem, Ray Datasets, offers the core\\nabstractions for loading, transforming, and passing references to data in a Ray Clus‐\\nter. Datasets are the “glue” that enables different libraries to interoperate on top of\\nRay. Y ou’ll see this in action in “External Library Integrations”  on page 134, where we\\nshow how you can do dataframe processing using the full expressiveness of the Dask\\nAPI using Dask on Ray and transform the result into a dataset. The main benefits of\\nRay Datasets are:\\nFlexibility\\nIt supports a wide range of data formats, work seamlessly with library integra‐\\ntions like Dask on Ray, and can be passed between Ray tasks and actors without\\ncopying data.\\nPerformance for ML workloads\\nIt offers important features like accelerator support, pipelining, and global ran‐\\ndom shuffles that accelerate ML training and inference workloads.\\n121\\n\\nThis chapter will familiarize you with the core concepts for doing data processing\\non Ray and help you understand how to accomplish common patterns as well as\\nwhy you would choose to use different pieces to accomplish a task. We assume a\\nbasic familiarity with data processing concepts such as map, filter , groupby , and\\npartition , but it’s not intended to be a tutorial on data science in general or a deep\\ndive into the internals of how these operations are implemented. Readers with a\\nlimited data science background should not have a problem following along.\\nWe’ll start introducing the core building block: Ray Datasets. This will cover the\\narchitecture, basics of the API, and an example of how Ray Datasets can enable build‐\\ning complex data-intensive applications. Then, we’ll briefly cover external library\\nintegrations on Ray, focusing on Dask on Ray. Finally, we’ll bring it all together by\\nbuilding a scalable end-to-end machine learning pipeline in a single Python script.\\nThe notebook for this chapter is available on GitHub  along with\\nthe data used in the end-to-end example .\\nRay Datasets\\nThe main goal of Ray Datasets is to support a scalable, flexible abstraction for data\\nprocessing on Ray. Datasets are intended to be the standard way to read, write, and\\ntransfer data across the full ecosystem of Ray libraries. One of the most powerful\\nuses of Ray Datasets is acting as the data ingest and preprocessing layer for machine\\nlearning workloads, allowing you to efficiently scale up training using Ray Train and\\nRay Tune. We explore this in more detail in “Building an ML Pipeline” on page 136 .\\nIf you’ve worked with other distributed data processing APIs such as Apache Spark’s\\nResilient Distributed Datasets in the past, the Ray Datasets API will be very familiar.\\nThe core of the API leans on functional programming and offers standard function‐\\nality such as reading and writing different data sources; performing basic transforma‐\\ntions like map, filter , and sort ; and performing some simple aggregations such as\\ngroupby .\\nUnder the hood, Ray Datasets implements distributed Apache Arrow . Apache Arrow\\nis a unified columnar data format for data processing libraries and applications.\\nIntegrating with Apache Arrow means that Datasets get interoperability with many of\\nthe most popular processing libraries, such as NumPy and Pandas, out of the box.\\nA Ray Dataset consists of a list of Ray object references, each of which points at a\\n“block” of data. These blocks are either Arrow tables or Python lists (for data that\\nisn’t supported by the Arrow format) in Ray’s shared memory object store, and the\\n122 | Chapter 6: Data Processing with Ray\\n\\ncompute over the data such as for map or filter operations happens in Ray tasks (and\\nsometimes actors).\\nBecause Ray Datasets relies on the core Ray primitives of tasks and objects in the\\nshared memory object store, it inherits key benefits of Ray: scalability to hundreds\\nof nodes, efficient memory usage due to sharing memory across processes on the\\nsame node, as well as object spilling and recovery to gracefully handle failures.\\nAdditionally, because Datasets are just lists of object references, they can also be\\npassed between tasks and actors efficiently without needing to make a copy of the\\ndata, which is crucial for making data-intensive applications and libraries scalable.\\nRay Datasets Basics\\nThis section will give an overview of Ray Datasets, covering how to get started read‐\\ning, writing, and transforming datasets. This is not a comprehensive reference but\\nrather an introduction to the basic concepts so we can build up to some interesting\\nexamples later, showing what makes Ray Datasets powerful. For up-to-date informa‐\\ntion on what’s supported and exact syntax, see the Ray Datasets documentation .\\nTo follow along with the examples in this section, make sure Ray Datasets is installed\\nlocally:\\npip install \"ray[data]==2.2.0\"\\nCreating a Ray Dataset\\nFirst, let’s create a simple Dataset and perform some basic operations on it:\\nimport ray\\n# Create a dataset containing integers in the range [0, 10000).\\nds = ray.data.range(10000)\\n# Basic operations: show the size of the dataset, get a few samples, \\n# print the schema.\\nprint(ds.count())  # -> 10000\\nprint(ds.take(5))  # -> [0, 1, 2, 3, 4]\\nprint(ds.schema())  # -> <class \\'int\\'>\\nHere we created a Dataset containing the numbers from 0 to 10,000 and then printed\\nsome basic information about it: the total number of records, a few samples, and the\\nschema.\\nReading from and writing to storage\\nOf course, for real workloads you’ll often want to read from and write to persistent\\nstorage to load your data and write the results. Writing and reading Ray Datasets\\nis simple; for example, to write a Dataset to a CSV file and then load it back into\\nmemory, we just need to use the built-in write_csv  and read_csv  utilities:\\nRay Datasets | 123\\n\\n1If you’re interested in following an example that reads actual data from an S3 bucket, see the Batch Inference\\nexample in Ray’s documentation .# Save the dataset to a local file and load it back.\\nray.data.range(10000).write_csv (\"local_dir\" )\\nds = ray.data.read_csv (\"local_dir\" )\\nprint(ds.count())\\nDatasets supports a number of common serialization formats such as CSV , JSON, and\\nParquet and can read from or write to local disk as well as remote storage like HDFS\\nor AWS S3.\\nIn the preceding example, we provided just a local file path ( \"local_dir\" ) so the\\ndataset was written to a directory on the local machine. If we wanted to write to\\nand read from S3 instead, we would provide a path like \"s3://my_bucket/\"  and\\nDatasets would automatically handle efficiently reading and writing remote storage,1\\nparallelizing the requests across many tasks to improve throughput.\\nNote that Ray Datasets also supports custom data sources that you can use to write to\\nany external data storage system that isn’t supported out of the box.\\nBuilt-in transformations\\nNow that we understand the basic APIs around how to create and inspect Datasets,\\nlet’s take a look at some of the built-in operations we can do on them. The following\\ncode sample shows three basic operations that Ray Datasets supports:\\nds1 = ray.data.range(10000)\\nds2 = ray.data.range(10000)\\nds3 = ds1.union(ds2)  \\nprint(ds3.count())  # -> 20000\\n# Filter the combined dataset to only the even elements.\\nds3 = ds3.filter(lambda x: x % 2 == 0)  \\nprint(ds3.count())  # -> 10000\\nprint(ds3.take(5))  # -> [0, 2, 4, 6, 8]\\n# Sort the filtered dataset.\\nds3 = ds3.sort()  \\nprint(ds3.take(5))  # -> [0, 0, 2, 2, 4]\\nunion  two Datasets together. The result is a new Dataset that contains all the\\nrecords of both.\\nfilter  the elements of a Dataset to include only even integers by providing a\\ncustom filter function.\\n124 | Chapter 6: Data Processing with Ray\\n\\nsort  the Dataset.\\nIn addition to these operations, Datasets also support common aggregations you\\nmight expect such as groupby , sum, min, etc. Y ou can also pass a user-defined function\\nfor custom aggregations.\\nBlocks and repartitioning\\nOne important thing to keep in mind when using Ray Datasets is the concept of\\nblocks . Blocks are the underlying chunks of data that make up a Dataset; operations\\nare applied to the underlying data one block at a time. If the number of blocks in a\\nDataset is too high, each block will be small, and there will be a lot of overhead for\\neach operation. If the number of blocks is too small, operations won’t be able to be\\nparallelized as efficiently.\\nIf we take a peek under the hood of the previous example, we can see that the initial\\ndatasets we created each had 200 blocks by default. When we combined them, the\\nresulting Dataset had 400 blocks. Given that the number of blocks is important for\\nefficiency, we may want to reshuffle the data to match our original 200 blocks and\\nretain the same parallelism. This process of changing the number of blocks is called\\nrepartitioning , and Ray Datasets offers a simple .repartition(num_blocks)  API to\\nachieve it. Let’s use the API to repartition our resulting dataset back into 200 blocks:\\nds1 = ray.data.range(10000)\\nprint(ds1.num_blocks ())  # -> 200\\nds2 = ray.data.range(10000)\\nprint(ds2.num_blocks ())  # -> 200\\nds3 = ds1.union(ds2)\\nprint(ds3.num_blocks ())  # -> 400\\nprint(ds3.repartition (200).num_blocks ())  # -> 200\\nBlocks also control the number of files that are created when we write a Dataset to\\nstorage (so if you want all of the data to be coalesced into a single output file, you\\nshould call .repartition(1)  before writing it).\\nSchemas and data formats\\nTo this point, we’ve been operating on simple Ray Datasets made up only of integers.\\nHowever, for more complex data processing we often want to have a schema, allow‐\\ning us to more easily comprehend the data and enforce types on each column.\\nGiven that Datasets are meant to be the point of interoperation for applications and\\nlibraries on Ray, they are designed to be agnostic to a specific datatype and offer\\nflexibility to read, write, and convert between many popular data formats.  Datasets\\nsupport Arrow’s columnar format, which enables converting between different types\\nRay Datasets | 125\\n\\nof structured data such as Python dictionaries, DataFrames, and serialized Parquet\\nfiles.\\nThe simplest way to create a Dataset with a schema is to create it from a list of Python\\ndictionaries:\\nds = ray.data.from_items ([{\"id\": \"abc\", \"value\": 1}, {\"id\": \"def\", \"value\": 2}])\\nprint(ds.schema())  # -> id: string, value: int64\\nIn this case, the schema was inferred from the keys in the dictionaries we passed in.\\nWe can also convert to/from data types from popular libraries such as Pandas:\\npandas_df  = ds.to_pandas ()  # pandas_df will inherit the schema from our Dataset.\\nHere we went from a Dataset to a Pandas DataFrame, but this also works in reverse: if\\nyou create a Dataset from a DataFrame, it will automatically inherit the schema from\\nthe DataFrame.\\nComputing Over Ray Datasets\\nIn the previous section, we introduced some of the functionality built in with Ray\\nDatasets such as filtering, sorting, and creating unions. However, one of the most\\npowerful parts of Ray Datasets is that it allows you to harness the flexible compute\\nmodel of Ray and perform computations efficiently over large amounts of data.\\nThe primary way to perform a custom transformation on a Dataset is using .map() .\\nThis allows you to pass a custom function that will be applied to the records of a\\nDataset. A basic example might be to square the records of a Dataset:\\nds = ray.data.range(10000).map(lambda x: x ** 2)\\nds.take(5)  # -> [0, 1, 4, 9, 16]\\nIn this example, we passed a simple lambda function, and the data we operated on\\nwas integers, but we could pass any function here and operate on structured data that\\nsupports the Arrow format.\\nWe can also choose to map batches of data instead of individual records\\nusing .map_batches() . Some types of computations are much more efficient when\\nthey’re vectorized , meaning that they use an algorithm or implementation that is more\\nefficient operating on a set of items instead of one at a time.\\nRevisiting our simple example of squaring the values in the Dataset, we can rewrite\\nit to be performed in batches and use the numpy.square -optimized implementation\\ninstead of the naive Python implementation:\\nimport numpy as np\\nds = ray.data.range(10000).map_batches (lambda batch: np.square(batch).tolist())\\nds.take(5)  # -> [0, 1, 4, 9, 16]\\n126 | Chapter 6: Data Processing with Ray\\n\\n2Parquet is a structured, column-oriented format that enables efficient compression, data storage, and data\\nretrieval. Many example and real-world datasets use Parquet, so it is used in the example provided. However,Vectorized computations are especially useful on GPUs when performing deep learn‐\\ning training or inference.  However, generally performing computations on GPUs also\\nhas significant fixed cost due to needing to load model weights or other data into the\\nGPU RAM. For this purpose, Ray Datasets supports mapping data using Ray actors.\\nRay actors are long-lived and can hold state, as opposed to stateless Ray tasks, so\\nwe can cache expensive operations costs by running them in the actor’s constructor\\n(such as loading a model onto a GPU).\\nFor example, to perform batch inference using Datasets, we need to pass a class\\ninstead of a function, specify that this computation should run using actors, and\\nuse .map_batches()  so we can perform vectorized inference. Datasets will automati‐\\ncally autoscale a group of actors to perform the map operation:\\ndef load_model ():\\n    # Returns a dummy model for this example.\\n    # In reality, this would likely load some model weights onto a GPU.\\n    class DummyModel :\\n        def __call__ (self, batch):\\n            return batch\\n    return DummyModel ()\\nclass MLModel:\\n    def __init__ (self):\\n        # load_model() will only run once per actor that\\'s started.\\n        self._model = load_model ()\\n    def __call__ (self, batch):\\n        return self._model(batch)\\nds.map_batches (MLModel, compute=\"actors\" )\\nTo run the inference on a GPU, we would pass num_gpus=1  to the map_batches  call to\\nspecify that the actors running the map function each require a GPU.\\nDataset Pipelines\\nBy default, Dataset operations are blocking, meaning they run synchronously from\\nstart to finish and there is only a single operation happening at a time.  This pattern\\ncan be very inefficient for some workloads, however. For example, consider the\\nfollowing set of Dataset transformations on Parquet data that might be used to do\\nbatch inference for a machine learning model:2\\nRay Datasets | 127\\n\\nthe code could easily be modified to use a different format by changing the read_parquet  and write_parquet\\ncalls.ds = (ray.data.read_parquet (\"s3://my_bucket/input_data\" )  \\n      .map(cpu_intensive_preprocessing )  \\n      .map_batches (gpu_intensive_inference , compute=\"actors\" , num_gpus =1)  \\n      .repartition (10))  \\nds.write_parquet (\"s3://my_bucket/output_predictions\" ) \\nThere are five stages to this process, and each stresses different parts of the system:\\nReading from remote storage requires ingress bandwidth to the cluster and may\\nbe limited by the throughput of the storage system. In this stage, a group of Ray\\ntasks is spawned that will read from remote storage in parallel, and the resulting\\nblocks of data are stored in the Ray object store.\\nPreprocessing the inputs requires CPU resources. The objects from the first\\nphase are passed into a group of tasks that will execute the cpu_intensive_\\npreprocessing  function on each block.\\nVectorized inference on the model requires GPU resources. The same process as\\nin the second stage is repeated for gpu_intensive_inference , except this time\\nthe function is run on actors that are each allocated a GPU, and multiple blocks\\nare passed into each function call (in batches). Actors are used for this step to\\navoid repeatedly reloading the model used for inference onto the GPU.\\nRepartitioning requires network bandwidth within the cluster. After completing\\nstage 3, more tasks are spawned to repartition the data into 10 blocks and write\\neach of those 10 blocks to remote storage.\\nWriting to remote storage requires egress bandwidth from the cluster and may be\\nlimited by the throughput of storage once again.\\nFigure 6-1  depicts a basic implementation where each stage runs in sequence. This\\nnaive implementation idles resources because each stage is blocking and run in\\nsequence.  For example, because GPU resources are used only in the final stage, they\\nwill be idle waiting for all of the data to be loaded and preprocessed.\\n128 | Chapter 6: Data Processing with Ray\\n\\nFigure 6-1. A naive Dataset computation, leading to idle resources between stages\\nIn this scenario, it would be more efficient to pipeline  the stages instead and allow\\nthem to overlap as shown in Figure 6-2 . This means that as soon as some data has\\nbeen read from storage, it is fed into the preprocessing stage, then to the inference\\nstage, and so on.\\nFigure 6-2. An optimized DatasetPipeline that enables overlapping compute between\\nstages and reduces idle resources\\nThis pipelining will improve the overall resource usage of the end-to-end workload,\\nimproving throughput and therefore decreasing the cost it takes to run the computa‐\\ntion (fewer idle resources is better!).\\nDatasets can be converted to DatasetPipelines  using ds.window() , enabling the pipe‐\\nlining behavior that we want in this scenario. A window specifies the number of\\nblocks that will be passed through a stage in the pipeline before being passed to\\nthe next stage. This behavior can be tuned using the blocks_per_window  parameter,\\nwhich defaults to 10.\\nLet’s rewrite the inefficient pseudocode to use a DatasetPipeline instead:\\nds = (ray.data.read_parquet (\"s3://my_bucket/input_data\" )\\n      .window(blocks_per_window =5)\\n      .map(cpu_intensive_preprocessing )\\n      .map_batches (gpu_intensive_inference , compute=\"actors\" , num_gpus =1)\\n      .repartition (10))\\nds.write_parquet (\"s3://my_bucket/output_predictions\" )\\nRay Datasets | 129\\n\\nThe only modification made was the addition of a .window()  call after read_parquet\\nand before the preprocessing stage. Now the Dataset has been converted to a Dataset‐\\nPipeline and its stages will proceed in parallel in five-block windows, decreasing idle\\nresources and improving efficiency.\\nDatasetPipelines can also be created using ds.repeat()  to repeat stages in a pipeline\\na finite or infinite number of times. This will be explored further in the next section,\\nwhere we’ll use it for a training workload. Of course, pipelining can be equally\\nbeneficial for training performance in addition to inference.\\nExample: Training Copies of a Classifier  in Parallel\\nOne of the key benefits of Datasets is that they can be passed between tasks and\\nactors.  In this section, we’ll explore how this functionality can be used to write effi‐\\ncient implementations of complex distributed workloads such as distributed hyper‐\\nparameter tuning and machine learning training. We’ll implement an example of\\ndistributed  training of ML models in this section, a topic that we’ll cover in much\\nmore detail in Chapter 7  when we introduce you to Ray Train.\\nAs discussed in Chapter 5 , a common pattern in machine learning training is to\\nexplore a range of hyperparameters to find the ones that result in the best model.\\nWe may want to run across a wide range of hyperparameters, and doing this naively\\ncould be very expensive.  Ray Data allows us to easily share the same in-memory data\\nacross a range of parallel training runs in a single Python script: we can load and\\npreprocess the data once and then pass a reference to it to many downstream actors\\nwho can read the data from shared memory.\\nAdditionally, sometimes when working with very large datasets, it is not feasible to\\nload the full training data into memory in a single process or on a single machine.\\nIn this case, it’s common to shard  the data, which means to give each worker its own\\nsubset of the data that can fit into memory. This local subset of the data is called\\na data shard . After each worker trains on its shard of data in parallel, the results\\nare combined either synchronously or asynchronously using a parameter server. Two\\nimportant considerations can make this difficult to get right:\\n•Many distributed training algorithms take a synchronous  approach, requiring the •\\nworkers to synchronize their weights after each training epoch. This means there\\nneeds to be some coordination between the workers to maintain consistency\\nbetween which batch of data they are operating on.\\n•It’s important that each worker gets a random sample of the data during each•\\nepoch. A global random shuffle has been shown to perform better than local\\nshuffle or no shuffle.\\n130 | Chapter 6: Data Processing with Ray\\n\\n3SGD stands for stochastic gradient descent , which is a common optimization algorithm used in machine\\nlearning, and specifically deep learning.\\n4In this and the following two chapters, we’re discussing more advanced ML examples that need additional\\ndependencies. We pin the versions of these dependencies here and in the book’s GitHub repo  to ensure the\\nexamples work as expected. Having said that, the examples very likely work with a relatively wide range of\\nversions, as long as you make sure they’re not too old.Figure 6-3  illustrates how the ray.data  package is used to create shards of data\\nforming a Ray Dataset from a given input dataset.\\nFigure 6-3. Creating a Ray Dataset from input data with the ray.data  package\\nLet’s walk through an example of how we can implement this type of pattern using\\nRay Datasets. In the example, we will train multiple copies of a machine learning\\nmodel using different hyperparameters across different workers in parallel.\\nWe’ll be training a scikit-learn SGDClassifier  algorithm on a generated binary classi‐\\nfication dataset, and the hyperparameter we’ll tune is the regularization term of this\\nclassifier.3 The actual details of the ML task and model aren’t too important to this\\nexample: you could replace the model and data with any number of examples. The\\nmain thing to focus on here is how we orchestrate the data loading and computation\\nusing Datasets.\\nTo follow along with the examples in this section, make sure you have Ray Datasets\\nand scikit-learn installed locally:4\\npip install \"ray[data]==2.2.0\" \"scikit-learn==1.0.2\"\\nFirst, let’s define our TrainingWorker  that will train a copy of the classifier on the\\ndata:\\nfrom sklearn import datasets\\nfrom sklearn.linear_model  import SGDClassifier\\nfrom sklearn.model_selection  import train_test_split\\n@ray.remote\\nclass TrainingWorker :\\nRay Datasets | 131\\n\\n    def __init__ (self, alpha: float):\\n        self._model = SGDClassifier (alpha=alpha)\\n    def train(self, train_shard : ray.data.Dataset):\\n        for i, epoch in enumerate (train_shard .iter_epochs ()):\\n            X, Y = zip(*list(epoch.iter_rows ()))\\n            self._model.partial_fit (X, Y, classes=[0, 1])\\n        return self._model\\n    def test(self, X_test: np.ndarray, Y_test: np.ndarray):\\n        return self._model.score(X_test, Y_test)\\nThere are three important things to note about the TrainingWorker :\\n•It’s a simple wrapper around the SGDClassifier  and instantiates it with a given •\\nalpha value.\\n•The main training function happens in the train  method. For each epoch, it •\\ntrains the classifier on the data available.\\n•We also have a test  method that can be used to run the trained model against a •\\ntesting set.\\nNow, let’s instantiate a number of TrainingWorker  instances with different hyper‐\\nparameters ( alpha  values):\\nALPHA_VALS  = [0.00008, 0.00009, 0.0001, 0.00011, 0.00012] \\nprint(f\"Starting {len(ALPHA_VALS )} training workers.\" )\\nworkers = [TrainingWorker .remote(alpha) for alpha in ALPHA_VALS ]\\nNext, we generate training and validation data and convert the training data to a\\nDataset.  Here, we’re using .repeat()  to create a DatasetPipeline. This defines the\\nnumber of epochs that our training will run for. In each epoch, the subsequent\\noperations will be applied to the Dataset, and the workers will be able to iterate over\\nthe resulting data. We also shuffle the data randomly and shard it to be passed to the\\ntraining workers, each getting an equal chunk:\\nX_train, X_test, Y_train, Y_test = train_test_split (  \\n    *datasets .make_classification ()\\n)\\ntrain_ds  = ray.data.from_items (list(zip(X_train, Y_train)))  \\nshards = (train_ds .repeat(10)  \\n          .random_shuffle_each_window ()  \\n          .split(len(workers), locality_hints =workers))  \\nray.get([\\n    worker.train.remote(shard) \\n132 | Chapter 6: Data Processing with Ray\\n\\n    for worker, shard in zip(workers, shards\\n)])  \\nGenerate training and validation data for a classification problem.\\nConvert the training data to a Dataset by using from_items .\\nDefine a DatasetPipeline using .repeat . This is similar to using .window  as we\\nshowed earlier, but it allows us to iterate over the same dataset multiple times (in\\nthis case, 10).\\nShuffle the data randomly each time it is repeated.\\nWe want each worker to have its own local shard of the data, so we split  the\\nDatasetPipeline  into multiple smaller ones that can be passed to each worker.\\nWait for training to complete on all the workers.\\nTo run the training on the workers, we invoke their train  method and pass in\\none shard of the DatasetPipeline  to each. We then block, waiting for training to\\ncomplete across all the workers. To summarize what happens during this phase:\\n1.Each epoch, each worker gets a random shard of the data.1.\\n2.The worker trains its local model on the shard of data assigned to it.2.\\n3.Once a worker has finished training on the current shard, it blocks until the other3.\\nworkers have finished.\\n4.The preceding three steps repeat for the remaining epochs (in this case, 10 total).4.\\nFinally, we can test the trained models from each worker on some test data to\\ndetermine which alpha value produced the most accurate model:\\n# Get validation results from each worker.\\nprint(ray.get([worker.test.remote(X_test, Y_test) for worker in workers]))\\nIn reality, for this type of workload you should reach for Ray Tune or Ray Train,\\nwhich we’ll cover in the next chapter, but this example conveys the power of Ray\\nDatasets for machine learning workloads. In just a few snippets of Python code, we\\nimplemented a complex distributed hyperparameter tuning and training workflow\\nthat could easily be scaled up to hundreds of machines and is agnostic to any\\nframework or specific ML task.\\nRay Datasets | 133\\n\\nExternal Library Integrations\\nWhile Ray Datasets supports a number of common data processing functionalities\\nout of the box, as we’ve discussed, it’s not a replacement for full data processing\\nsystems.  Instead, as shown in Figure 6-4 , it’s more focused on performing “last mile”\\nprocessing such as basic data loading, cleaning, and featurization before ML training\\nor inference.\\nFigure 6-4. A typical workflow  using Ray for machine learning: use external systems for\\nprimary data processing and ETL, use Ray Datasets for last-mile preprocessing\\nHowever, a number of other, more fully featured DataFrame and relational data\\nprocessing systems integrate with Ray, such as:\\n•Dask on Ray•\\n•RayDP (Spark on Ray)•\\n•Modin (Pandas on Ray)•\\n•MARS on Ray•\\nThese are standalone data processing libraries you may be familiar with outside the\\ncontext of Ray. Each of these tools has an integration with the Ray Core that enables\\nmore expressive data processing than comes with the built-in Ray Datasets while still\\nusing Ray’s deployment tooling, scalable scheduling, and shared memory object store\\nfor exchanging data. As shown in Figure 6-5 , this complements Ray Datasets and\\nenables end-to-end data processing on Ray.\\nFigure 6-5  shows the benefit of Ray Data ecosystem integrations, enabling more\\nexpressive data processing on Ray. These libraries integrate with Ray Datasets to feed\\ninto downstream libraries such as Ray Train.\\n134 | Chapter 6: Data Processing with Ray\\n\\nFigure 6-5. Ray Data ecosystem integrations enable more expressive data processing\\non Ray\\nFor the purposes of this book, we’ll explore Dask on Ray in slightly more depth to\\ngive you a feel for what these integrations look like. If you’re interested in the details\\nof a specific integration, see the latest Ray documentation  for up-to-date information.\\nTo follow along with the examples in this section, install Ray and Dask:\\npip install \"ray[data]==2.2.0\" \"dask==2022.2.0\"\\nDask  is a Python library for parallel computing that is specifically targeted at scaling\\nanalytics and scientific computing workloads to a cluster. One of the most popular\\nfeatures of Dask is Dask DataFrames , which offers a subset of the Pandas DataFrame\\nAPI that can be scaled to a cluster of machines in cases where processing in memory\\non a single node is not feasible. DataFrames work by creating a task graph  that is\\nsubmitted to a scheduler for execution. The most typical way to execute Dask Data‐\\nFrames operations is using the Dask distributed scheduler, but there is a pluggable\\nAPI that allows other schedulers to execute these task graphs as well.\\nRay comes packaged with a Dask scheduler backend, allowing Dask DataFrame task\\ngraphs to be executed as Ray tasks and therefore use the Ray scheduler and shared\\nmemory object store.  This doesn’t require modifying the core DataFrames code at all;\\ninstead, to run using Ray, all you need to do is first connect to a running Ray Cluster\\n(or run Ray locally) and then enable the Ray scheduler backend:\\nimport ray\\nfrom ray.util.dask  import enable_dask_on_ray\\nray.init()  # Start or connect to Ray.\\nenable_dask_on_ray ()  # Enable the Ray scheduler backend for Dask.\\nExternal Library Integrations | 135\\n\\nNow we can run regular Dask DataFrames code and have it scaled across the Ray\\nCluster. For example, we might want to do some time-series analysis using standard\\nDataFrame operations like filter  and groupby  and compute the standard deviation\\n(example taken from Dask documentation):\\nimport dask\\ndf = dask.datasets .timeseries ()\\ndf = df[df.y > 0].groupby(\"name\").x.std()\\ndf.compute()  # Trigger the task graph to be evaluated.\\nIf you’re used to Pandas or other DataFrame libraries, you might wonder why we\\nneed to call df.compute() . This is because Dask is lazy by default and will compute\\nresults only on demand, allowing it to optimize the task graph that will be executed\\nacross the cluster.\\nOne of the most powerful aspects of Dask on Ray is that it integrates very nicely\\nwith Ray Datasets. We can convert a Ray Dataset to a Dask DataFrame and vice versa\\nusing built-in utilities:\\nimport ray\\nds = ray.data.range(10000)\\n# Convert the Dataset to a Dask DataFrame.\\ndf = ds.to_dask()\\nprint(df.std().compute())  # -> 2886.89568\\n# Convert the Dask DataFrame back to a Dataset.\\nds = ray.data.from_dask (df)\\nprint(ds.std())  # -> 2886.89568\\nThis simple example might not look impressive because we’re able to compute the\\nstandard deviation using either Dask DataFrames or Ray Datasets. However, as you’ll\\nsee in the next section when we build an end-to-end ML pipeline, this enables power‐\\nful workflows. For example, we can use the full expressiveness of DataFrames to do\\nour featurization and preprocessing and then pass the data directly into downstream\\noperations such as distributed training or inference while keeping everything in\\nmemory. This highlights how Ray Datasets enables a wide range of use cases on top of\\nRay and how integrations like Dask on Ray make the ecosystem even more powerful.\\nBuilding an ML Pipeline\\nAlthough we were able to build a simple distributed training application from scratch\\nin the previous section, there were many edge cases, opportunities for performance\\noptimization, and usability features that we would want to address to build a real-\\nworld application. As you’ve learned in Chapters 4 and 5, Ray has an ecosystem of\\nlibraries that enable us to build production-ready ML applications. In this section,\\n136 | Chapter 6: Data Processing with Ray\\n\\n5Another challenge of relying on many tools is cultural. Knowledge transfer of best practices can be costly,\\nespecially in larger companies.we’ll explore how to use Datasets as the “glue layer” to build an ML pipeline from end\\nto end.\\nTo successfully productionize a machine learning model, we need to collect and\\ncatalog data using standard ETL processes. However, that’s not the end of the story:\\nto train a model, we also often need to do featurization of the data before feeding\\ninto our training process, and how we feed the data into training can strongly impact\\ncost and performance. After training a model, we’ll also want to run inference across\\nmany different datasets—that’s the whole point of training the model, after all!\\nThough this might seem like just a chain of steps, in practice the data processing\\nworkflow for ML is an iterative process of experimentation to define the right\\nset of features and train a high-performing model on them. Efficiently loading,\\ntransforming, and feeding the data into training and inference is also crucial for\\nperformance, which translates directly to cost for compute-intensive models. Imple‐\\nmenting such ML pipelines often means stitching together multiple systems and\\nmaterializing intermediate results to remote storage between the stages. This has two\\nmajor downsides:\\n•It requires orchestrating many different systems and programs for a single work‐•\\nflow. This can be a lot for any ML practitioner to handle, so many people\\nuse workflow orchestration systems like Apache Airflow . While Airflow has its\\nbenefits, it’s also a lot of complexity to introduce (especially in development).\\n•Running our ML workflow across multiple systems means we need to read from•\\nand write to storage between each stage.5 This incurs significant overhead and\\ncost due to data transfer and serialization.\\nIn contrast, using Ray we are able to build a complete ML pipeline as a single\\napplication that can be run as a single Python script as depicted in Figure 6-6 . The\\necosystem of built-in and third-party libraries makes it possible to mix and match\\nthe right functionality for a given use case and build scalable, production-ready\\npipelines. Importantly, Ray Datasets acts as the glue layer that enables efficient data\\nloading, preprocessing, and computing while avoiding expensive serialization costs\\nand keeping intermediate data in shared memory.\\nFigure 6-6  shows a simplified version of the typical ML workflow and where Ray\\nfits in that flow. ML ’s multiple steps often require iteration; without Ray this means\\nstitching together many independent systems for one end-to-end process. Ray acts\\nas a unified compute layer, enabling most of the workflow to be run as a single\\napplication.\\nBuilding an ML Pipeline | 137\\n\\nFigure 6-6. Ray acting as unified  compute layer for complex ML workflows\\nIn the next chapter we’re going to show you a concrete example of building an\\nend-to-end ML pipeline using Ray Datasets and other libraries in the Ray ecosystem\\nin practice.\\nSummary\\nThis chapter introduced Ray Datasets, a core building block in Ray. Ray Datasets\\noffers built-in functionality for distributed data processing, but its true power lies in\\nits integrations with both first- and third-party libraries. We’ve covered only a small\\nportion of its functionality. For more details, API references, and examples, see the\\ndocumentation .\\nWe’ve also shown you a simple example of distributed training of a scikit-learn\\nclassifier using Ray Datasets and discussed external library integrations such as Dask\\non Ray. Lastly, we’ve indicated the value of building end-to-end ML pipelines using\\nthe Ray ecosystem, which allows you to run your entire workflow in a single Python\\nscript. For data scientists and machine learning engineers, this means faster iteration\\ntime, better ML models, and ultimately more business value.\\n138 | Chapter 6: Data Processing with Ray\\n\\nCHAPTER 7\\nDistributed Training with Ray Train\\nEdward Oakes & Richard Liaw\\nIn Chapter 6  we discussed how to train copies of a simple model on shards of data\\nusing Ray Datasets—but there’s much more to distributed training than that.  As we\\nindicated in Chapter 1 , Ray has a dedicated library for distributed training called Ray\\nTrain. It comes with an extensive suite of machine learning training integrations and\\nallows you to scale your experiments seamlessly on Ray Clusters.\\nWe will start this chapter by showing you why you might need to scale your ML\\ntraining and then introduce you to the different ways of doing so. After that, we’ll\\nintroduce Ray Train and walk through an extensive end-to-end example. We’ll also\\ncover some key concepts you need to know to use Ray Train, such as preprocessors,\\ntrainers, and checkpoints. Finally, we’ll cover some of the more advanced functional‐\\nity that Ray Train provides. As always, you can use the notebook for this chapter  to\\nfollow along.\\nThe Basics of Distributed Model Training\\nMachine learning often requires a lot of heavy computation. Depending on the type\\nof model that you’re training, whether it be a gradient boosted tree or a neural\\nnetwork, you may face some common problems with training ML models:\\n•The time it takes to finish training is too long.•\\n•The data is too large to fit into one machine.•\\n•The model itself is too large to fit into a single machine.•\\n139\\n\\n1This applies specifically to the gradient computation in neural networks.For the first case, training can be accelerated by processing data with increased\\nthroughput.  Some ML algorithms, such as neural networks, can parallelize parts of\\nthe computation to speed up training.1\\nIn the second case, your choice of algorithm may require you to fit all the available\\ndata from a dataset into memory, but the given single-node memory may not be\\nsufficient. If that’s the case, you would need to split the data across multiple nodes\\nand train in a distributed manner. On the other hand, sometimes your algorithm may\\nnot require data to be distributed, but if you’re using a distributed database system\\nto begin with, you still want a training framework that can leverage your distributed\\ndata.\\nWhen your model doesn’t fit into a single machine, you may need to split it up into\\nmultiple parts spread across multiple machines. The approach of splitting models\\nacross multiple machines is called model parallelism . To run into this issue, you\\nfirst need a model that is large enough to not fit into a single machine. Usually,\\nlarge companies like Google or Meta need model parallelism, and they also rely on\\nin-house solutions to handle the distributed training.\\nThe first two problems often arise much earlier in ML development than the third.\\nThe solutions we just sketched for these problems fall under the umbrella of data-\\nparallel training. Instead of splitting up the model across multiple machines, you rely\\non distributed data to speed up training.\\nSpecifically for the first problem, if you can speed up your training process, hopefully\\nwith minimal or no loss in accuracy, and you can do so cost-efficiently, why not go\\nfor it? And if you have distributed data, whether by necessity for your algorithm or\\nthe way you store your data, you need a training solution to deal with it. As you will\\nsee, Ray Train is built for efficient, data-parallel training. Figure 7-1  summarizes the\\ntwo basic types of distributed training.\\nFigure 7-1. Data parallelism versus model parallelism in distributed training\\n140 | Chapter 7: Distributed Training with Ray Train\\n\\nIntroduction to Ray Train by Example\\nRay Train is a library for distributed data-parallel training on Ray.  It offers key\\ntools for different parts of the training workflow, from feature processing to scalable\\ntraining to integrations with ML tracking tools, to export mechanisms for models.\\nIn a basic ML training pipeline you will use the following key components of Ray\\nTrain:\\nTrainers\\nRay Train has several Trainer classes that make it possible to do distributed\\ntraining. Trainers are wrapper classes around third-party training frameworks\\nlike XGBoost, Pytorch, and TensorFlow providing integration with core Ray\\nactors (for distribution), Ray Tune, and Ray Datasets.\\nPredictors\\nOnce you have a trained model, you can use it to get predictions. For batches of\\ninput data you use so-called batch predictors, which are also used to evaluate the\\nperformance of a model on a validation set.\\nAdditionally, Ray Train provides several common Preprocessor  objects and utilities\\nto process dataset objects into consumable features for Trainers. Finally, Ray Train\\nprovides a Checkpoint  class that allows you to save and restore the state of a training\\nrun. In our first walk-through we will not use any preprocessors, but we will cover\\nthem in more detail later.\\nRay Train is built with first-class support for training on large datasets. Along the\\nsame philosophy that you should not have to think about how to parallelize your\\ncode, you can simply “connect” your large dataset to Ray Train without thinking\\nabout how to ingest and feed your data into different parallel workers.\\nLet’s put these components into practice by walking through our first Ray Train\\nexample. To load the training data, we’re going to leverage our knowledge from\\nChapter 6  and make heavy use of Ray Datasets.\\nPredicting Big Tips in NYC Taxi Rides\\nThis section walks through a practical, end-to-end example of building a deep learn‐\\ning pipeline using Ray.  We will build a binary classification model to predict whether\\na taxi ride will result in a big tip (>20% of the fare) using the public New Y ork City\\nTaxi and Limousine Commission (TLC) Trip Record Data . Our workflow will closely\\nresemble that of a typical ML practitioner:\\n1.Load the data, do some basic preprocessing, and compute features we’ll use in1.\\nour model.\\n2.Define a neural network and train it using distributed data-parallel training.2.\\nIntroduction to Ray Train by Example | 141\\n\\n3.Apply the trained neural network to a fresh batch of data.3.\\nThe example will use Dask on Ray and train a PyTorch neural network, but note that\\nnothing here is specific to either of those libraries: Ray Datasets and Ray Train can be\\nused with a wide range of popular machine learning tools. To follow along with the\\nexample code in this section, install Ray, PyTorch, and Dask:\\npip install \"ray[data,train]==2.2.0\" \"dask==2022.2.0\" \"torch==1.12.1\"\\npip install \"xgboost==1.6.2\" \"xgboost-ray>=0.1.10\"\\nIn the following examples, we’ll be loading the data from local disk to make it easy\\nto run the examples on your machine. The data is available in the book’s GitHub\\nrepository . The file paths in the next examples assume you’ve cloned the repository\\nand are running from within its top-level directory.\\nLoading, Preprocessing, and Featurization\\nThe first step in training our model is to load and preprocess it. To do this, we’ll\\nbe using Dask on Ray, for which you’ve already seen a first example in Chapter 6 .\\nDask on Ray gives us a convenient DataFrames API and the ability to scale up the\\npreprocessing across a cluster and efficiently pass it into our training and inference\\noperations.  Here is our code for preprocessing data and building features for our\\nmodel, defined in a single load_dataset  function:\\nimport ray\\nfrom ray.util.dask  import enable_dask_on_ray\\nimport dask.dataframe  as dd\\nLABEL_COLUMN  = \"is_big_tip\"\\nFEATURE_COLUMNS  = [\"passenger_count\" , \"trip_distance\" , \"fare_amount\" ,\\n                   \"trip_duration\" , \"hour\", \"day_of_week\" ]\\nenable_dask_on_ray ()\\ndef load_dataset (path: str, *, include_label =True):\\n    columns = [\"tpep_pickup_datetime\" , \"tpep_dropoff_datetime\" , \"tip_amount\" ,\\n               \"passenger_count\" , \"trip_distance\" , \"fare_amount\" ]\\n    df = dd.read_parquet (path, columns=columns)  \\n    df = df.dropna()  \\n    df = df[(df[\"passenger_count\" ] <= 4) &\\n            (df[\"trip_distance\" ] < 100) &\\n            (df[\"fare_amount\" ] < 1000)]\\n    df[\"tpep_pickup_datetime\" ] = dd.to_datetime (df[\"tpep_pickup_datetime\" ])\\n    df[\"tpep_dropoff_datetime\" ] = dd.to_datetime (df[\"tpep_dropoff_datetime\" ])\\n    df[\"trip_duration\" ] = (df[\"tpep_dropoff_datetime\" ] -\\n142 | Chapter 7: Distributed Training with Ray Train\\n\\n                           df[\"tpep_pickup_datetime\" ]).dt.seconds\\n    df = df[df[\"trip_duration\" ] < 4 * 60 * 60] # 4 hours.\\n    df[\"hour\"] = df[\"tpep_pickup_datetime\" ].dt.hour\\n    df[\"day_of_week\" ] = df[\"tpep_pickup_datetime\" ].dt.weekday  \\n    if include_label :\\n        df[LABEL_COLUMN ] = df[\"tip_amount\" ] > 0.2 * df[\"fare_amount\" ]  \\n    df = df.drop(  \\n        columns=[\"tpep_pickup_datetime\" , \"tpep_dropoff_datetime\" , \"tip_amount\" ]\\n    )\\n    return ray.data.from_dask (df).repartition (100)  \\nDrop unused columns of the initial Dask DataFrame loaded from Parquet files.\\nDo basic cleaning, and drop null values and outliers.\\nAdd three new features: trip duration, hour the trip started, and day of the week.\\nCalculate the label column: if the tip was more or less than 20% of the fare.\\nDrop all unused columns.\\nReturn a repartitioned Ray Dataset created from Dask .\\nThis involves basic data loading and cleaning as well as transforming some columns\\ninto a format that can be used as features in our ML model. For instance, we\\ntransform the pickup and drop-off date-times, which are provided as a string, into\\nthree numerical features: trip_duration , hour , and day_of_week . This is made easy\\nby Dask’s built-in support for Python datetime utilities . If this data is going to be used\\nfor training, we also need to compute the label column.\\nFinally, once we’ve computed our preprocessed Dask DataFrame, we transform it into\\na Ray Dataset so we can pass it into our training and inference processes later.\\nDefining  a Deep Learning Model\\nNow that we’ve cleaned and prepared the data, we need to define a model architecture\\nthat we’ll use for the model. In practice, this would likely be an iterative process\\nand involve researching the state of the art for similar problems. For the sake of\\nour example, we’ll keep things simple and use a basic PyTorch neural network that\\nwe call FarePredictor . The neural network has three linear transformations starting\\nwith the dimension of our feature vector, and then it outputs a value between 0\\nand 1 using a Sigmoid activation function. We also use batch normalization  layers in\\nIntroduction to Ray Train by Example | 143\\n\\nthe network to improve training. This output value will be rounded to produce the\\nbinary prediction of whether or not the ride will result in a big tip:\\nimport torch\\nimport torch.nn  as nn\\nimport torch.nn.functional  as F\\nclass FarePredictor (nn.Module):\\n    def __init__ (self):\\n        super().__init__ ()\\n        self.fc1 = nn.Linear(6, 256)\\n        self.fc2 = nn.Linear(256, 16)\\n        self.fc3 = nn.Linear(16, 1)\\n        self.bn1 = nn.BatchNorm1d (256)\\n        self.bn2 = nn.BatchNorm1d (16)\\n    def forward(self, x):\\n        x = F.relu(self.fc1(x))\\n        x = self.bn1(x)\\n        x = F.relu(self.fc2(x))\\n        x = self.bn2(x)\\n        x = torch.sigmoid(self.fc3(x))\\n        return x\\nDistributed Training with Ray Train\\nNow that we’ve defined the neural network architecture, we need a way to efficiently\\ntrain it on our data. This dataset is very large, so our best bet is to perform data-\\nparallel training .\\nThis means we train the model on multiple machines in parallel, each of which has a\\ncopy of the model and a subset of the data. We will use Ray Train to define a scalable\\ntraining process that will use PyTorch DataParallel  under the hood. We won’t go into\\nconceptual details of the training process here, but we’ll discuss them in the sections\\nfollowing this end-to-end example.\\nThe upcoming example uses imports from the ray.air  module.\\nWe mentioned Ray AIR in Chapter 1  and will introduce it formally\\nin Chapter 10 . For now, treat this module as a useful utility for\\ndefining and running your distributed training processes.\\nIn particular, we’re using a so-called AIR session  that can be\\nused to report  metrics collected during the training process. This\\nfollows a usage pattern similar to the tune.report  API that we\\ndiscussed in Chapter 5 .\\n144 | Chapter 7: Distributed Training with Ray Train\\n\\nThe first thing we need to do is define the core logic needed to train on a batch of\\ndata on each worker in each epoch. This will take in a local shard of the full dataset,\\nrun it through the local copy of the model, and perform backpropagation to update\\nthe model weights. After each epoch, the worker will use Ray Train utilities to report\\nthe result and save the current model weights for use later:\\nfrom ray.air import session\\nfrom ray.air.config  import ScalingConfig\\nimport ray.train  as train\\nfrom ray.train.torch  import TorchCheckpoint , TorchTrainer\\ndef train_loop_per_worker (config: dict):  \\n    batch_size  = config.get(\"batch_size\" , 32)\\n    lr = config.get(\"lr\", 1e-2)\\n    num_epochs  = config.get(\"num_epochs\" , 3)\\n    dataset_shard  = session.get_dataset_shard (\"train\")  \\n    model = FarePredictor ()\\n    dist_model  = train.torch.prepare_model (model)  \\n    loss_function  = nn.SmoothL1Loss ()\\n    optimizer  = torch.optim.Adam(dist_model .parameters (), lr=lr)\\n    for epoch in range(num_epochs ):  \\n        loss = 0\\n        num_batches  = 0\\n        for batch in dataset_shard .iter_torch_batches (  \\n                batch_size =batch_size , dtypes=torch.float\\n        ):\\n            labels = torch.unsqueeze (batch[LABEL_COLUMN ], dim=1)\\n            inputs = torch.cat(\\n                [torch.unsqueeze (batch[f], dim=1) for f in FEATURE_COLUMNS ], \\n                dim=1\\n            )\\n            output = dist_model (inputs)\\n            batch_loss  = loss_function (output, labels)\\n            optimizer .zero_grad ()\\n            batch_loss .backward ()\\n            optimizer .step()\\n            num_batches  += 1\\n            loss += batch_loss .item()\\n        session.report(  \\n            {\"epoch\": epoch, \"loss\": loss},\\n            checkpoint =TorchCheckpoint .from_model (dist_model )\\n        )\\nIntroduction to Ray Train by Example | 145\\n\\n2The code loads only a subset of the data for testing; to run at scale we would use all partitions of the data when\\ncalling load_dataset  and increase num_workers  when training the model.\\nPass a config  dictionary into our training loop to specify some parameters at\\nruntime.\\nRetrieve the data shard for the current worker, using Ray Train’s get_data_shard\\nutility.\\nPrepare the PyTorch model for distributed training by applying prepare_model .\\nDefine a standard PyTorch training loop, iterating over batches of data and\\nperforming backpropagation.\\nThe only nonstandard part is the use of iter_torch_batches  to iterate over the\\ndata shard.\\nAfter each epoch, report  the loss  computed and a model checkpoint  using a Ray\\nsession .\\nIn case you’re not familiar with PyTorch, note that the code between the definition of\\nthe loss_function  and aggregating the batch_loss  to our loss  is a standard training\\nloop for a PyTorch model (except for iterating over batches of the dataset shard,\\nwhich is specific to Ray).\\nNow that the training process has been defined, we need to load the training and\\nvalidation data to feed into our training workers. Here, we call the load_dataset\\nfunction defined earlier that will do preprocessing and featurization.2\\nThis dataset is passed into a TorchTrainer  along with some configuration parameters\\nsuch as the batch size, number of epochs, and number of workers to use.  Each worker\\nwill have access to a shard of the data locally and can iterate over it. After training has\\ncompleted, we can fetch the final trained checkpoint from the returned result object:\\ntrainer = TorchTrainer (\\n    train_loop_per_worker =train_loop_per_worker ,  \\n    train_loop_config ={  \\n        \"lr\": 1e-2, \"num_epochs\" : 3, \"batch_size\" : 64\\n    },\\n    scaling_config =ScalingConfig (num_workers =2),  \\n    datasets ={  \\n        \"train\": load_dataset (\"nyc_tlc_data/yellow_tripdata_2020-01.parquet\" )\\n    },\\n)\\n146 | Chapter 7: Distributed Training with Ray Train\\n\\nresult = trainer.fit()  \\ntrained_model  = result.checkpoint\\nEach TorchTrainer  requires you to specify a train_loop_per_worker .\\nOptionally, if your train loop takes in a config  dictionary, you can specify it as\\ntrain_loop_config .\\nEvery Ray Train Trainer  needs a so-called ScalingConfig  to know how to scale\\ntraining on your Ray Cluster.\\nAnother required argument of each Trainer  is a datasets  dictionary. We define\\na \"train\"  Dataset here, and that is what we use in our training loop.\\nY ou can simply .fit()  a TorchTrainer  to start training.\\nThe last line exports our trained model as a checkpoint for later use in downstream\\napplications like serving and inference. Ray Train generates these checkpoints to\\nserialize intermediate state for training. Checkpoints can include both models and\\nother training artifacts, such as preprocessors.\\nDistributed Batch Inference\\nOnce we’ve trained a model and gotten the best accuracy, the next step is to actually\\napply it in practice. Sometimes this means powering a low-latency service, which\\nwe’ll explore in Chapter 8 , but often the task is to apply the model across batches of\\ndata as they come in.\\nLet’s use the trained model weights from our trained_model  and apply them across a\\nnew batch of data (in this case, it’ll just be another chunk of the same public dataset).\\nTo do this, first we need to load, preprocess, and featurize the data in the same\\nway we did for training. Then we will load our model and map it across the whole\\ndataset. Ray Datasets allows us to do this efficiently with Ray actors, even using GPUs\\njust by changing one parameter. We simply load the trained model checkpoint and\\ncall .predict_pipelined()  on it. This will use Ray Datasets to perform distributed\\nbatch inference across the data:\\nfrom ray.train.torch  import TorchPredictor\\nfrom ray.train.batch_predictor  import BatchPredictor\\nbatch_predictor  = BatchPredictor (trained_model , TorchPredictor )\\nds = load_dataset (\\n    \"nyc_tlc_data/yellow_tripdata_2021-01.parquet\" , include_label =False)\\nbatch_predictor .predict_pipelined (ds, blocks_per_window =10)\\nIntroduction to Ray Train by Example | 147\\n\\nThis example showed how Ray Train and Datasets can be used to implement an\\nend-to-end machine learning workflow as a single application. We were able to fea‐\\nturize the dataset, train and validate an ML model, and then apply that model across\\na different dataset in a single Python script. Ray Datasets acted as the glue layer,\\nconnecting the different stages and avoiding expensive serialization costs between\\nthem. We also used checkpoints to store the model and ran a Ray Train batch\\nprediction job to apply the model to a new dataset.\\nNow, that you’ve seen your first example of Ray Train, let’s take a closer look at its\\nprimary abstraction, the Trainer .\\nMore on Trainers in Ray Train\\nAs you’ve seen in the example of using a TorchTrainer , Trainers are framework-\\nspecific classes that run model training in a distributed fashion. All Ray Trainer\\nclasses share a common interface. At this point, it’s enough to know about two aspects\\nof this interface, namely:\\n•The .fit()  method, which fits a given Trainer  with the given datasets, configu‐ •\\nration, and desired scaling properties\\n•The .checkpoint  property, which returns the Ray Checkpoint  object for this •\\nTrainer\\nRay Train’s Trainers integrate with common machine learning frameworks such as\\nPyTorch, Hugging Face, TensorFlow, Horovod, scikit-learn, and more. There’s even\\na Trainer  specifically for RLlib models, which we don’t cover here. Let’s discuss\\nanother PyTorch example to point out specific aspects of the Trainer  API, with an\\nemphasis on how to migrate an existing PyTorch model to Ray Train.\\nGradient Boosting Frameworks\\nRay Train also offers support for gradient boosted decision tree frameworks.\\nXGBoost is an optimized distributed gradient boosting library designed to be highly\\nefficient, flexible, and portable. It implements ML algorithms under the gradient\\nboosting framework. XGBoost provides a parallel tree boosting that solves many data\\nscience problems quickly and accurately.\\nLightGBM is a gradient boosting framework based on tree-based learning algorithms.\\nCompared to XGBoost, it is a relatively new framework but one that is quickly\\nbecoming popular in both academic and production use cases.\\nTo use these frameworks, you can use the XGBoostTrainer  and LightGBMTrainer\\nclasses, respectively.\\n148 | Chapter 7: Distributed Training with Ray Train\\n\\nIn this example we want to focus on the details of Ray Train itself, so we’ll use a\\nmuch simpler training dataset and a small neural network that takes random noise\\nas input. We define a three-layer NeuralNetwork , an explicit training_loop , similar\\nto the one we saw in the previous section, that you can use to train the model. For\\nclarity, we extract the training code run for each epoch in a helper function called\\ntrain_one_epoch :\\nimport torch\\nimport torch.nn  as nn\\nimport torch.nn.functional  as F\\nfrom ray.data  import from_torch\\nnum_samples  = 20\\ninput_size  = 10\\nlayer_size  = 15\\noutput_size  = 5\\nnum_epochs  = 3\\nclass NeuralNetwork (nn.Module):\\n    def __init__ (self):\\n        super().__init__ ()\\n        self.fc1 = nn.Linear(input_size , layer_size )\\n        self.relu = nn.ReLU()\\n        self.fc2 = nn.Linear(layer_size , output_size )\\n    def forward(self, x):\\n        x = F.relu(self.fc1(x))\\n        x = self.fc2(x)\\n        return x\\ndef train_data ():\\n    return torch.randn(num_samples , input_size )  \\ninput_data  = train_data ()\\nlabel_data  = torch.randn(num_samples , output_size )\\ntrain_dataset  = from_torch (input_data )  \\ndef train_one_epoch (model, loss_fn, optimizer ):  \\n    output = model(input_data )\\n    loss = loss_fn(output, label_data )\\n    optimizer .zero_grad ()\\n    loss.backward ()\\n    optimizer .step()\\ndef training_loop ():  \\n    model = NeuralNetwork ()\\nMore on Trainers in Ray Train | 149\\n\\n    loss_fn = nn.MSELoss()\\n    optimizer  = torch.optim.SGD(model.parameters (), lr=0.1)\\n    for epoch in range(num_epochs ):\\n        train_one_epoch (model, loss_fn, optimizer )\\nUse a randomly generated dataset.\\nCreate a Ray Dataset from this data with from_torch .\\nExtract the PyTorch code to train one epoch into a helper function.\\nThis training loop can be run as is to train your PyTorch model on a single\\nmachine.\\nTypically, if you wanted to distribute your training without Ray Train, you would\\nneed to do these two things:\\n•Establish a backend that coordinates interprocess communication.•\\n•Instantiate multiple parallel processes on each node that you want to distribute•\\nyour training across.\\nIn contrast, let’s see how easy it is to use Ray Train to distribute your training process.\\nMigrating to Ray Train with Minimal Code Changes\\nWith Ray Train, you can make a one-line change to your code to take care of both\\ninterprocess communication and process instantiation under the hood.\\nThe code change is made by calling prepare_model  on the PyTorch model that you\\nplan to train. This change is literally the only difference between the training_loop\\ndefined before and the following distributed_training_loop :\\nfrom ray.train.torch  import prepare_model\\ndef distributed_training_loop ():\\n    model = NeuralNetwork ()\\n    model = prepare_model (model)  \\n    loss_fn = nn.MSELoss()\\n    optimizer  = torch.optim.SGD(model.parameters (), lr=0.1)\\n    for epoch in range(num_epochs ):\\n        train_one_epoch (model, loss_fn, optimizer )\\nPrepare the model for distributed training by calling prepare_model .\\n150 | Chapter 7: Distributed Training with Ray Train\\n\\nThen, we can instantiate a TorchTrainer  model, which has three required arguments:\\ntrain_loop_per_worker\\nThis function trains your model for each worker. It has access to the datasets\\nprovided and can be fed an optional config  dictionary that can be passed to your\\ntrainer as train_loop_config . This function will typically report metrics, e.g.,\\nvia a session .\\ndatasets\\nThis dictionary can contain several keys with Ray Datasets as values. It’s kept\\nflexible so that you can have training data, validation data, or any other type of\\ndata needed for your training loop.\\nscaling_config\\nThis ScalingConfig  object specifies how your training should scale. For\\ninstance, you can specify the number of training workers with num_workers  and\\nthe use of GPUs with the use_gpu  flag. We’ll elaborate on this more in the next\\nsection.\\nHere’s how you set up your Trainer in our example:\\nfrom ray.air.config  import ScalingConfig\\nfrom ray.train.torch  import TorchTrainer\\ntrainer = TorchTrainer (\\n    train_loop_per_worker =distributed_training_loop ,\\n    scaling_config =ScalingConfig (\\n        num_workers =2,\\n        use_gpu=False\\n    ),\\n    datasets ={\"train\": train_dataset }\\n)\\nresult = trainer.fit()\\nWith an initialized Trainer, you can call fit() , which will execute the training across\\nyour Ray Cluster. Figure 7-2  summarizes working with a TorchTrainer .\\nMore on Trainers in Ray Train | 151\\n\\nFigure 7-2. Working with a TorchTrainer  requires you to specify a training loop,\\ndatasets, and a scaling configuration\\nScaling Out Trainers\\nThe Ray Train philosophy is that the user should not need to think about how to\\nparallelize their code. Specifying a scaling_config  allows you to scale your training\\nwithout writing distributed logic and to declaratively specify the compute resources\\nused by a Trainer. The nice thing about this specification is that you don’t need\\nto think about the underlying hardware. In particular, you can use hundreds of\\nworkers by specifying the parameters of your cluster nodes in your ScalingConfig\\naccordingly:\\nimport ray\\nfrom ray.air.config  import ScalingConfig\\nfrom ray.train.xgboost  import XGBoostTrainer\\nray.init(address=\"auto\")  \\nscaling_config  = ScalingConfig (num_workers =200, use_gpu=True)  \\ntrainer = XGBoostTrainer (  \\n    scaling_config =scaling_config ,\\n    # ...\\n)\\nConnect to an existing, large Ray Cluster here.\\nDefine a ScalingConfig  according to the cluster’s available resources.\\nWe can then use Ray Train’s XGBoost integration to train a model on this cluster.\\n152 | Chapter 7: Distributed Training with Ray Train\\n\\nPreprocessing with Ray Train\\nData preprocessing is a common technique for transforming raw data into features\\nfor an ML model, and we’ve seen many examples in this and the previous chapter.  So\\nfar we’ve “manually” preprocessed our data by writing custom functions to transform\\nour data into the right format. But Ray Train has several built-in preprocessors for\\ncommon use cases and also provides interfaces to define your own custom logic.\\nThe Preprocessor  is the core class offered by Ray Train for handling data prepro‐\\ncessing. Each preprocessor has the following APIs:\\n.transform()\\nUsed to process and apply a processing transformation to a dataset.\\n.fit()\\nUsed to calculate and store aggregate state about the dataset on a preprocessor.\\nReturns self  for chaining.\\n.fit_transform()\\nSyntactic sugar for performing transformations that require aggregate state. May\\nbe optimized at the implementation level for specific preprocessors.\\n.transform_batch()\\nUsed to apply the same transformation on batches for prediction.\\nWe often want to make sure we can use the same data preprocessing operations at\\ntraining time and at serving time.  Training-serving skew, a major problem in deploy‐\\ning ML, describes a situation where there is a difference between performance during\\ntraining and performance during serving. This skew is often caused by a discrepancy\\nbetween how you handle data in the training and serving pipelines. Thus, you want to\\nmake sure you have consistent data handling for training and serving.\\nY ou can use the preceding preprocessors by passing them to the constructor of a\\ntrainer . That means once you’ve created a preprocessor , you don’t have to apply\\nit to your Ray Datasets manually. Instead, you can pass it to your trainer , and Ray\\nTrain will take care of applying it in a distributed fashion. Here is how this works\\nschematically:\\nfrom ray.data.preprocessors  import StandardScaler\\nfrom ray.train.xgboost  import XGBoostTrainer\\ntrainer = XGBoostTrainer (\\n    preprocessor =StandardScaler (...),\\n    # ...\\n)\\nresult = trainer.fit()\\nMore on Trainers in Ray Train | 153\\n\\nSome preprocessing operators such as one-hot encoders are easy to run in training\\nand transfer to serving. However, other operators such as those that do standardiza‐\\ntion are a bit trickier, since you don’t want to do large data crunching (to find the\\nmean of a particular column) during serving time.\\nFortunately, the Ray Train preprocessors are serializable so you can easily get consis‐\\ntency from training to serving just by serializing these operators. For instance, you\\ncan simply pickle a preprocessor  like this:\\nimport pickle\\nfrom ray.data.preprocessors  import StandardScaler\\npreprocessor =StandardScaler (...)\\npickle.dumps(preprocessor )\\nNext, let’s discuss a concrete example of a training procedure using preprocessors, by\\nalso showing you how to tune your Trainer’s hyperparameters.\\nIntegrating Trainers with Ray Tune\\nRay Train provides an integration with Ray Tune that allows you to perform HPO in\\njust a few lines of code. Tune will create one trial per hyperparameter configuration.\\nIn each trial, a new Trainer will be initialized and run the training function with its\\ngenerated configuration.\\nIn the following code, we create an XGBoostTrainer  and specify hyperparameter\\nranges for common hyperparameters. Specifically, we’re going to choose between\\ntwo different preprocessors  in our training scenario. To be precise, we will use a\\nStandardScaler , which translates and scales each specified column by its mean\\nand standard deviation (the resulting columns will thus follow a standard normal\\ndistribution) and MinMaxScaler , which simply scales each column to the range [0, 1].\\nHere’s the corresponding parameter space we will search over next:\\nimport ray\\nfrom ray.air.config  import ScalingConfig\\nfrom ray import tune\\nfrom ray.data.preprocessors  import StandardScaler , MinMaxScaler\\ndataset = ray.data.from_items (\\n    [{\"X\": x, \"Y\": 1} for x in range(0, 100)] +\\n    [{\"X\": x, \"Y\": 0} for x in range(100, 200)]\\n)\\nprep_v1 = StandardScaler (columns=[\"X\"])\\nprep_v2 = MinMaxScaler (columns=[\"X\"])\\nparam_space  = {\\n    \"scaling_config\" : ScalingConfig (\\n154 | Chapter 7: Distributed Training with Ray Train\\n\\n        num_workers =tune.grid_search ([2, 4]),\\n        resources_per_worker ={\\n            \"CPU\": 2,\\n            \"GPU\": 0,\\n        },\\n    ),\\n    \"preprocessor\" : tune.grid_search ([prep_v1, prep_v2]),\\n    \"params\" : {\\n        \"objective\" : \"binary:logistic\" ,\\n        \"tree_method\" : \"hist\",\\n        \"eval_metric\" : [\"logloss\" , \"error\"],\\n        \"eta\": tune.loguniform (1e-4, 1e-1),\\n        \"subsample\" : tune.uniform(0.5, 1.0),\\n        \"max_depth\" : tune.randint(1, 9),\\n    },\\n}\\nWe can now create a Trainer  as before, this time going with an XGBoostTrainer  and\\nthen passing it to an instance of a Tuner  from Ray Tune, which we can .fit()  just\\nlike the trainer  itself:\\nfrom ray.train.xgboost  import XGBoostTrainer\\nfrom ray.air.config  import RunConfig\\nfrom ray.tune  import Tuner\\ntrainer = XGBoostTrainer (\\n    params={},\\n    run_config =RunConfig (verbose=2),\\n    preprocessor =None,\\n    scaling_config =None,\\n    label_column =\"Y\",\\n    datasets ={\"train\": dataset}\\n)\\ntuner = Tuner(\\n    trainer,\\n    param_space =param_space ,\\n)\\nresults = tuner.fit()\\nNote that we’re using another component of a Ray Trainer here that you haven’t seen\\nbefore, the RunConfig . This configuration  is used for all runtime options of a Trainer,\\nin our case the log verbosity of the experiment (0 would be silent; 1 gives only status\\nupdates; 2, the default, gives status updates and brief results; and 3 gives detailed\\nresults).\\nCompared to other distributed hyperparameter tuning solutions, Ray Tune and Ray\\nTrain have some unique features. Ray’s solution is fault-tolerant, and it has the ability\\nMore on Trainers in Ray Train | 155\\n\\n3MLflow and TensorBoard are open source projects for ML experiment tracking and visualization. They are\\nvery useful for monitoring the progress of your machine learning model.to specify the dataset and preprocessor as a parameter, as well as to adjust the number\\nof workers during training time.\\nUsing Callbacks to Monitor Training\\nTo explore one more feature of Ray Train, you may want to plug in your training\\ncode with your favorite experiment management framework. Ray Train provides an\\ninterface to fetch intermediate results and callbacks to process or log them. It comes\\nwith built-in callbacks for popular tracking frameworks, but you can implement your\\nown callback via Tune’s LoggerCallback  interface.\\nFor instance, you can log results in JSON format using the JsonLoggerCallback ,\\nto TensorBoard using the TBXLoggerCallback , or to MLflow using the MLFLowLogger\\nCallback .3 The following example shows how you can use all three in one single\\ntraining run by specifying a list of callbacks :\\nfrom ray.air.callbacks.mlflow  import MLflowLoggerCallback\\nfrom ray.tune.logger  import TBXLoggerCallback , JsonLoggerCallback\\ntraining_loop  = ...\\ntrainer = ...\\ntrainer.fit(\\n    training_loop ,\\n    callbacks =[\\n        MLflowLoggerCallback (),\\n        TBXLoggerCallback (),\\n        JsonLoggerCallback ()\\n])\\nSummary\\nIn this chapter, we’ve discussed the basics of distributed model training and showed\\nyou how to run data-parallel training with Ray Train. We walked you through an\\nextensive example that used both Ray Data and Ray Train on an interesting dataset.\\nSpecifically, we demonstrated how to use Dask on Ray to load, preprocess, and\\nfeaturize your datasets, and then use Ray Train to run a distributed PyTorch training\\nloop. We then discussed Ray Trainers in more detail and showed how they integrate\\nwith Ray Tune via Tuners, how you can use them with preprocessors, and how you\\ncan use callbacks to monitor training.\\n156 | Chapter 7: Distributed Training with Ray Train\\n\\nCHAPTER 8\\nOnline Inference with Ray Serve\\nEdward Oakes\\nIn Chapters 6 and 7 you learned how to use Ray to process data, train ML models,\\nand apply them in a batch inference setting. However, many of the most exciting use\\ncases for machine learning involve online inference .\\nOnline inference is the process of using ML models to enhance API endpoints that\\nusers interact with directly or indirectly.  This is important in situations where latency\\nmatters: you can’t simply apply models to data behind the scenes and serve the\\nresults.  There are many real-world examples of use cases where online inference can\\nprovide a lot of value, for example:\\nRecommendation systems\\nProviding recommendations for products (e.g., online shopping) or content (e.g.,\\nsocial media) is a bread-and-butter use case for machine learning. While it’s\\npossible to do this offline, recommendation systems often benefit from reacting\\nto users’ preferences in real time. This requires performing online inference\\nusing recent behavior as a key feature.\\nChat bots\\nOnline services often have real-time chat windows to provide support to custom‐\\ners from the comfort of their keyboard. Traditionally, these chat windows were\\nstaffed by customer support staff, but a recent trend to reduce labor costs and\\nimprove time-to-resolution is replacing them with ML-powered chat bots that\\ncan be online 24/7. These chat bots require a sophisticated mix of multiple\\nmachine learning techniques and must be able to respond to customer input in\\nreal time.\\n157\\n\\nEstimating arrival times\\nRide sharing, navigation, and food delivery services all rely on being able to\\nprovide an accurate estimate of arrival times (e.g., for your driver, yourself, or\\nyour dinner). Providing accurate estimates is very difficult because it requires\\naccounting for real-world factors such as traffic patterns, weather, and accidents.\\nEstimates are also refreshed many times over the course of one trip.\\nThese are just a few examples of how applying machine learning in an online setting\\ncan provide a lot of value in application domains that are traditionally very difficult\\n(imagine writing logic by hand to estimate arrival times!). The list of applications\\ngoes on: a number of nascent domains such as self-driving cars, robotics, and video-\\nprocessing pipelines are also being redefined by machine learning.\\nAll these applications share one crucial requirement: latency.  In the case of online\\nservices, low latency is paramount for providing a good user experience. For applica‐\\ntions that interact with the real world (such as robotics or self-driving cars), higher\\nlatency can have even stronger implications for safety or accuracy.\\nThis chapter will provide a gentle introduction to Ray Serve, a Ray-native library that\\nenables building online inference applications on top of Ray. First, we will discuss\\nthe challenges of online inference that Ray Serve addresses. Then, we’ll cover the\\narchitecture of Ray Serve and introduce its core functionality. Finally, we will use\\nRay Serve to build an end-to-end online inference API consisting of multiple natural\\nlanguage processing models. Y ou can follow along with the code in the notebook for\\nthis chapter .\\nKey Characteristics of Online Inference\\nIn the previous section we discussed that the main goal of online inference is to inter‐\\nact with ML models with low latency. However, this has long been a key requirement\\nfor API backends and web servers, so a natural question is: what’s different about\\nserving ML models?\\nML Models Are Compute Intensive\\nMany of the challenges in online inference are a result of one key characteristic:\\nML models are very compute intensive. Compared to traditional web serving where\\nrequests are primarily handled by I/O-intensive database queries or other API calls,\\nmost ML models boil down to performing many linear algebra computations: pro‐\\nvide a recommendation, estimate an arrival time, or detect an object in an image.\\nThis is especially true for the recent trend of “deep learning, ” which is an arm\\nof ML characterized by neural networks that are growing larger over time. Often,\\ndeep learning models can also benefit significantly from using specialized hardware\\n158 | Chapter 8: Online Inference with Ray Serve\\n\\nsuch as GPUs or TPUs, which have special-purpose instructions optimized for ML\\ncomputations and enable vectorized computations across multiple inputs in parallel.\\nMany online inference applications must be run 24/7. When combined with the\\nfact that ML models are compute intensive, operating online inference services can\\nbe very expensive, requiring allocation of many CPUs and GPUs at all times. The\\nprimary challenges of online inference boil down to serving models in a way that\\nminimizes end-to-end latency while also reducing cost. Key properties that online\\ninference systems provide to satisfy these requirements include:\\n•Support for specialized hardware such as GPUs and TPUs.•\\n•The ability to scale the resources used for a model up and down in response to•\\nrequest load.\\n•Support for request batching to take advantage of vectorized computations.•\\nML Models Aren’t Useful in Isolation\\nOften when ML is discussed in the academic or research setting, the focus is on\\nan individual, isolated task such as object recognition or classification. However,\\nreal-world applications are not usually so clear cut and well defined. Instead, a\\ncombination of multiple ML models and business logic is required to solve a problem\\nfrom end to end. For example, consider a product recommendation  use case. While\\nwe could apply a multitude of known ML techniques to the core problem of making a\\nrecommendation, a lot of equally important challenges exist around the edges, many\\nof which will be specific to each use case:\\n•Validating inputs and outputs to ensure the result returned to the user makes•\\nsense semantically. Often, we may have some manual rules such as avoiding\\nreturning the same recommendation to a user multiple times in succession.\\n•Fetching up-to-date information about the user and available products and con‐•\\nverting it into features for the model (in some cases, this may be performed by an\\nonline feature store).\\n•Combining the results of multiple models using manual rules such as filtering the•\\ntop results or selecting the model with highest confidence.\\nImplementing an online inference API requires the ability to integrate all of these\\npieces into one unified service. Therefore, it’s important to have the flexibility to\\ncompose multiple models along with custom business logic. These pieces can’t be\\nviewed in isolation: the “glue” logic often needs to evolve alongside the models\\nthemselves.\\nKey Characteristics of Online Inference | 159\\n\\nAn Introduction to Ray Serve\\nRay Serve is a scalable compute layer for serving ML models on top of Ray. Serve\\nis framework-agnostic, meaning that it isn’t tied to a specific ML library; rather, it\\ntreats models as ordinary Python code. Additionally, it allows you to flexibly combine\\nnormal Python business logic alongside ML models. This makes it possible to build\\nonline inference services completely: a Serve application could validate user input,\\nquery a database, perform scalable inference across multiple ML models, and com‐\\nbine, filter, and validate the output—all in the process of handling a single inference\\nrequest. Indeed, combining the results of multiple ML models is one of the key\\nstrengths of Ray Serve, as you’ll see in “Multimodel Inference Graphs” on page 166 .\\nWhile flexible, Ray Serve has purpose-built features for compute-heavy ML models,\\nenabling dynamic scaling and resource allocation to ensure that request load can\\nbe handled efficiently across many CPUs and/or GPUs. Here, Serve inherits a lot of\\nbenefits from being built on top of Ray: it’s scalable to hundreds of machines, offers\\nflexible scheduling policies, and offers low-overhead communication across processes\\nusing Ray’s core APIs.\\nThis section incrementally introduces core functionality from Ray Serve with a focus\\non how it helps address the challenges of online inference previously outlined. To\\nfollow along with the code samples in this section, you’ll need the following Python\\npackages installed locally:\\npip install \"ray[serve]==2.2.0\" \"transformers==4.21.2\" \"requests==2.28.1\"\\nRunning the examples assumes that you have the code saved locally in a file named\\napp.py  in the current working directory.\\nArchitectural Overview\\nRay Serve is built on top of Ray, so it inherits a lot of benefits such as scalability,\\nlow-overhead communication, an API well suited to parallelism, and the ability to\\nleverage shared memory via the object store. The core primitive in Ray Serve is a\\ndeployment , which you can think of as a managed group of Ray actors that can be\\naddressed together and that will handle requests load-balanced across them.  Each\\nactor in a deployment is called a replica  in Ray Serve. Often, a deployment will map\\none-to-one with an ML model, but deployments can contain arbitrary Python code,\\nso they might also house business logic.\\nRay Serve enables exposing deployments over HTTP and defining the input parsing\\nand output logic. However, one of the most important features of Ray Serve is\\nthat deployments can also call into each other directly using a native Python API,\\nwhich will translate to direct actor calls between the replicas. This enables flexible,\\nhigh-performance composition of models and business logic; you’ll see this in action\\nlater in the section.\\n160 | Chapter 8: Online Inference with Ray Serve\\n\\n1Deployments can also send traffic to each other directly. This enables building more complex applications\\ninvolving model composition or mixing ML models with business logic.Figure 8-1  provides a basic view of how a Ray Serve application runs on top of a\\nRay Cluster: multiple deployments are placed across a Ray Cluster. Each deployment\\nis made up of one more “replicas, ” each of which is a Ray actor. Incoming traffic is\\nrouted through an HTTP proxy that will load-balance requests across the replicas.1\\nFigure 8-1. The architecture of a Ray Serve application\\nUnder the hood, the deployments making up a Ray Serve application are managed\\nby a centralized controller  actor.  This is a detached actor managed by Ray that will\\nbe restarted upon failure. The controller is in charge of creating and updating replica\\nactors, broadcasting updates to other actors in the system, and performing health\\nchecking and failure recovery. If a replica or an entire Ray node crashes for any\\nreason, the controller will detect the failures and ensure that the actors are recovered\\nand can continue serving traffic.\\nDefining  a Basic HTTP Endpoint\\nThis section will introduce Ray Serve by defining a simple HTTP endpoint wrapping\\na single ML model. The model we’ll deploy is a sentiment classifier: given a text\\ninput, it will predict if the output had a positive or negative sentiment. We’ll be\\nusing a pretrained sentiment classifier from the Hugging Face Transformers library ,\\nwhich provides a simple Python API for pretrained models that will abstract away the\\ndetails of the model and allow us to focus on the serving logic.\\nTo deploy this model using Ray Serve, we need to define a Python class and turn\\nit into a Serve deployment  using the @serve.deployment  decorator. The decorator\\nallows us to pass a number of useful options to configure the deployment; we will\\nexplore some of those options in “Scaling and Resource Allocation” on page 163 :\\nAn Introduction to Ray Serve | 161\\n\\nfrom ray import serve\\nfrom transformers  import pipeline\\n@serve.deployment\\nclass SentimentAnalysis :\\n    def __init__ (self):\\n        self._classifier  = pipeline (\"sentiment-analysis\" )\\n    def __call__ (self, request) -> str:\\n        input_text  = request.query_params [\"input_text\" ]\\n        return self._classifier (input_text )[0][\"label\"]\\nThere are a few important points to note here. First, we instantiate our model in\\nthe constructor of the class. This model may be very large, so downloading it and\\nloading it into memory can be slow (up to several minutes). In Ray Serve, the code in\\nthe constructor will be run only once in each replica on startup, and any properties\\ncan be cached for future use. Second, we define the logic to handle a request in the\\n__call__  method. This takes a Starlette  HTTP request as input and can return any\\nJSON-serializable output. In this case, we’ll return a single string from the output of\\nour model: \"POSITIVE\"  or \"NEGATIVE\" .\\nOnce a deployment is defined, we use the .bind()  API to instantiate a copy of it.\\nThis is where we can pass optional arguments to the constructor to configure the\\ndeployment (such as a remote path to download model weights from). Note that this\\ndoesn’t actually run the deployment but packages it with its arguments (this will be\\nmore important later when we combine multiple models):\\nbasic_deployment  = SentimentAnalysis .bind()\\nWe can run the bound deployment using the serve.run  Python API or correspond‐\\ning serve run  CLI command. Assuming you save the preceding code in a file called\\napp.py , you can run it locally with the following command:\\nserve run app:basic_deployment\\nThis will instantiate a single replica of our deployment and host it behind a local\\nHTTP server. To test it, we can use the Python requests  package:\\nimport requests\\nprint(requests .get(\\n    \"http://localhost:8000/\" , params={\"input_text\" : \"Hello friend!\" }\\n).json())\\nTesting the sentiment classifier on a sample input text of \"Hello friend!\" , it cor‐\\nrectly classifies the text as positive!\\n162 | Chapter 8: Online Inference with Ray Serve\\n\\n2Under the hood, Ray Serve serializes the user-provided FastAPI app object. Then, when each deployment\\nreplica runs, Ray Serve deserializes and runs the FastAPI app the same way it would be run in a typical web\\nserver. At runtime, there will be an independent FastAPI server running in each Ray Serve replica.This example is effectively the “hello world” of Ray Serve: we deployed a single\\nmodel behind a basic HTTP endpoint. Note, however, that we had to manually parse\\nthe input HTTP request and feed it into our model. For this basic example it was\\njust a single line of code, but real-world applications often take a more complex\\nschema as input, and hand-writing HTTP logic can be tedious and error-prone.  To\\nenable writing more expressive HTTP APIs, Serve integrates with the FastAPI Python\\nframework .2\\nA Serve deployment can wrap a FastAPI  app, using its expressive APIs for parsing\\ninputs and configuring HTTP behavior. In the following example, we rely on FastAPI\\nto handle parsing the input_text  query parameter, allowing removal of the boiler‐\\nplate parsing code:\\nfrom fastapi import FastAPI\\napp = FastAPI()\\n@serve.deployment\\n@serve.ingress(app)\\nclass SentimentAnalysis :\\n    def __init__ (self):\\n        self._classifier  = pipeline (\"sentiment-analysis\" )\\n    @app.get(\"/\")\\n    def classify (self, input_text : str) -> str:\\n        return self._classifier (input_text )[0][\"label\"]\\nfastapi_deployment  = SentimentAnalysis .bind()\\nThe modified deployment should have exactly the same behavior on this example\\n(try it using serve run !), but it will gracefully handle invalid inputs. These may\\nlook like minor benefits for this simple example, but for more complex APIs this\\ncan make a world of difference. We won’t delve deeper into the details of FastAPI\\nhere, but for more information on its features and syntax, check out their excellent\\ndocumentation .\\nScaling and Resource Allocation\\nAs mentioned, machine learning models are often compute hungry. Therefore, it’s\\nimportant to be able to allocate the correct amount of resources to your ML applica‐\\ntion to handle request loads while minimizing cost. Ray Serve allows you to adjust the\\nAn Introduction to Ray Serve | 163\\n\\nresources dedicated to a deployment in two ways: by tuning the number of replicas\\nof the deployment and tuning the resources allocated to each replica. By default, a\\ndeployment consists of a single replica that uses a single CPU, but these parameters\\ncan be adjusted in the @serve.deployment  decorator (or using the corresponding\\ndeployment.options  API).\\nLet’s modify the SentimentClassifier  example to scale out to multiple replicas and\\nadjust the resource allocation so that each replica uses two CPUs instead of one (in\\npractice, you would want to profile and understand your model to set this parameter\\ncorrectly).  We’ll also add a print statement to log the process ID of the process\\nhandling each request to show that the requests are now load balanced across two\\nreplicas:\\napp = FastAPI()\\n@serve.deployment (num_replicas =2, ray_actor_options ={\"num_cpus\" : 2})\\n@serve.ingress(app)\\nclass SentimentAnalysis :\\n    def __init__ (self):\\n        self._classifier  = pipeline (\"sentiment-analysis\" )\\n    @app.get(\"/\")\\n    def classify (self, input_text : str) -> str:\\n        import os\\n        print(\"from process:\" , os.getpid())\\n        return self._classifier (input_text )[0][\"label\"]\\nscaled_deployment  = SentimentAnalysis .bind()\\nRunning this new version of our classifier with serve run app:scaled_deployment\\nand querying it using requests  as we did previously, you should see that there are\\nnow two copies of the model handling requests! We could easily scale up to tens or\\nhundreds of replicas just by tweaking num_replicas  in the same way: Ray enables\\nscaling to hundreds of machines and thousands of processes in a single cluster.\\nIn this example we scaled to a static number of replicas with each replica consuming\\ntwo full CPUs, but Serve also supports more expressive resource allocation policies.\\nFor example:\\n•Enabling a deployment to use GPUs simply requires setting num_gpus  instead of •\\nnum_cpus . Serve supports the same resource types as Ray Core, so deployments\\ncan also use TPUs or other custom resources.\\n•Resources can be fractional , allowing replicas to be efficiently bin-packed. •\\nFor example, if a single replica doesn’t saturate a full GPU, you can allocate\\nnum_gpus=0.5  to it and multiplex with another model.\\n164 | Chapter 8: Online Inference with Ray Serve\\n\\n•For applications with varying request load, a deployment can be configured to•\\ndynamically autoscale the number of replicas based on the number of requests\\ncurrently in flight.\\nFor more details about resource allocation options, refer to the latest Ray Serve\\ndocumentation .\\nRequest Batching\\nMany machine learning models can be efficiently vectorized , meaning that multiple\\ncomputations can be run in parallel more efficiently than running them sequentially.\\nThis is especially beneficial when running models on GPUs that are purpose-built\\nfor efficiently performing many computations in parallel. In the context of online\\ninference this offers a path for optimization: serving multiple requests (possibly from\\ndifferent sources) in parallel can drastically improve the throughput of the system\\n(and therefore save cost).\\nTwo high-level strategies take advantage of request batching: client-side batching\\nand server-side batching. In client-side batching , the server accepts multiple inputs\\nin a single request, and clients include logic to send them in batches instead of one\\nat a time. This is useful in situations where a single client is frequently sending\\nmany inference requests. Server-side batching , in contrast, enables the server to batch\\nmultiple requests without requiring any modification on the client. This can also be\\nused to batch requests across multiple clients, which enables efficient batching even\\nin situations with many clients that send relatively few requests each.\\nRay Serve offers a built-in utility for server-side batching, the @serve.batch  deco‐\\nrator, that requires just a few code changes. This batching support uses Python’s\\nasyncio  capabilities to enqueue multiple requests into a single function call. The\\nfunction should take in a list of inputs and return the corresponding list of outputs.\\nOnce again, let’s revisit the sentiment classifier from earlier and this time modify it\\nto perform server-side batching. The underlying Hugging Face pipeline  supports\\nvectorized inference; all we need to do is pass a list of inputs, and it will return the\\ncorresponding list of outputs. We’ll split the call to the classifier into a new method,\\nclassify_batched , that will take a list of input texts as input, perform inference\\nacross them, and return the outputs in a formatted list. classify_batched  will use\\nthe @serve.batch  decorator to automatically perform batching. The behavior can be\\nconfigured using the max_batch_size  and batch_timeout_wait_s  parameters. Here\\nwe’ll set the max batch size to 10 and wait for up to 100 ms:\\napp = FastAPI()\\n@serve.deployment\\n@serve.ingress(app)\\nAn Introduction to Ray Serve | 165\\n\\nclass SentimentAnalysis :\\n    def __init__ (self):\\n        self._classifier  = pipeline (\"sentiment-analysis\" )\\n    @serve.batch(max_batch_size =10, batch_wait_timeout_s =0.1)\\n    async def classify_batched (self, batched_inputs ):\\n        print(\"Got batch size:\" , len(batched_inputs ))\\n        results = self._classifier (batched_inputs )\\n        return [result[\"label\"] for result in results]\\n    @app.get(\"/\")\\n    async def classify (self, input_text : str) -> str:\\n        return await self.classify_batched (input_text )\\nbatched_deployment  = SentimentAnalysis .bind()\\nNotice that both the classify  and classify_batched  methods now use Python’s\\nasync  and await  syntax, meaning that many of these calls can run concurrently in the\\nsame process.\\nTo test this behavior, we’ll use the serve.run  Python API to send requests using the\\nPython-native handle to our deployment:\\nimport ray\\nfrom ray import serve\\nfrom app import batched_deployment\\nhandle = serve.run(batched_deployment )  \\nray.get([handle.classify .remote(\"sample text\" ) for _ in range(10)])\\nGet a handle to the deployment so we can send requests in parallel.\\nThe handle returned by serve.run  can be used to send multiple requests in parallel:\\nhere, we send 10 requests in parallel and wait for them all to return. Without batch‐\\ning, each request would be handled sequentially, but because we enabled batching, we\\nshould see the requests handled all at once (evidenced by batch size printed in the\\nclassify_batched  method). Running on a CPU, this might be marginally faster than\\nrunning sequentially, but running the same handler on a GPU we would observe a\\nsignificant speedup for the batched version.\\nMultimodel Inference Graphs\\nUp until now, we’ve been deploying and querying a single Serve deployment wrap‐\\nping one ML model. As described earlier, ML models often are not useful in isolation:\\nmany applications require multiple models to be composed together and for business\\nlogic to be intertwined with machine learning. The real power of Ray Serve is its\\nability to compose multiple models along with regular Python logic into a single\\napplication. This is possible by instantiating many different deployments and passing\\n166 | Chapter 8: Online Inference with Ray Serve\\n\\na reference between them. Each of these deployments can use all of the features\\nwe’ve discussed up to this point: they can be independently scaled, perform request\\nbatching, and use flexible resource allocations.\\nThis section illustrates common multimodel serving patterns but doesn’t actually\\ncontain any ML models yet. The focus is on the core capabilities that Serve provides.\\nCore feature: binding multiple deployments\\nAll types of multimodel inference graphs in Ray Serve center around the ability to\\npass a reference to one deployment into the constructor of another.  To do this, we use\\nanother feature of the .bind()  API: a bound deployment can be passed to another\\ncall to .bind() , and this will resolve to a “handle” to the deployment at runtime.\\nThis enables deployments to be deployed and instantiated independently and then\\ncall each other at runtime. Here is the most basic example of a multideployment Serve\\napplication:\\n@serve.deployment\\nclass DownstreamModel :\\n    def __call__ (self, inp: str):\\n        return \"Hi from downstream model!\"\\n@serve.deployment\\nclass Driver:\\n    def __init__ (self, downstream ):\\n        self._d = downstream\\n    async def __call__ (self, *args) -> str:\\n        return await self._d.remote()\\ndownstream  = DownstreamModel .bind()\\ndriver = Driver.bind(downstream )\\nIn this example, the downstream model is passed into the “driver” deployment. Then\\nat runtime the driver deployment calls into the downstream model.  The driver could\\ntake any number of models passed in, and the downstream model could even take\\nother downstream models of its own.\\nPattern 1: Pipelining\\nThe first common multimodel pattern among ML applications is “pipelining”: calling\\nmultiple models in sequence, where the input of one model depends on the output of\\nthe previous. Image processing, for example, often consists of a pipeline with multiple\\nstages of transformations such as cropping, segmentation, and object recognition\\nor optical character recognition (OCR). Each of these models may have different\\nAn Introduction to Ray Serve | 167\\n\\nproperties, with some of them being lightweight transformations that can run on a\\nCPU and others being heavyweight deep learning models that run on a GPU.\\nSuch pipelines can easily be expressed using Serve’s API. Each stage of the pipeline\\nis defined as an independent deployment, and each deployment is passed into a\\ntop-level “pipeline driver. ” In the following example, we pass two deployments into a\\ntop-level driver, and the driver calls them in sequence. Note that many requests to the\\ndriver could be happening concurrently; therefore, it is possible to efficiently saturate\\nall stages of the pipeline:\\n@serve.deployment\\nclass DownstreamModel :\\n    def __init__ (self, my_val: str):\\n        self._my_val = my_val\\n    def __call__ (self, inp: str):\\n        return inp + \"|\" + self._my_val\\n@serve.deployment\\nclass PipelineDriver :\\n    def __init__ (self, model1, model2):\\n        self._m1 = model1\\n        self._m2 = model2\\n    async def __call__ (self, *args) -> str:\\n        intermediate  = self._m1.remote(\"input\")\\n        final = self._m2.remote(intermediate )\\n        return await final\\nm1 = DownstreamModel .bind(\"val1\")\\nm2 = DownstreamModel .bind(\"val2\")\\npipeline_driver  = PipelineDriver .bind(m1, m2)\\nTo test this example, you can once again use the serve run  API. Sending a test\\nrequest to the pipeline returns \"\\'input|val1|val2\\'\"  as output: each downstream\\n“model” appended its own value to construct the final result. In practice, each of these\\ndeployments could be wrapping its own ML model, and a single request may flow\\nacross many physical nodes in a cluster.\\nPattern 2: Broadcasting\\nIn addition to sequentially chaining models together, it’s often useful to broadcast an\\ninput or intermediate result to multiple models in parallel. This could be to perform\\n“ensembling, ” or combining the results of multiple independent models into a single\\nresult, or used in a situation where different models may perform better on different\\ninputs.  Often the results of the models need to be combined in some way into a final\\nresult: either simply concatenated or maybe a single result chosen from the lot.\\n168 | Chapter 8: Online Inference with Ray Serve\\n\\nThis is expressed very similarly to the pipelining example: a number of downstream\\nmodels are passed into a top-level driver. In this case, it’s important that we call\\nthe models in parallel: waiting for the result of each before calling the next would\\ndramatically increase the overall latency of the system:\\n@serve.deployment\\nclass DownstreamModel :\\n    def __init__ (self, my_val: str):\\n        self._my_val = my_val\\n    def __call__ (self):\\n        return self._my_val\\n@serve.deployment\\nclass BroadcastDriver :\\n    def __init__ (self, model1, model2):\\n        self._m1 = model1\\n        self._m2 = model2\\n    async def __call__ (self, *args) -> str:\\n        output1, output2 = self._m1.remote(), self._m2.remote()\\n        return [await output1, await output2]\\nm1 = DownstreamModel .bind(\"val1\")\\nm2 = DownstreamModel .bind(\"val2\")\\nbroadcast_driver  = BroadcastDriver .bind(m1, m2)\\nTesting this endpoint after running it once again with serve run  returns \\'[\"val1\",\\n\"val2\"]\\' , the combined output of the two models called in parallel.\\nPattern 3: Conditional logic\\nFinally, while many ML applications fit roughly into one of the preceding patterns,\\noften having static control flow can be very limiting. Take, for instance, the example\\nof building a service to extract license plate numbers from user-uploaded images. In\\nthis case, we’ll likely need to build an image processing pipeline as discussed, but\\nwe also don’t simply want to feed any image into the pipeline blindly. If the user\\nuploads something other than a car or an image that is low quality, we likely want to\\nshort circuit, avoid calling into the heavyweight and expensive pipeline, and provide a\\nuseful error message. Similarly, in a product recommendation use case we may want\\nto select a downstream model based on user input or the result of an intermediate\\nmodel. Each of these examples requires embedding custom logic alongside our ML\\nmodels.\\nWe can accomplish this trivially using Serve’s multimodel API because our computa‐\\ntion graph is defined as ordinary Python logic rather than as a statically defined\\ngraph.  For instance, in the next example, we use a simple random number generator\\nAn Introduction to Ray Serve | 169\\n\\n(RNG) to decide which of two downstream models to call into. In a real-world\\nexample, the RNG could be replaced with business logic, a database query, or the\\nresult of an intermediate model:\\n@serve.deployment\\nclass DownstreamModel :\\n    def __init__ (self, my_val: str):\\n        self._my_val = my_val\\n    def __call__ (self):\\n        return self._my_val\\n@serve.deployment\\nclass ConditionalDriver :\\n    def __init__ (self, model1, model2):\\n        self._m1 = model1\\n        self._m2 = model2\\n    async def __call__ (self, *args) -> str:\\n        import random\\n        if random.random() > 0.5:\\n            return await self._m1.remote()\\n        else:\\n            return await self._m2.remote()\\nm1 = DownstreamModel .bind(\"val1\")\\nm2 = DownstreamModel .bind(\"val2\")\\nconditional_driver  = ConditionalDriver .bind(m1, m2)\\nEach call to this endpoint returns either \"val1\"  or \"val2\"  with 50/50 probability.\\nEnd-to-End Example: Building an NLP-Powered API\\nIn this section, we’ll use Ray Serve to build an end-to-end natural language pro‐\\ncessing (NLP) pipeline hosted for online inference. Our goal will be to provide a\\nWikipedia summarization endpoint that will leverage multiple NLP models and some\\ncustom logic to provide a succinct summary of the most relevant Wikipedia page for\\na given search term.\\nThis task will bring together many of the concepts and features we’ve discussed:\\n•We’ll be combining custom business logic along with multiple ML models.•\\n•The inference graph will consist of all three multimodel patterns: pipelining,•\\nbroadcasting, and conditional logic.\\n•Each model will be hosted as a separate Serve deployment so they can be inde‐•\\npendently scaled and given their own resource allocation.\\n170 | Chapter 8: Online Inference with Ray Serve\\n\\n3We’ve created this app.py  file for you in the book’s GitHub repository . After cloning the repo and installing all\\nthe dependencies, you should be able to run serve run app:<deployment_name>  directly from the notebook\\ndirectory for each deployment we’re referencing in a serve run  call in this chapter.•One of the models will leverage vectorized computation via batching.•\\n•The API will be defined using Ray Serve’s FastAPI  for input parsing and defining •\\nour output schema.\\nOur online inference pipeline will be structured as shown in Figure 8-2 :\\n1.The user will provide a keyword search term.1.\\n2.We’ll fetch the content for the most relevant Wikipedia article for the search2.\\nterm.\\n3.A sentiment analysis model will be applied to the article. Anything with a “nega‐3.\\ntive” sentiment will be rejected, and we’ll return early.\\n4.The article content will be broadcast to summarizer and named entity recogni‐4.\\ntion models.\\n5.We’ll return a composed result based on the summarizer and named entity5.\\nrecognition outputs.\\nFigure 8-2. The architecture for an NLP pipeline to summarize Wikipedia articles\\nThis pipeline will be exposed over HTTP and return the results in a structured\\nformat.  By the end of this section, we’ll have the pipeline running from end to end\\nlocally and ready to scale up on a cluster. Let’s get started!\\nBefore we dive into the code, you’ll need the following Python packages installed\\nlocally:\\npip install \"ray[serve]==2.2.0\" \"transformers==4.21.2\"\\npip install \"requests==2.28.1\" \"wikipedia==1.4.0\"\\nAdditionally, in this section we’ll assume that all the code samples are available locally\\nin a file called app.py  so we can run the deployments using serve run  from the same\\ndirectory.3\\nEnd-to-End Example: Building an NLP-Powered API | 171\\n\\nFetching Content and Preprocessing\\nThe first step is to fetch the most relevant page from Wikipedia given a user-provided\\nsearch term. For this, we will leverage the Wikipedia package on PyPI to do the heavy\\nlifting. We’ll first search for the term and then select the top result and return its page\\ncontent. If no results are found, we’ll return None —this edge case will be handled later\\nwhen we define the API:\\nfrom typing import Optional\\nimport wikipedia\\ndef fetch_wikipedia_page (search_term : str) -> Optional [str]:\\n    results = wikipedia .search(search_term )\\n    # If no results, return to caller.\\n    if len(results) == 0:\\n        return None\\n    # Get the page for the top result.\\n    return wikipedia .page(results[0]).content\\nNLP Models\\nNext, we need to define the ML models that will do the heavy lifting of our API.  We’ll\\nbe using the Hugging Face Transformers library  as it provides convenient APIs to\\npretrained state-of-the-art ML models so we can focus on the serving logic.\\nThe first model we’ll use is a sentiment classifier, the same one we used in the\\npreceding examples. The deployment for this model will take advantage of vectorized\\ncomputations using Serve’s batching API:\\nfrom ray import serve\\nfrom transformers  import pipeline\\nfrom typing import List\\n@serve.deployment\\nclass SentimentAnalysis :\\n    def __init__ (self):\\n        self._classifier  = pipeline (\"sentiment-analysis\" )\\n    @serve.batch(max_batch_size =10, batch_wait_timeout_s =0.1)\\n    async def is_positive_batched (self, inputs: List[str]) -> List[bool]:\\n        results = self._classifier (inputs, truncation =True)\\n        return [result[\"label\"] == \"POSITIVE\"  for result in results]\\n    async def __call__ (self, input_text : str) -> bool:\\n        return await self.is_positive_batched (input_text )\\n172 | Chapter 8: Online Inference with Ray Serve\\n\\nWe’ll also use a text summarization model to provide a succinct summary for the\\nselected article. This model takes an optional max_length  argument to cap the length\\nof the summary. Because we know this is the most computationally expensive of the\\nmodels, we set num_replicas=2 ; that way, if we have many requests coming in at the\\nsame time, it can keep up with the throughput of the other models.  In practice, we\\nmay need more replicas to keep up with the input load, but we could know that only\\nfrom profiling and monitoring:\\n@serve.deployment (num_replicas =2)\\nclass Summarizer :\\n    def __init__ (self, max_length : Optional [int] = None):\\n        self._summarizer  = pipeline (\"summarization\" )\\n        self._max_length  = max_length\\n    def __call__ (self, input_text : str) -> str:\\n        result = self._summarizer (\\n            input_text , max_length =self._max_length , truncation =True)\\n        return result[0][\"summary_text\" ]\\nThe final model in our pipeline will be a named entity recognition model: this will\\nattempt to extract named entities from the text. Each result will have a confidence\\nscore, so we can set a threshold to accept only results above a certain threshold. We\\nmay also want to cap the total number of entities returned. The request handler for\\nthis deployment calls the model and then uses some basic business logic to enforce\\nthe provided confidence threshold and limit the number of entities:\\n@serve.deployment\\nclass EntityRecognition :\\n    def __init__ (self, threshold : float = 0.90, max_entities : int = 10):\\n        self._entity_recognition  = pipeline (\"ner\")\\n        self._threshold  = threshold\\n        self._max_entities  = max_entities\\n    def __call__ (self, input_text : str) -> List[str]:\\n        final_results  = []\\n        for result in self._entity_recognition (input_text ):\\n            if result[\"score\"] > self._threshold :\\n                final_results .append(result[\"word\"])\\n            if len(final_results ) == self._max_entities :\\n                break\\n        return final_results\\nHTTP Handling and Driver Logic\\nWith the input preprocessing and ML models defined, we’re ready to define the\\nHTTP API and driver logic. First, we define the schema of the response that we’ll\\nreturn from the API using Pydantic . The response includes whether the request was\\nsuccessful and a status message in addition to our summary and named entities. This\\nEnd-to-End Example: Building an NLP-Powered API | 173\\n\\nwill allow us to return a helpful response in error conditions such as when no result is\\nfound or the sentiment analysis comes back as negative:\\nfrom pydantic  import BaseModel\\nclass Response (BaseModel ):\\n    success: bool\\n    message: str = \"\"\\n    summary: str = \"\"\\n    named_entities : List[str] = []\\nNext, we need to define the actual control flow logic that will run in the driver\\ndeployment. The driver itself will not do any of the actual heavy lifting; instead, it will\\ncall into our three downstream model deployments and interpret their results. It will\\nalso house the FastAPI app definition, parsing the input and returning the correct\\nResponse  model based on the results of the pipeline:\\nfrom fastapi import FastAPI\\napp = FastAPI()\\n@serve.deployment\\n@serve.ingress(app)\\nclass NLPPipelineDriver :\\n    def __init__ (self, sentiment_analysis , summarizer , entity_recognition ):\\n        self._sentiment_analysis  = sentiment_analysis\\n        self._summarizer  = summarizer\\n        self._entity_recognition  = entity_recognition\\n    @app.get(\"/\", response_model =Response )\\n    async def summarize_article (self, search_term : str) -> Response :\\n        # Fetch the top page content for the search term if found.\\n        page_content  = fetch_wikipedia_page (search_term )\\n        if page_content  is None:\\n            return Response (success=False, message=\"No pages found.\" )\\n        # Conditionally continue based on the sentiment analysis.\\n        is_positive  = await self._sentiment_analysis .remote(page_content )\\n        if not is_positive :\\n            return Response (success=False, message=\"Only positivitiy allowed!\" )\\n        # Query the summarizer and named entity recognition models in parallel.\\n        summary_result  = self._summarizer .remote(page_content )\\n        entities_result  = self._entity_recognition .remote(page_content )\\n        return Response (\\n            success=True,\\n            summary=await summary_result ,\\n            named_entities =await entities_result\\n        )\\n174 | Chapter 8: Online Inference with Ray Serve\\n\\n4The models we use in this example are quite large, so be warned that it will likely take several minutes to\\ndownload them the first time you run this example.Fetch the page content in the main handler’s body using our fetch_wikipedia_page\\nlogic (if no result is found, an error is returned). Then, we call into the sentiment\\nanalysis model. If this returns negative, we terminate early and return an error\\nto avoid calling the other expensive ML models. Finally, we broadcast the article\\ncontents to both the summary and named entity recognition models in parallel.\\nThe results of the two models are stitched together into the final response, and we\\nreturn success. Remember that we may have many calls to this handler running\\nconcurrently: the calls to the downstream models don’t block the driver, and it could\\ncoordinate calls to many replicas of the heavyweight models.\\nPutting It All Together\\nAt this point, we have defined all the core logic. All that’s left is to bind the graph of\\ndeployments together and run it:\\nsentiment_analysis  = SentimentAnalysis .bind()\\nsummarizer  = Summarizer .bind()\\nentity_recognition  = EntityRecognition .bind(threshold =0.95, max_entities =5)\\nnlp_pipeline_driver  = NLPPipelineDriver .bind(\\n    sentiment_analysis , summarizer , entity_recognition )\\nFirst, we need to instantiate each of the deployments with any relevant input argu‐\\nments. For example, here we pass a threshold and limit for the entity recognition\\nmodel. The most important piece we pass is a reference to each of the three models\\ninto the driver so it can coordinate the computation. Now that we’ve defined the full\\nNLP pipeline, we can run it using serve run :4\\nserve run app:nlp_pipeline_driver\\nThis will deploy each of the four deployments locally and make the driver available\\nat http://localhost:8000 . We can query the pipeline using the requests  to see it in\\naction. First, let’s try querying for an entry on Ray Serve:\\nimport requests\\nprint(requests .get(\\n    \"http://localhost:8000/\" , params={\"search_term\" : \"rayserve\" }\\n).text)\\n\\'{\"success\":false,\"message\":\"No pages found.\",\\n  \"summary\":\"\",\"named_entities\":[]}\\'\\nEnd-to-End Example: Building an NLP-Powered API | 175\\n\\nUnfortunately, this page doesn’t exist yet! The first chunk of validation business logic\\nkicks in and returns a “No pages found” message. Let’s try looking for something\\nmore common:\\nprint(requests .get(\\n    \"http://localhost:8000/\" , params={\"search_term\" : \"war\"}\\n).text)\\n\\'{\"success\":false,\"message\":\"Only positivitiy allowed!,\\n  \"summary\":\"\",\"named_entities\":[]}\\'\\nMaybe we were just interested in learning about history, but this article was a bit\\ntoo negative for our sentiment classifier. Let’s try something more neutral this time—\\nwhat about science?\\nprint(requests .get(\\n    \"http://localhost:8000/\" , params={\"search_term\" : \"physicist\" }\\n).text)\\n\\'{\"success\":true,\"message\":\"\",\"summary\":\" Physics is the natural science that\\nstudies matter, its fundamental constituents, its motion and behavior through\\nspace and time, and the related entities of energy and force . During the\\nScientific Revolution in the 17th century these natural sciences emerged as\\nunique research endeavors in their own right . Physics intersects with many\\ninterdisciplinary areas of research, such as biophysics and quantum chemistry .\\n\",\"named_entities\":[\"Scientific\",\"Revolution\", \"Ancient\",\"Greek\",\"Egyptians\"]}\\'\\nThis example successfully ran through the full pipeline: the API responded with a\\ncogent summary of the article and a list of relevant named entities.\\nTo recap, in this section we built an online NLP API using Ray Serve. This inference\\ngraph consisted of multiple ML models in addition to custom business logic and\\ndynamic control flow. Each model can be independently scaled and have its own\\nresource allocation, and we can exploit vectorized computations using server-side\\nbatching. Because we were able to test the API locally, the next step would be to\\ndeploy to production. Ray Serve makes it easy to deploy on Kubernetes or other\\ncloud provider offerings using the Ray Cluster launcher, and we could easily scale up\\nto handle many users by tweaking the resource allocations for our deployments.\\nSummary\\nThis chapter introduced Ray Serve, a Ray-native library for building online inference\\nAPIs. Ray Serve is focused on solving the unique challenges of serving machine\\nlearning models in production, offering functionality to efficiently scale models and\\nallocate resources as well as compose multiple models along with business logic.\\nAdditionally, like all of Ray, Serve is designed to be a general-purpose solution that\\navoids vendor lock-in.\\n176 | Chapter 8: Online Inference with Ray Serve\\n\\nAlthough we walked through an end-to-end example of a multimodel pipeline, this\\nchapter has covered only a small portion of Ray Serve’s functionality and best practi‐\\nces for real-world applications. For more detail and examples, read the Ray Serve\\ndocumentation .\\nSummary | 177\\n\\n\\n\\n1Ray itself is not opinionated about how you set up your cluster. In fact, you have many options, many of\\nwhich are described in this chapter. Apart from the open source solutions we’re describing here, there are also\\nfully managed commercial solutions available, such as those offered by Anyscale or Domino Data Lab.\\n2While this chapter technically has an accompanying notebook , the material presented here is not well-suited\\nfor development in an interactive Python session. We recommend that you work through these examples on\\nthe command line. No matter where you decide to work, make sure to have Ray installed with pip install\\n\"ray==2.2.0\" .CHAPTER 9\\nRay Clusters\\nRichard Liaw\\nSo far we have focused on teaching you the basics of Ray for building machine\\nlearning applications. Y ou know how to parallelize your Python code with Ray Core\\nand run reinforcement learning experiments with RLlib. Y ou’ve also seen how to\\npreprocess data with Ray Datasets, tune hyperparameters with Ray Tune, and train\\nmodels with Ray Train. But one of the key features that Ray brings is the ability to\\nscale out seamlessly onto multiple machines. Outside a lab environment or a big tech\\ncompany, it may be difficult set up multiple machines and join them into a single Ray\\nCluster. This chapter is all about how to do that.1\\nCloud technology has commoditized access to cheap machines for anyone.  But it is\\noften quite difficult to figure out the right APIs to handle the cloud provider tools.\\nThe Ray team has provided a couple of tools that abstract the complexity away. There\\nare three primary ways of launching or deploying a Ray Cluster. Y ou can do so\\nmanually, via a Kubernetes operator, or via the cluster launcher CLI tool.\\nIn the first part of this chapter we’ll cover these three methods in detail.2 We only\\nbriefly explain manual cluster creation and the cluster launcher CLI, and we spend\\nmost of our time explaining how to use the Kubernetes operator. After that, we’ll\\ncover how to run Ray Clusters on the cloud and how to autoscale them up and down.\\n179\\n\\n3Depending on your setup or work situation, this might not be a realistic assumption for you. Don’t worry,\\nwe’ll cover ways to create Ray Clusters that don’t require you to have any machines running. In any case,\\nknowing the steps involved for manual Ray Cluster creation is useful to fall back on.\\n4If you already have remote Redis instances, you can use them by specifying the environment variable\\nRAY_REDIS_ADDRESS=ip1:port1,ip2:port2... . Ray will use the first address as primary and the rest as\\nshards.Manually Creating a Ray Cluster\\nLet’s start with the most basic way of creating a Ray Cluster.  To build a Ray Cluster\\nmanually we assume that you have a list of machines that can communicate with each\\nother and have Ray installed on them.3\\nTo start with, you can choose any machine to be the head node. On this node, run the\\nfollowing command:\\nray start --head --port=6379\\nThis command will print out the IP address of the Ray GCS server that was started,\\nnamely, the local node IP address plus the port number you specified:4\\n...\\nNext steps\\nTo connect to this Ray runtime from another node, run\\n  ray start --address=\\'<head-address>:6379\\'\\nIf connection fails, check your firewall settings and network configuration.\\nY ou need this <head-address>  to connect your other nodes to the cluster, so make\\nsure to copy it. If you omit the --port  argument, Ray will use a random port.\\nNext, we can connect every other node in your cluster to the head node by running a\\nsingle command on each node:\\nray start --address =<head-address>\\nMake sure to pass the correct <head-address> , which should look something like\\n123.45.67.89:6379 . Running this command, you should see output of the following\\nform:\\n--------------------\\nRay runtime started.\\n--------------------\\nTo terminate  the Ray runtime,  run\\n  ray stop\\nIf you wish to specify that a machine has 10 CPUs and 1 GPU, you can do this with\\nthe flags --num-cpus=10  and --num-gpus=1 . If you see Ray runtime started. , then\\n180 | Chapter 9: Ray Clusters\\n\\n5If you want to learn more about the specific flags used in the upcoming examples, we suggest reviewing the\\nofficial reference guide .\\nthe node successfully connected to the head node at the --address . Y ou should now\\nbe able to connect to the cluster with ray.init(address=\\'auto\\') .\\nIf the head node and the new node you want to connect are on\\na separate subnetwork with Network Address Translation (NAT),\\nyou can’t use the <head-address>  printed by the command starting\\nthe head node as --address . In this case, you need to find the\\naddress that will reach the head node from the new node. If the\\nhead node has a domain address like compute04.berkeley.edu ,\\nyou can use that in place of an IP address and rely on the DNS.\\nIf you see Unable to connect to GCS at ... , it means the head\\nnode is inaccessible at the given --address . This could be for\\nseveral reasons. For example, maybe the head node is not actually\\nrunning, a different version of Ray is running at the specified\\naddress, the specified address is wrong, or firewall settings are\\npreventing access.\\nIf the connection fails, to check whether each port can be reached from a node, you\\ncan use a tool such as nmap  or nc. Here’s an example of how to run a check with both\\ntools in a successful case:5\\n$ nmap -sV --reason  -p $PORT $HEAD_ADDRESS\\nNmap scan report for compute04.berkeley.edu  (123.456.78.910 )\\nHost is up, received  echo-reply  ttl 60 (0.00087s latency).\\nrDNS record for 123.456.78.910:  compute04.berkeley.edu\\nPORT     STATE SERVICE REASON         VERSION\\n6379/tcp open  redis?  syn-ack\\nService detection  performed.  Please report any incorrect\\n  results at https://nmap.org/submit/  .\\n$ nc -vv -z $HEAD_ADDRESS  $PORT\\nConnection  to compute04.berkeley.edu  6379 port [tcp/...] succeeded!\\nIf your node cannot access the port and IP address specified, you might see:\\n$ nmap -sV --reason  -p $PORT $HEAD_ADDRESS\\nNmap scan report for compute04.berkeley.edu  (123.456.78.910 )\\nHost is up (0.0011s latency).\\nrDNS record for 123.456.78.910:  compute04.berkeley.edu\\nPORT     STATE  SERVICE REASON       VERSION\\n6379/tcp closed redis   reset ttl 60\\nService detection  performed.  Please report any incorrect\\n  results at https://nmap.org/submit/  .\\n$ nc -vv -z $HEAD_ADDRESS  $PORT\\nnc: connect to compute04.berkeley.edu  port 6379 (tcp) failed: Connection  refused\\nManually Creating a Ray Cluster | 181\\n\\nNow, if you want to  stop the Ray processes on any node, simply run ray stop . That’s\\nthe manual way of creating a Ray Cluster. Let’s move on to discussing the deployment\\nof your Ray Clusters with the popular Kubernetes orchestration framework.\\nDeployment on Kubernetes\\nKubernetes is an industry-standard platform for cluster resource management. It\\nallows software teams to seamlessly deploy, manage, and scale their business appli‐\\ncations in a wide variety of production environments. It was initially developed\\nby Google, but many organizations have now adopted Kubernetes as their cluster\\nresource management solution.\\nThe community-maintained KubeRay project  is the standard way of deploying and\\nmanaging Ray Clusters on Kubernetes. The KubeRay operator helps deploy and\\nmanage Ray Clusters on top of Kubernetes ( Figure 9-1 ). Clusters are defined as a\\ncustom RayCluster  resource and managed by a fault-tolerant Ray controller. The\\noperator automates provisioning, management, autoscaling, and operations of Ray\\nClusters deployed to Kubernetes. The main features of this operator are:\\n•Management of first-class RayCluster  via a custom resource. •\\n•Support for heterogeneous worker types in a single Ray Cluster.•\\n•Built-in monitoring via Prometheus.•\\n•Use of PodTemplate  to create Ray pods. •\\n•Updated status based on the running pods.•\\n•Automatically populate environment variables in the containers.•\\n•Automatically prefix your container command with the Ray start command.•\\n•Automatically adding the volumeMount at /dev/shm  for shared memory. •\\n•Use of ScaleStrategy  to remove specific nodes in specific groups. •\\nFigure 9-1. An overview of KubeRay\\n182 | Chapter 9: Ray Clusters\\n\\nSetting Up Your First KubeRay Cluster\\nY ou can deploy the operator by cloning the KubeRay repository  and calling the\\nfollowing command:\\nexport KUBERAY_VERSION =v0.3.0\\nkubectl create -k \"github.com/ray-project/kuberay/manifests/\\\\\\ncluster-scope-resources?ref= ${KUBERAY_VERSION }&timeout=90s\"\\nkubectl apply -k \"github.com/ray-project/kuberay/manifests/\\\\\\nbase?ref= ${KUBERAY_VERSION }&timeout=90s\"\\nY ou can verify that the operator has been deployed using this command:\\nkubectl -n ray-system  get pods\\nWhen deployed, the operator will watch for Kubernetes events (create/delete/update)\\nfor the raycluster  resource updates. Upon these events, the operator can create a\\ncluster consisting of a head pod and multiple worker pods, delete a cluster, or update\\nthe cluster by adding or removing worker pods. Now let’s deploy a new Ray Cluster\\nusing a provided default cluster configuration (we’ll get to this YAML file later):\\nwget \"https://raw.githubusercontent.com/ray-project/kuberay/\\\\\\n${KUBERAY_VERSION }/ray-operator/config/samples/ray-cluster.complete.yaml\"\\nkubectl create -f ray-cluster.complete.yaml\\nThe KubeRay operator configures a Kubernetes service targeting the Ray head pod.\\nTo identify the service, run:\\nkubectl get service --selector =ray.io/cluster =raycluster-complete\\nThe output of this command should resemble the following structure:\\nNAME                           TYPE        CLUSTER-IP     EXTERNAL-IP\\nraycluster-complete-head-svc   ClusterIP   xx.xx.xxx.xx   <none>\\n   PORT(S)                       AGE\\n   6379/TCP,8265/TCP,10001/TCP   6m10s\\nThe three ports indicated in the output correspond to the following services of the\\nRay head pod:\\n6379\\nThe Ray head’s GCS service. Ray worker pods connect to this service when\\njoining the cluster.\\n8265\\nExposes the Ray Dashboard and the Ray Job Submission service.\\n10001\\nExposes the Ray Client server.\\nDeployment on Kubernetes | 183\\n\\nY ou should note that the Docker images we’re using are quite large and can take\\na while to download. Also, even if you see the expected output from kubectl get\\nservice , that doesn’t mean your cluster is ready to use yet. Y ou should look at the\\npod status and make sure they are all actually in “Running” state.\\nInteracting with the KubeRay Cluster\\nY ou might be wondering why we spend so much time on Kubernetes, since you’re\\nmost likely just interested in learning how to run Ray scripts on it.  That’s understand‐\\nable, and we’ll get to that in a moment.\\nFirst, let’s use the following Python script as the desired script to run on the cluster.\\nWe’ll name it script.py  (for simplicity), and the script will connect to the Ray Cluster\\nand run a couple standard Ray commands:\\nimport ray\\nray.init(address=\"auto\")\\nprint(ray.cluster_resources ())\\n@ray.remote\\ndef test():\\n    return 12\\nray.get([test.remote() for i in range(12)])\\nThere are three primary ways to run this script: using kubectl exec , Ray Job Sub‐\\nmission, or Ray Client. We’ll cover them in the following sections.\\nRunning Ray programs with kubectl\\nTo start with, you can directly interact with the head pod via kubectl exec . Use this\\ncommand to get a Python interpreter on the head pod:\\nkubectl exec `kubectl get pods -o custom-columns =POD:metadata.name  |\\\\\\n  grep raycluster-complete-head ` -it -c ray-head  -- python\\nWith this Python terminal, you can connect and run your own Ray application:\\nimport ray\\nray.init(address=\"auto\")\\n...\\nThere are other ways of interacting with these services without kubectl , but they\\nwill require some networking setup. The easiest route, and the one we’ll take in what\\nfollows, is to use port-forwarding.\\n184 | Chapter 9: Ray Clusters\\n\\nUsing the Ray Job Submission server\\nY ou can run scripts on the cluster by using the Ray Job Submission server.  Y ou can\\nuse the server to send a script or a bundle of dependencies and run custom scripts\\nwith that set of dependencies. To start, you’ll need to port-forward the job submission\\nserver port:\\nkubectl port-forward  service/raycluster-complete-head-svc  8265:8265\\nNow, submit the script by setting the RAY_ADDRESS  variable to the job server submis‐\\nsion endpoint and using the Ray Job Submission CLI:\\nexport RAY_ADDRESS =\"http://localhost:8265\"\\nray job submit --working-dir =. -- python script.py\\nY ou’ll see an output that looks like:\\nJob submission  server address:  http://127.0.0.1:8265\\n2022-05-20 23:35:36,066  INFO dashboard_sdk.py:276\\n -- Uploading  package gcs://_ray_pkg_533a957683abeba8.zip.\\n2022-05-20 23:35:36,067  INFO packaging.py:416\\n -- Creating  a file package for local directory  \\'.\\'.\\n-------------------------------------------------------\\nJob \\'raysubmit_U5hfr1rqJZWwJmLP\\'  submitted  successfully\\n-------------------------------------------------------\\nNext steps\\n  Query the logs of the job:\\n    ray job logs raysubmit_U5hfr1rqJZWwJmLP\\n  Query the status of the job:\\n    ray job status raysubmit_U5hfr1rqJZWwJmLP\\n  Request the job to be stopped:\\n    ray job stop raysubmit_U5hfr1rqJZWwJmLP\\nTailing logs until the job exits (disable with --no-wait ):\\n{\\'memory\\' : 47157884109 .0, \\'object_store_memory\\' : 2147483648 .0,\\n \\'CPU\\': 16.0, \\'node:127.0.0.1\\' : 1.0}\\n------------------------------------------\\nJob \\'raysubmit_U5hfr1rqJZWwJmLP\\'  succeeded\\n------------------------------------------\\nY ou can use --no-wait  to run the job in the background.\\nRay Client\\nTo connect to the cluster via Ray Client from your local machine, first make sure the\\nlocal Ray installation and Python minor version match the Ray and Python versions\\nrunning in the Ray Cluster. To do that, you can run ray --version  and python\\n--version  on both instances. In practice, you will be using a container, in which\\nDeployment on Kubernetes | 185\\n\\ncase you can simply make sure you run everything in the same container. Also, if the\\nversions don’t match, you will see a warning message informing you of the issue.\\nNext, run the following command:\\nkubectl port-forward service/raycluster-complete-head-svc 10001:10001\\nThis command will block. The local port 10001 will now be forwarded to the Ray\\nhead’s Ray Client server.\\nTo run a Ray workload on your remote Ray Cluster, open a local Python shell and\\nstart a Ray Client connection:\\nimport ray\\nray.init(address=\"ray://localhost:10001\" )\\nprint(ray.cluster_resources ())\\n@ray.remote\\ndef test():\\n    return 12\\nray.get([test.remote() for i in range(12)])\\nWith this method, you can just run the Ray program directly on your laptop (instead\\nof needing to ship the code over via kubectl  or job submission).\\nExposing KubeRay\\nIn the previous examples, we used port-forwarding as a simple way to access the\\nRay head’s services. For production use cases, you may want to consider other means\\nof exposing these services. The following notes are generic to services running on\\nKubernetes.\\nBy default, the Ray service is accessible from anywhere within  the Kubernetes cluster\\nwhere the Ray operator is running. For example, to use the Ray Client from a\\npod in the same Kubernetes namespace as the Ray Cluster, use ray.init(\"ray://\\nraycluster-complete-head-svc:10001\") .\\nTo connect from another Kubernetes namespace, use ray.init(\"ray://raycluster-\\ncomplete-head-svc.default.svc.cluster.local:10001\") . (If the Ray Cluster is a\\nnondefault namespace, use the namespace in place of default .)\\nIf you are trying to access the service from outside the cluster, use an ingress control‐\\nler. Any standard ingress controller should work with Ray Client and Ray Dashboard.\\nPick a solution compatible with your networking and security requirements—further\\nguidance is beyond the scope of this book.\\n186 | Chapter 9: Ray Clusters\\n\\nConfiguring  KubeRay\\nLet’s take a closer look at the configuration for a Ray Cluster running on Kubernetes.\\nThe example file kuberay/ray-operator/config/samples/ray-cluster.complete.yaml  is a\\ngood reference. Here is a condensed view of a Ray Cluster config’s most salient\\nfeatures:\\napiVersion:  ray.io/v1alpha1\\nkind: RayCluster\\nmetadata:\\n  name: raycluster-complete\\nspec:\\n  headGroupSpec:\\n    rayStartParams:\\n      port: \\'6379\\'\\n      num-cpus:  \\'1\\'\\n      ...\\n    template:  # Pod template\\n        metadata:  # Pod metadata\\n        spec: # Pod spec\\n            containers:\\n            - name: ray-head\\n              image: rayproject/ray:1.12.1\\n              resources:\\n                limits:\\n                  cpu: \"1\"\\n                  memory: \"1024Mi\"\\n                requests:\\n                  cpu: \"1\"\\n                  memory: \"1024Mi\"\\n              ports:\\n              - containerPort:  6379\\n                name: gcs\\n              - containerPort:  8265\\n                name: dashboard\\n              - containerPort:  10001\\n                name: client\\n              env:\\n                - name: \"RAY_LOG_TO_STDERR\"\\n                  value: \"1\"\\n              volumeMounts:\\n                - mountPath:  /tmp/ray\\n                  name: ray-logs\\n            volumes:\\n            - name: ray-logs\\n              emptyDir:  {}\\n  workerGroupSpecs:\\n  - groupName:  small-group\\n    replicas:  2\\n    rayStartParams:\\n        ...\\n    template:  # Pod template\\nDeployment on Kubernetes | 187\\n\\n        ...\\n  - groupName:  medium-group\\n    ...\\nIt’s ideal, when possible, to size each Ray pod such that it takes\\nup the entire Kubernetes node on which it is scheduled. In other\\nwords, it’s best to run one large Ray pod per Kubernetes node;\\nrunning multiple Ray pods on one Kubernetes node introduces\\nunnecessary overhead. However, running multiple Ray pods on\\none Kubernetes node makes sense in some situations, such as if:\\n•Many users are running Ray Clusters on a Kubernetes cluster•\\nwith limited compute resources.\\n•Y ou or your organization is not directly managing Kubernetes•\\nnodes (e.g., when deploying on GKE Autopilot).\\nSome of the primary configuration values that you may use are:\\nheadGroupSpec  and workerGroupSpecs\\nA Ray Cluster consists of a head pod and a number of worker pods.  The head\\npod’s configuration is specified under headGroupSpec . Configuration for worker\\npods is specified under workerGroupSpecs . There may be multiple worker\\ngroups, each group with its own configuration template . The replicas  field\\nof a workerGroup  specifies the number of worker pods of each group to keep in\\nthe cluster.\\nrayStartParams\\nThis is a string-string map of arguments to the Ray pod’s ray start  entry point.\\nFor the full list of arguments, refer to the documentation for ray start . We\\nmake special note of the num-cpus  and num-gpus  field arguments:\\nnum-cpus\\nThis field tells the Ray Scheduler how many CPUs are available to the Ray\\npod. The CPU count can be autodetected from the Kubernetes resource\\nlimits specified in the group spec’s pod template . It is sometimes useful to\\noverride this autodetected value. For example, setting num-cpus:\"0\"  will pre‐\\nvent Ray workloads with nonzero CPU requirements from being scheduled\\non the head node.\\nnum-gpus\\nThis specifies the number of GPUs available to the Ray pod. At the time of\\nwriting, this field is not detected from the group spec’s pod template . Thus,\\nnum-gpus  must be set explicitly for GPU workloads.\\n188 | Chapter 9: Ray Clusters\\n\\ntemplate\\nThis is where the bulk of the headGroup  or workerGroup ’s configuration goes.\\nThe template  is a Kubernetes Pod template that determines the configuration for\\nthe pods in the group.\\nresources\\nIt’s important to specify container CPU and memory requests and limits for each\\ngroup spec. For GPU workloads, you may also wish to specify GPU limits, e.g.,\\nnvidia.com/gpu: 1  if using an Nvidia GPU device plug-in.\\nnodeSelector  and tolerations\\nY ou can control the scheduling of a worker group’s Ray pods by setting the\\nnodeSelector  and tolerations  fields of the pod spec. Specifically, these fields\\ndetermine on which Kubernetes nodes the pods may be scheduled. Note that\\nthe KubeRay operator operates at the level of pods—KubeRay is agnostic to the\\nsetup of the underlying Kubernetes nodes. Kubernetes node configuration is left\\nto your Kubernetes cluster’s admins.\\nRay container images\\nIt’s important to specify the images used by your cluster’s Ray containers. The\\nhead and workers of the clusters should all use the same Ray version. In most\\ncases, it makes sense to use the exact same container image for the head and all\\nworkers of a given Ray Cluster. To specify custom dependencies for your cluster,\\nyou should build an image based on one of the official rayproject/ray  images.\\nVolume mounts\\nVolume mounts can be used to preserve logs or other application data originating\\nin your Ray containers. (See “Configuring Logging for KubeRay” on page 189 .)\\nContainer environment variables\\nContainer environment variables may be used to modify Ray’s behavior.  For\\nexample, RAY_LOG_TO_STDERR  will redirect logs to STDERR rather than writing\\nthem to the container’s filesystem.\\nConfiguring  Logging for KubeRay\\nRay Cluster processes typically write logs to the directory /tmp/ray/session_latest/logs\\nin the pod. These logs are also visible in the Ray Dashboard. To persist Ray logs\\nbeyond the lifetime of a pod, you may use one of the following techniques:\\nAggregate logs from the container’s filesystem\\nFor this strategy, mount an empty-dir volume with mountPath  /tmp/ray/  in the\\nRay container (see the preceding example configuration). Y ou can mount the log\\nvolume into a sidecar container running a log aggregation tool such as Promtail .\\nDeployment on Kubernetes | 189\\n\\n6To dynamically set up environments after the cluster has been deployed, you can use a runtime environment.Container STDERR logging\\nAn alternative is to redirect logging to STDERR. To do this, set the environment\\nvariable RAY_LOG_TO_STDERR=1  on all Ray containers. In terms of Kubernetes\\nconfiguration, that means adding an entry to the env field of the Ray container in\\neach Ray groupSpec :\\nenv:\\n    ...\\n    - name: \"RAY_LOG_TO_STDERR\"\\n      value: \"1\"\\n    ...\\nY ou may then use a Kubernetes logging tool geared toward aggregation from the\\nSTDERR  and STDOUT  streams.\\nUsing the Ray Cluster Launcher\\nThe goal of the Ray Cluster Launcher is to make it easy to deploy a Ray Cluster on\\nany cloud. Here’s what it will do for you:\\n•Provision a new instance/machine using the cloud provider’s SDK.•\\n•Execute shell commands to set up Ray with the provided options.•\\n•Optionally run any custom, user-defined setup commands. This can be useful for•\\nsetting environment variables and installing packages.6\\n•Initialize the Ray Cluster for you.•\\n•Deploy an autoscaler process.•\\nWe will walk you through the details of autoscaling in “ Autoscaling” on page 194. For\\nnow, let’s focus on using the cluster launcher to deploy a Ray Cluster. To do so, you\\nneed to provide a cluster configuration file.\\nConfiguring  Your Ray Cluster\\nTo run your Ray Cluster, you must specify the resource requirements in a cluster\\nconfiguration file.\\nHere’s our “scaffold” cluster specification. This is just about the minimum you’ll need\\nto specify to launch your cluster. For more information about cluster YAML files, see\\na large example here (let’s call it cluster.yaml ):\\n# An unique identifier for the head node and workers of this cluster.\\ncluster_name : minimal\\n190 | Chapter 9: Ray Clusters\\n\\n# The maximum number of workers nodes to launch in addition to the head\\n# node. min_workers default to 0.\\nmax_workers : 1\\n# Cloud provider–specific configuration.\\nprovider :\\n    type: aws\\n    region: us-west-2\\n    availability_zone : us-west-2a\\n# How Ray will authenticate with newly launched nodes.\\nauth:\\n    ssh_user : ubuntu\\nUsing the Cluster Launcher CLI\\nNow that you have a cluster configuration file, you can use the Cluster Launcher CLI\\nto deploy the specified cluster:\\nray up cluster.yaml\\nThis single line of code will take care of everything done via the manual cluster\\nsetup. It will interact with the cloud provider to provision the head node and start the\\nappropriate Ray services or processes on that node.\\nThis single line of code will not automatically start all specified nodes. In fact, it will\\nstart only a single “head” node, and run ray start --head ...  on that head node.\\nA Ray autoscaling process will then use the provided cluster configuration to start the\\nworker nodes as a background thread after the head node has started.\\nInteracting with a Ray Cluster\\nAfter you start a cluster, you’ll often want to interact with it through a variety of\\nactions:\\n•Run a script on the cluster•\\n•Move files, logs, and artifacts off the cluster•\\n•SSH onto the nodes to inspect machine details•\\nThere is a CLI for interacting with clusters launched by the Ray Cluster Launcher.\\nIf you have an existing script (like a script.py , from earlier), you can run the script\\non the cluster via ray job submit  after you’ve port-forwarded the Job Submission\\nendpoint:\\n# Run in one terminal:\\nray attach cluster.yaml  -p 8265\\nUsing the Ray Cluster Launcher | 191\\n\\n# Run in a separate terminal:\\nexport RAY_ADDRESS =http://localhost:8265\\nray job submit --working-dir =. -- python script.py\\n--working-dir  will move your local files onto the cluster, and python train.py  will\\nbe run on a shell on the cluster.\\nLet’s say after you run this script, you generate artifacts, like a results.log  file that you\\nwant to inspect. Use ray rsync-down  to move the file back:\\nray rsync-down  cluster.yaml  /path/on/cluster/results.log  ./results.log\\nWorking with Cloud Clusters\\nThis section demonstrates how to deploy Ray Clusters on AWS and other cloud\\nproviders.\\nAWS\\nFirst, install boto ( pip install boto3 ) and configure your AWS credentials in\\n$HOME/.aws/credentials , as described in the boto docs .\\nOnce boto is configured to manage resources on your AWS account, you should\\nbe ready to launch your cluster. The provided example-full.yaml  cluster config file\\nwill create a small cluster with an m5.large head node (on-demand) configured to\\nautoscale up to two m5.large spot instance workers .\\nTest that it works by running the following commands from your local machine:\\n# Create or update the cluster. When the command finishes, it will print\\n# out the command that can be used to SSH into the cluster head node.\\n$ ray up ray/python/ray/autoscaler/aws/example-full.yaml\\n# Get a remote screen on the head node.\\n$ ray attach ray/python/ray/autoscaler/aws/example-full.yaml\\n# Try running a Ray program with \\'ray.init(address=\"auto\")\\'.\\n# Tear down the cluster.\\n$ ray down ray/python/ray/autoscaler/aws/example-full.yaml\\n192 | Chapter 9: Ray Clusters\\n\\nUsing Other Cloud Providers\\nRay Clusters can be deployed on most major clouds, including GCP and Azure. Here\\nis a template to get started for Google Cloud:\\n# A unique identifier for the head node and workers of this cluster.\\ncluster_name : minimal\\n# The maximum number of worker nodes to launch in addition to the head\\n# node. min_workers default to 0.\\nmax_workers : 1\\n# Cloud-provider specific configuration.\\nprovider :\\n    type: gcp\\n    region: us-west1\\n    availability_zone : us-west1-a\\n    project_id : null # Globally unique project id\\n# How Ray will authenticate with newly launched nodes.\\nauth:\\n    ssh_user : ubuntu\\nHere is a template to get started for Azure:\\n# An unique identifier for the head node and workers of this cluster.\\ncluster_name : minimal\\n# The maximum number of workers nodes to launch in addition to the head\\n# node. min_workers default to 0.\\nmax_workers : 1\\n# Cloud-provider specific configuration.\\nprovider :\\n    type: azure\\n    location : westus2\\n    resource_group : ray-cluster\\n# How Ray will authenticate with newly launched nodes.\\nauth:\\n    ssh_user : ubuntu\\n    # You must specify paths to matching private and public key pair files.\\n    # Use `ssh-keygen -t rsa -b 4096` to generate a new ssh key pair.\\n    ssh_private_key : ~/.ssh/id_rsa\\n    # Changes to this should match what is specified in file_mounts.\\n    ssh_public_key : ~/.ssh/id_rsa.pub\\nY ou can read more about this on the Ray documentation .\\nWorking with Cloud Clusters | 193\\n\\nAutoscaling\\nRay is designed to support highly elastic workloads that are most efficient on an\\nautoscaling cluster. At a high level, the autoscaler attempts to launch and terminate\\nnodes to ensure that workloads have sufficient resources to run, while minimizing the\\nidle resources. It does this by considering:\\n•User-specified hard limits (min/max workers)•\\n•User-specified node types (nodes in a Ray Cluster do not have to be •\\nhomogenous)\\n•Information from the Ray Core’s scheduling layer about the current resource•\\nusage/demands of the cluster\\n•Programmatic autoscaling hints•\\nThe autoscaler resource demand scheduler will look at the pending tasks, actors, and\\nplacement groups, resource demands from the cluster. It will then try to add the\\nminimum list of nodes that can fulfill these demands.\\nWhen worker nodes are idle for more than idle_timeout_minutes , they will be\\nremoved. The head node is never removed unless the cluster is torn down.\\nThe autoscaler uses a simple binpacking algorithm  to pack the user demands into\\nthe available cluster resources. The remaining unfulfilled demands are placed on the\\nsmallest list of nodes that satisfies the demand while maximizing utilization (starting\\nfrom the smallest node). Y ou can learn more about the autoscaling algorithm in the\\nAutoscaling section of the Ray architecture whitepaper .\\nRay also provides documentation and tooling for other cluster managers such as\\nYARN, SLURM, and LFS. Y ou can read more about this in the Ray documentation .\\nSummary\\nIn this chapter you learned how to spin up your own Ray Clusters so that you can\\ndeploy your Ray applications on them. Besides manually setting up and shutting\\ndown clusters, we had a closer look at deploying Ray Clusters on Kubernetes using\\nKubeRay. We also looked at the Ray Cluster Launcher in detail and discussed how\\nto work with cloud clusters on clouds such as AWS, GCP , and Azure. Finally, we\\ndiscussed how to use the Ray autoscaler to scale your Ray Clusters.\\nNow that you know more about scaling Ray Clusters, we’ll come back to the applica‐\\ntion side of things in Chapter 10  and discuss how all the Ray ML libraries we’ve seen\\neffectively come together to form the Ray AI Runtime.\\n194 | Chapter 9: Ray Clusters\\n\\nCHAPTER 10\\nGetting Started with the Ray AI Runtime\\nWe’ve come a long way since you read about Ray AIR in Chapter 1 . Besides the\\nfundamentals of Ray Clusters and the basics of the Ray Core API, you’ve picked up\\na good understanding of all higher-level libraries of Ray that can be leveraged in\\nAI workloads, namely, Ray RLlib, Tune, Train, Datasets, and Serve in the chapters\\nleading up to this one. The main reason we deferred a deeper discussion of Ray AIR\\nuntil now is that it’s so much easier to think about its concepts and compute complex\\nexamples if you know its building blocks.\\nIn this chapter we’ll introduce you to the core concepts of Ray AIR and how you can\\nuse it to build and deploy common workflows. We’ll build an AIR application that\\nleverages many of Ray’s data science libraries that you already know about. We will\\nalso tell you when and why to use AIR and give you a brief overview of its technical\\nunderpinnings. An in-depth discussion of the relationship of AIR with other systems,\\nsuch as integrations and key differences, will be tackled in Chapter 11  when we talk\\nabout Ray’s ecosystem as it relates to AIR.\\nWhy Use AIR?\\nRunning ML workloads with Ray has been a constant evolution over the last couple\\nof years. Ray RLlib and Tune were the first libraries built on top of Ray Core.\\nComponents like Ray Train, Serve, and more recently Ray Datasets followed shortly\\nafter.  The addition of Ray AIR as an umbrella for all other Ray ML libraries is the\\nresult of active discussions with and feedback from the ML community. Ray, as a\\nPython-native tool with good GPU support and stateful primitives (Ray actors) for\\ncomplex ML workloads, is a natural candidate for building a runtime like AIR.\\nRay AIR is a unified toolkit for your ML workloads that offers many third-party\\nintegrations for model training or accessing custom data sources. In the spirit of the\\n195\\n\\nother ML libraries built on top of Ray Core, AIR hides lower-level abstractions and\\nprovides an intuitive API that was inspired by common patterns from tools such as\\nscikit-learn.\\nAt its core, Ray AIR was built for both data scientists and ML engineers alike. As\\na data scientist, you can use it to build and scale your end-to-end experiments or\\nindividual subtasks such as preprocessing, training, tuning, scoring, or serving of ML\\nmodels.  As an ML engineer, you can go so far as to build a custom ML platform on\\ntop of AIR or simply leverage its unified API to integrate it with other libraries from\\nyour ecosystem. And Ray always gives you the flexibility to drop down and delve into\\nthe lower-level Ray Core API.\\nAs part of the Ray ecosystem, AIR can leverage all its benefits, which includes a\\nseamless transition from experimentation on a laptop to production workflows on a\\ncluster. Y ou often see data science teams “hand over” their ML code to teams respon‐\\nsible for production systems. In practice this can be expensive and time-consuming,\\nas this process often involves modifying or even rewriting parts of the code. As we\\nwill see, Ray AIR helps you with this transition because AIR takes care of concerns\\nsuch as scalability, reliability, and robustness for you.\\nRay AIR already has a respectable number of integrations today, but it’s also fully\\nextensible.  And as we will show you in the next section, its unified API provides a\\nsmooth workflow that allows you to drop-in-replace many of its components. For\\ninstance, you can use the same interface to define an XGBoost or PyTorch Trainer\\nwith AIR, which makes experimentation with various ML models convenient.\\nAt the same time, by choosing AIR you can avoid the problem of working with\\nseveral (distributed) systems and writing glue code for them that’s difficult to deal\\nwith. Teams working with many moving parts often experience rapid deprecation\\nof integrations and a high maintenance burden. These issues can lead to migration\\nfatigue , a reluctance to adopt new ideas due to the anticipated complexity of system\\nchanges.\\nAs with every chapter, you can follow the code examples in the\\naccompanying Jupyter notebook .\\n196 | Chapter 10: Getting Started with the Ray AI Runtime\\n\\nKey AIR Concepts by Example\\nAIR’s design philosophy is to provide you with the ability to tackle your ML workloads\\nin a single script, run by a single system . Let’s begin with AIR and its critical concepts\\nby walking through an extended usage example. Here’s what we’re going to do:\\n1.Load the breast cancer data set that you’ve already seen in Chapter 7  as a Ray 1.\\nDataset and use AIR to preprocess it.\\n2.Define an XGBoost model for training a classifier on this data.2.\\n3.Set up a so-called Tuner for our training procedure to tune its hyperparameters.3.\\n4.Store checkpoints of trained models.4.\\n5.Run batch prediction using AIR.5.\\n6.Deploy our predictor as a service with AIR.6.\\nY ou tackle these steps by building scalable pipelines with the AIR API. To follow\\nalong this example, make sure to install the following requirements:\\npip install \"ray[air]==2.2.0\" \"xgboost-ray>=0.1.10\" \"xgboost>=1.6.2\"\\npip install \"numpy>=1.19.5\" \"pandas>=1.3.5\" \"pyarrow>=6.0.1\" \"aiorwlock==1.3.0\"\\nFigure 10-1  summarizes the steps we’re going to take in the following example,\\nalongside the AIR components we’ll use.\\nFigure 10-1. From data loading to inference with AIR as the single distributed system\\nKey AIR Concepts by Example | 197\\n\\n1This gives you parity between your training and serving pipelines, which makes working with AIR convenient\\nbecause you don’t have to reimplement pipelines for different use cases.Ray Datasets and Preprocessors\\nThe standard way to load data in Ray AIR is with Ray Datasets. AIR Preprocessors\\nare used to transform input data into features for ML experiments. We’ve already\\nbriefly touched on preprocessors in Chapter 7  but have not discussed them in the\\ncontext of AIR yet.\\nSince Ray AIR Preprocessors operate on Datasets and leverage the Ray ecosystem,\\nthey allow you to scale your preprocessing steps efficiently. During training an AIR\\nPreprocessor is fitted  to the specified training data and can then later be used for both\\ntraining and serving.1 AIR comes packaged with many common preprocessors that\\ncover many use cases. If you don’t find the one you need, you can easily define a\\ncustom preprocessor on your own.\\nIn our example, we want to read a CSV file from an S3 bucket into a columnar\\ndataset first, using the read_csv  utility.  Then we split our dataset into a training and\\na test dataset and define an AIR Preprocessor, StandardScaler , which normalizes all\\nspecified columns of our dataset to have a mean of 0 and a variance of 1. Note that\\njust specifying a preprocessor does not transform the data just yet. Here is how you\\nimplement this:\\nimport ray\\nfrom ray.data.preprocessors  import StandardScaler\\ndataset = ray.data.read_csv (\\n    \"s3://anonymous@air-example-data/breast_cancer.csv\"\\n    )  \\ntrain_dataset , valid_dataset  = dataset.train_test_split (test_size =0.2)\\ntest_dataset  = valid_dataset .drop_columns (cols=[\"target\" ])  \\npreprocessor  = StandardScaler (columns=[\"mean radius\" , \"mean texture\" ])  \\nLoad the breast cancer CSV file from S3 using Ray Datasets.\\nAfter defining a training and a test dataset, we drop the target  column on the\\ntest data.\\nDefine an AIR Preprocessor to scale two variables of the dataset  to be normally\\ndistributed.\\n198 | Chapter 10: Getting Started with the Ray AI Runtime\\n\\nNote that for simplicity we’re using the test dataset as a validation dataset in future\\ntraining as well, hence the naming convention.\\nBefore moving on to the training step in AIR workflows, let’s look at the different\\ntypes of AIR Preprocessors that are available to you ( Table 10-1 ). If you want to know\\nmore about all available preprocessors, you can consult the user guide on this topic .\\nIn this book we’re using preprocessors only for feature scaling, but the other types of\\nRay AIR Preprocessors can be very useful as well.\\nTable 10-1. Ray AIR Preprocessors\\nPreprocessor type Examples\\nFeature scalers MaxAbsScaler, MinMaxScaler, Normalizer, PowerTransformer, StandardScaler\\nGeneric preprocessors BatchMapper, Chain, Concatenator, SimpleImputer\\nCategorical encoders Categorizer, LabelEncoder, OneHotEncoder\\nText encoders Tokenizer, FeatureHasher\\nTrainers\\nOnce you have your training and test datasets ready and your preprocessors defined,\\nyou can move on to specifying a Trainer that runs an ML algorithm on your data.\\nTrainers from the Ray Train package were introduced in Chapter 7 ; they provide\\na consistent wrapper for training frameworks such as TensorFlow, PyTorch, or\\nXGBoost. In this example we’ll focus on the latter, but it’s important to note that\\nall other framework integrations work exactly the same way in terms of the Ray AIR\\nAPI.\\nLet’s define a so-called XGBoostTrainer , one of the many specific Trainer  implemen‐\\ntations that come with Ray AIR.  Defining such a trainer requires you to specify these\\narguments:\\n•An AIR ScalingConfig  that describes how you want to scale out training on •\\nyour Ray Cluster\\n•A label_column  that specifies which column of your dataset is used as a label  in •\\nsupervised learning with XGBoost\\n•A datasets  argument with at least a train  key and an optional valid  key to •\\nspecify the training and validation datasets, respectively\\n•An AIR preprocessor  to compute the features of your ML model •\\n•Framework-specific parameters (e.g., the number of boosting rounds in•\\nXGBoost), as well as a common set of parameters called params  visualized in\\nFigure 10-2 :\\nfrom ray.air.config  import ScalingConfig\\nfrom ray.train.xgboost  import XGBoostTrainer\\nKey AIR Concepts by Example | 199\\n\\n2Technically speaking, not every trainer needs to specify a datasets  argument. Y ou can also use framework-\\nspecific data loaders, though we can’t show you any examples here.trainer = XGBoostTrainer (\\n    scaling_config =ScalingConfig (  \\n        num_workers =2,\\n        use_gpu=False,\\n    ),\\n    label_column =\"target\" ,\\n    num_boost_round =20,  \\n    params={\\n        \"objective\" : \"binary:logistic\" ,\\n        \"eval_metric\" : [\"logloss\" , \"error\"],\\n    },\\n    datasets ={\"train\": train_dataset , \"valid\": valid_dataset },  \\n    preprocessor =preprocessor ,  \\n)\\nresult = trainer.fit()  \\nprint(result.metrics)\\nEvery Trainer  comes with a scaling configuration. Here we’re using two Ray\\nworkers and no GPU.\\nXGBoostTrainers  need specific configuration, as well as a training\\nobjective  and evaluation metrics to track.\\nThe Trainer  specifies the datasets it’s supposed to operate on.2\\nIn the same way, you can provide AIR Preprocessors that the Trainer  should\\nuse.\\nAfter everything is defined, a simple fit call is enough to start the training\\nprocedure.\\nTrainers provide scalable ML training that operates on AIR Datasets and preproces‐\\nsors. On top of that, they’re also built to integrate well with Ray Tune for HPO, as\\nwe’ll see next.\\nTo summarize this section, Figure 10-2  shows how AIR Trainers fit ML models on\\nRay Datasets given AIR Preprocessors and a scaling configuration.\\n200 | Chapter 10: Getting Started with the Ray AI Runtime\\n\\nFigure 10-2. AIR Trainers operate on Ray Datasets and use AIR Preprocessors and\\nscaling configurations\\nTuners and Checkpoints\\nTuners , introduced with Ray 2.0 as part of AIR, offer scalable hyperparameter tuning\\nthrough Ray Tune. Tuners work seamlessly with AIR Trainers, but also support arbi‐\\ntrary training functions. In our example, instead of calling fit()  on your trainer\\ninstance from the previous section, you can pass your trainer  into a Tuner. To do\\nso, a Tuner needs to be instantiated with a parameter space to search over, a so-called\\nTuneConfig . This config has all Tune-specific configurations like the metric you\\nwant to optimize and an optional RunConfig  that lets you configure runtime-specific\\naspects such as the log verbosity of your Tune run.\\nContinuing with the XGBoostTrainer  we defined earlier, here’s how you wrap that\\ntrainer  instance in a Tuner to calibrate the max_depth  parameter of your XGBoost\\nmodel:\\nfrom ray import tune\\nparam_space  = {\"params\" : {\"max_depth\" : tune.randint(1, 9)}}\\nmetric = \"train-logloss\"\\nfrom ray.tune.tuner  import Tuner, TuneConfig\\nfrom ray.air.config  import RunConfig\\ntuner = Tuner(\\n    trainer,  \\n    param_space =param_space ,  \\n    run_config =RunConfig (verbose=1),\\n    tune_config =TuneConfig (num_samples =2, metric=metric, mode=\"min\"),  \\n)\\nresult_grid  = tuner.fit()  \\nbest_result  = result_grid .get_best_result ()\\nprint(\"Best Result:\" , best_result )\\nKey AIR Concepts by Example | 201\\n\\nInitialize your Tuner with a Trainer instance, which in turn specifies the scaling\\nconfiguration of the run.\\nY our Tuner also takes a param_space  to search over.\\nIt also needs a dedicated TuneConfig  to tell Tune how to optimize your Trainer,\\ngiven its parameter space.\\nTuner runs are started the same way as Trainers, namely, by calling .fit() .\\nWhenever you run AIR Trainers or Tuners, they generate framework-specific check‐\\npoints.  Y ou can use these checkpoints to load models for usage across several AIR\\nlibraries, such as Tune, Train, or Serve. Y ou can get a checkpoint by accessing the\\nresult of a .fit()  call on either a Trainer or a Tuner. In our example, that means you\\ncan simply access a checkpoint  on the best_result  object, or any other entry from\\nthe result_grid  like this:\\ncheckpoint  = best_result .checkpoint\\nprint(checkpoint )\\nThe other main way to work with checkpoints is by creating one from an existing,\\nframework-specific model. Every ML framework supported by AIR can be used to do\\nthis, but since it’s easiest to define a simple model with it, we’re going to show you\\nhow this looks for a sequential TensorFlow Keras model:\\nfrom ray.train.tensorflow  import TensorflowCheckpoint\\nimport tensorflow  as tf\\nmodel = tf.keras.Sequential ([\\n    tf.keras.layers.InputLayer (input_shape =(1,)),\\n    tf.keras.layers.Dense(1)\\n])\\nkeras_checkpoint  = TensorflowCheckpoint .from_model (model)\\nHaving checkpoints is great because they’re AIR’s native model exchange format.\\nY ou can also use them to pick up trained models at a later stage, without having\\nto worry about custom ways to store and load the models in question. Figure 10-3\\nschematically shows how AIR Tuners work with AIR Trainers.\\n202 | Chapter 10: Getting Started with the Ray AI Runtime\\n\\nFigure 10-3. An AIR Tuner calibrates the hyperparameters of AIR Trainers\\nBatch Predictors\\nOnce you have trained a model through AIR, that is, by fitting either a Trainer\\nor a Tuner , you can use the resulting AIR Checkpoint for prediction on batches\\nof data in Python. To do that you create a BatchPredictor  from your Checkpoint\\nand then use its predict  method on your Dataset. In our case we need to use the\\nframework-specific class of the predictor, namely, XGBoostPredictor , to tell AIR how\\nto load the checkpoint  correctly:\\nfrom ray.train.batch_predictor  import BatchPredictor\\nfrom ray.train.xgboost  import XGBoostPredictor\\ncheckpoint  = best_result .checkpoint\\nbatch_predictor  = BatchPredictor .from_checkpoint (checkpoint , XGBoostPredictor ) \\npredicted_probabilities  = batch_predictor .predict(test_dataset ) \\npredicted_probabilities .show()\\nLoad an XGBoost model from a checkpoint into a BatchPredictor  object.\\nRun batch inference on our test dataset to get predicted probabilities.\\nThis can be visualized in Figure 10-4 .\\nFigure 10-4. Using AIR BatchPredictors  from AIR Checkpoints to run batch inference\\non AIR Datasets\\nKey AIR Concepts by Example | 203\\n\\n3If you run the following example from a Jupyter notebook, you do not have to worry about it blocking\\nthe notebook—it will run just fine. Starting a server like this is often implemented as a blocking call, but\\nPredictorDeployment  isn’t.Deployments\\nInstead of using a BatchPredictor  and interacting with the model in question\\ndirectly, you can leverage Ray Serve to deploy an inference service that you can\\nquery over HTTP . Y ou do that by using the PredictorDeployment  class and deploy\\nit using our checkpoint . The only slightly tricky part about this is that our model\\noperates on dataframes, which we can’t directly send over HTTP . That means we need\\nto explicitly tell our prediction service how to pick up and transform the payload  we\\ndefine and create a dataframe from it. We do this by specifying an adapter  for our\\ndeployment:3\\nfrom ray import serve\\nfrom fastapi import Request\\nimport pandas as pd\\nfrom ray.serve  import PredictorDeployment\\nasync def adapter(request: Request):  \\n    payload = await request.json()\\n    return pd.DataFrame .from_dict (payload)\\nserve.start(detached =True)\\ndeployment  = PredictorDeployment .options(name=\"XGBoostService\" )  \\ndeployment .deploy(  \\n    XGBoostPredictor ,\\n    checkpoint ,\\n    http_adapter =adapter\\n)\\nprint(deployment .url)\\nAn adapter takes an HTTP request object and returns data in a format accepted\\nby our model.\\nAfter starting Serve we can create a deployment for our model.\\nTo actually deploy  the deployment object we need to pass in the model check\\npoint , the adapter  function, and the XGBoostPredictor  class for correct model\\nloading.\\n204 | Chapter 10: Getting Started with the Ray AI Runtime\\n\\nTo test this deployment, let’s create some sample input from our test data that we\\ncan throw at our service. For simplicity, we take the first item of our test dataset and\\nconvert it to a Python dictionary so that the ubiquitous Requests library can post  a\\nrequest to our deployment URL with it:\\nimport requests\\nfirst_item  = test_dataset .take(1)\\nsample_input  = dict(first_item [0])\\nresult = requests .post(  \\n    deployment .url,\\n    json=[sample_input ]\\n)\\nprint(result.json())\\nserve.shutdown ()  \\nPosts our sample_input  to deployment.url  with requests .\\nAfter you’re finished using the service, you can safely shut down Ray Serve.\\nFigure 10-5  summarizes how AIR deployments with the PredictorDeployment  class\\nwork.\\nFigure 10-5. Creating PredictorDeployments from AIR Checkpoints to serve models that\\nusers can interact with\\nFigure 10-6  gives you a quick visual overview of all the components and concepts\\ninvolved in Ray AIR, including pseudocode for all the main AIR components we’ve\\ncovered in this chapter.\\nKey AIR Concepts by Example | 205\\n\\nFigure 10-6. AIR combines many Ray libraries by providing a unified  API for common\\ndata science workloads\\nIt’s important to stress again that we’ve been using a single Python script  for this\\nexample and a single distributed system in Ray AIR to do all the heavy lifting. In\\nfact, you can use this example script and scale it out to a large cluster that uses CPUs\\nfor preprocessing and GPUs for training and separately configure the deployment\\nsimply by modifying the parameters of the scaling configuration and similar options\\nin that script. This isn’t as easy or common as it may seem, and it is not unusual\\nfor data scientists to have to use multiple frameworks (e.g., one for data loading and\\nprocessing, one for training, and one for serving).\\n206 | Chapter 10: Getting Started with the Ray AI Runtime\\n\\n4Of course, the model used for inference has to be loaded first, but since its parameters don’t change during\\nprediction, trained models can be considered static data in this case. We sometimes refer to this situation as\\nsoft state .\\n5Sometimes AIR uses Ray actors for stateless tasks for performance reasons, such as caching models in batch\\ninference.\\nY ou can also use Ray AIR with RLlib, but the integration is still in\\nits early stages. For instance, to integrate RLlib with AIR Trainers,\\nyou’ d use the RLTrainer  that allows you to pass in all arguments\\nthat you’ d pass to a standard RLlib algorithm. After training, you\\ncan store the resulting RL model in an AIR Checkpoint, just as\\nwith any other AIR Trainer. To deploy your trained RL model,\\nyou can use Serve’s PredictorDeployment  class by passing your\\ncheckpoint along with the RLPredictor  class.\\nThis API might be subject to change, but you can see an example of\\nhow this works in the AIR documentation .\\nWorkloads That Are Suited for AIR\\nNow that we’ve seen examples of AIR and its fundamental concepts, let’s zoom out\\na little and discuss in principle which kinds of workloads you can run with it. We’ve\\ntackled all of these workloads already throughout the book, but it’s good to recap\\nthem systematically. As the name suggests, AIR is built to capture common tasks in\\nAI projects. These tasks can be roughly classified in the following way:\\nStateless computation\\nTasks like preprocessing data or computing model predictions on a batch of data\\nare stateless.4 Stateless workloads can be computed independently in parallel.\\nIf you recall our treatment of Ray tasks from Chapter 2 , stateless computation\\nis exactly what they were built for. AIR primarily uses Ray tasks for stateless\\nworkloads.5 Many big data processing  tools fall into this category.\\nStateful computation\\nIn contrast, model training and hyperparameter tuning are stateful operations, as\\nthey update the model state during their respective training procedure. Updating\\nstateful workers in such distributed training  is a difficult topic that Ray handles\\nfor you. AIR uses Ray actors for stateful computations.\\nComposite workloads\\nCombining stateless and stateful computation, for instance by first processing\\nfeatures and then training a model, is quite common in AI workloads. In fact,\\nit’s rare for end-to-end projects to exclusively use one or the other. Running such\\nadvanced composite workloads in a distributed fashion can be described as big\\nWorkloads That Are Suited for AIR | 207\\n\\n6Note that preprocessors can be stateful, but we haven’t discussed any examples of that scenario here.data training , and AIR is built to handle both the stateless and stateful parts\\nefficiently.\\nOnline serving\\nLastly, AIR is built to handle scalable online serving of (multiple) models. The\\ntransition from the previous three workloads to serving is frictionless by design,\\nas you still operate within the same AIR ecosystem.\\nFigure 10-7  illustrates the typical stateless tasks of Ray AIR.\\nFigure 10-7. Stateless AIR tasks\\nThese four tasks map to Ray’s libraries straightforwardly. For instance, in this\\nchapter we’ve discussed several ways in which Ray Datasets are used for stateless\\ncomputation.  Y ou could run batch inference on a given Dataset by passing it into a\\nBatchPredictor  loaded from an AIR Checkpoint. Or you could preprocess a Dataset\\nto produce features for later training.6\\nLikewise, there are three AIR libraries dedicated to stateful computations, namely,\\nTrain, Tune, and RLlib. As we’ve seen, both Train and RLlib integrate seamlessly with\\nTune in AIR by passing the respective Trainer  objects into a Tuner .\\nWhen it comes to advanced composite workloads, Ray AIR and its combined usage\\nof both tasks and actors really shines. For instance, some ML training procedures\\nneed to perform complex data processing tasks during training. Others may require\\nshuffling the training dataset before each epoch. Since Ray AIR’s training libraries\\n(based on actors) can seamlessly leverage data processing operations (mostly based\\non tasks), even the most complex use cases can be reflected in AIR.\\nAlso, since you can use any AIR Checkpoint with Ray Serve, AIR makes it easy to\\nswitch from training to serving workloads, using the same infrastructure. We’ve seen\\nhow you can use a PredictorDeployment  to host a model behind an HTTP endpoint,\\nwhich is optimized for low latency and high throughput. By deploying AIR, you can\\n208 | Chapter 10: Getting Started with the Ray AI Runtime\\n\\n7On top of multiple replicas for single models, AIR also supports deployment of multiple models. This allows\\nyou to compose multiple models or run A/B testing.scale your prediction services to multiple replicas and use Ray’s autoscaling features\\nto adjust your cluster according to inbound traffic.7\\nY ou can use these types of workloads in different scenarios, too. For instance, you can\\nuse AIR to replace and scale out a single component of an existing pipeline. Or you\\ncan create your own end-to-end machine learning apps with AIR, as we’ve indicated\\nin this chapter. Lastly, you can also use AIR to build your own AI platform, a topic\\nthat we’ll look at in Chapter 11 .\\nFigure 10-8  summarizes how these four types of AI workloads are covered by AIR as\\npart of the Ray ecosystem.\\nFigure 10-8. The four types of AI workloads AIR enables you to run on Ray Clusters\\nNext, we’ll discuss several aspects of each of these four workload types of AIR in more\\ndetail.  Specifically, we will investigate how Ray executes such workloads internally.\\nAlso, we’re going to dive a little deeper into the technical aspects behind Ray AIR,\\nsuch as how it manages memory or handles failures. We can give you only a brief\\noverview of these topics but will provide links to more advanced material as we go.\\nAIR Workload Execution\\nLet’s have a closer look at the execution model of AIR.\\nStateless execution\\nRay Datasets use Ray tasks or actors to execute transformations. Tasks are preferred\\nsince they allow for easier and more flexible scheduling. The Datasets library uses a\\nscheduling strategy that evenly balances tasks and their output across your cluster.\\nWorkloads That Are Suited for AIR | 209\\n\\n8All crucial AIR components such as Trainer  or BatchPredictor  support this pipelining feature.Actors are used if a transformation has state or needs an expensive setup, like loading\\nlarge model checkpoints. In this case, loading large models in an actor once to\\nreuse it for inference tasks can benefit overall performance. When Ray Datasets is\\nusing actors, these actors are created first, and the necessary data (in our example\\nthe loaded model) is transferred to them before the transformation in question gets\\nexecuted.\\nIn general, datasets are stored in Ray object store memory, while large datasets are\\nspilled to disk. But for stateless transformations there is often no need to keep inter‐\\nmediate results in memory. Using pipelining  on Datasets, as we’ve shown in Chap‐\\nter 6 , data can instead be streamed to and from storage to increase performance.8 The\\nidea is to load only the fraction of the data currently needed to execute a transforma‐\\ntion. This can drastically reduce the memory footprint of the transformation and\\noften speed up the overall execution.\\nStateful execution\\nRay Train and RLlib spawn actors for their distributed training workers. As demon‐\\nstrated multiple times, both libraries also integrate seamlessly with Tune. In Chapter 5\\nwe detailed how Tune launches Trials, which in essence are groups of actors executing\\na certain workload. If you use Train or RLlib with Tune, that in turn means that a\\ntree of actors gets created, namely, an actor for each Tune Trial, and subactors for the\\nparallel training workers requested by Train or RLlib.\\nIn terms of workload execution, this naturally creates an inner and an outer layer.\\nEach subactor in a Tune Trial has full autonomy over its workload, as requested in\\nthe respective Train or RLlib training run you specified. This represents the inner\\nexecution layer. On the outer layer, Tune needs to monitor the status of individual\\ntrials, which it does by periodically reporting metrics to the Trial driver. Figure 10-9\\nillustrates this nested creation and execution of Ray actors for this scenario.\\nFigure 10-9. Running distributed training on AIR with Tune\\n210 | Chapter 10: Getting Started with the Ray AI Runtime\\n\\nComposite workload execution\\nComposite workloads leverage both task- and actor-based computation simultane‐\\nously.  This can lead to interesting resource allocation challenges. Trial actors need to\\nreserve their resources upfront, but stateless tasks don’t. The problem you can run\\ninto is that all available resources might get reserved for your training actors with\\nnone left for data loading tasks.\\nAIR prevents this by allowing Tune to reserve a maximum of 80% of a node’s\\nCPU. Y ou can tune this parameter, but this is a sensible default that ensures basic\\nresource availability for stateless computations. In the common scenario in which\\nyour training operations leverage GPUs and your data processing steps don’t, this\\nbecomes a nonissue.\\nOnline serving execution\\nFor online serving Ray Serve manages a pool of stateless actors to serve your requests.\\nSome actors listen for incoming requests and call other actors to perform the predic‐\\ntions. Requests are automatically load-balanced using a round-robin algorithm to the\\npool of actors hosting the models. Load metrics are sent to the Serve component to\\nperform autoscaling.\\nAIR Memory Management\\nIn this section we take a bit of a deep dive into the specific memory management\\ntechniques of AIR. We’ll discuss to what extent the Ray object store is used by AIR.\\nY ou can skip this section if you find it too technical. The upshot is that Ray employs\\nsmart techniques that will ensure your data and compute are properly distributed and\\nscheduled.\\nWhen you load data using Ray Datasets, you already know that internally these\\nDatasets are partitioned into blocks of data in your cluster. A block is simply a\\ncollection of Ray objects. Choosing the right block size is difficult and presents a\\ntrade-off between the overhead of having to manage too many small blocks and\\nrisking out-of-memory (OOM) exceptions due to blocks that are too large. AIR takes\\na pragmatic approach and tries to distribute in such a way that blocks don’t exceed\\n512 MB. In case this can’t be ensured, a warning will be issued. Should a block not fit\\ninto memory, AIR will spill your data to local disk.\\nY our stateful workloads will use the Ray object store to varying degrees. For instance,\\nRLlib uses Ray objects to broadcast model weights to individual rollout workers and\\nfor experience data collection. Tune uses it to set up Trials by sending and retrieving\\nAIR Checkpoints. For technical reasons actors risk running into OOM issues if too\\nWorkloads That Are Suited for AIR | 211\\n\\n9Stateful workloads use Python’s heap memory, which is not managed by Ray.\\n10GCS can be deployed in high availability  (HA) mode to prevent this, but that is typically beneficial only for\\nonline serving workloads.much memory is required relative to the allocated resources.9 If you know your\\nmemory requirements in advance, you can adapt the memory  in your ScalingConfig\\naccordingly, or simply ask for additional cpu resources.\\nIn composite workloads stateful actors (e.g., for training) have to access data created\\nby stateless tasks (e.g., for preprocessing), which makes memory allocation more\\nchallenging. Let’s look at two scenarios:\\n•If the actors responsible for training have enough space (in the object store) to fit•\\nall training data in memory, the situation is simple. First the preprocessing steps\\nrun, and then all data blocks are downloaded to the respective nodes. Training\\nactors then simply iterate data that’s kept in memory.\\n•Otherwise, data processing needs pipelined execution , which means that data will •\\nbe processed by tasks on the fly and will be downloaded on demand by training\\nActors afterward. If the respective training actor is co-located on the node that\\ndid the processing, data will be retrieved from shared memory.\\nAIR Failure Model\\nAIR offers fault tolerance for most stateless computations through lineage reconstruc‐\\ntion. This means Ray will reconstruct Dataset blocks if they are lost due to node\\nfailures by resubmitting the necessary tasks, enabling workloads to scale to large\\nclusters. Note that fault tolerance does not apply to head node failures. And a crash of\\nthe Global Control Service (GCS) storing cluster metadata will kill all your jobs in the\\ncluster.10\\nJobs involving stateful computations primarily rely on checkpoint-based fault toler‐\\nance.  Tune will restart distributed trials from their last checkpoint as configured in\\nits failure configuration. With a configured checkpoint interval, this means that Tune\\ncan run trials effectively on clusters consisting of “spot instances. ” In addition, it is\\npossible to resume entire Tune experiments from the experiment-wide checkpoint in\\ncase of whole-cluster failure.\\nComposite workloads inherit the fault-tolerance strategies of both stateless and\\nstateful workloads, retaining the best of both worlds. This means that lineage recon‐\\nstruction applies to the stateless portion of the workload, and application-level check‐\\npointing still applies to the overall computation.\\n212 | Chapter 10: Getting Started with the Ray AI Runtime\\n\\nAutoscaling AIR Workloads\\nAIR libraries can run on autoscaling Ray Clusters that we introduced in Chapter 9 .\\nFor stateless workloads, Ray will autoscale automatically if there are queued tasks (or\\nqueued Dataset compute actors). For stateful workloads, Ray will autoscale up if there\\nare pending placement groups (i.e., Tune trials) not yet scheduled in the cluster. Ray\\nwill autoscale down when nodes are idle. A node is considered idle when there is no\\nresource usage on the node and also no Ray objects in memory or spilled on disk\\non the node. Since most AIR libraries leverage objects, this means that nodes may be\\nkept if they are holding objects referenced by workers on other nodes (e.g., Dataset\\nblock used by another trial).\\nY ou should be aware that autoscaling may result in less than ideal data balancing\\nin the cluster because nodes that are started earlier naturally run more tasks over\\ntheir lifetime. Consider limiting (e.g., starting with a certain minimum cluster size) or\\ndisabling autoscaling to optimize the efficiency of data-intensive workloads.\\nSummary\\nIn this chapter you’ve seen how all the Ray libraries we’ve introduced come together\\nto form the Ray AI Runtime. Y ou’ve learned about all the key concepts that allow\\nyou to build scalable ML projects, from experimentation to production. In particular,\\nyou’ve seen how Ray Datasets are used for stateless computations such as feature\\npreprocessing, and how Ray Train, Tune, and RLlib are used for stateful computa‐\\ntions such as model training. Seamlessly combining these types of computations in\\ncomplex AI workloads and scaling them out to large clusters is a key strength of AIR.\\nDeploying your AIR projects comes essentially for free, as AIR fully integrates with\\nRay Serve as well.\\nIn Chapter 11  we’ll show you how Ray, and particularly AIR, fits into the broader\\nlandscape of related tools. Knowing the rich set of integrations and extensions of\\nRay’s ecosystem will help you understand how to leverage Ray in your own projects.\\nSummary | 213\\n\\n\\n\\nCHAPTER 11\\nRay’s Ecosystem and Beyond\\nOver the course of this book, you’ve seen many examples of Ray’s ecosystem.  Now it’s\\ntime to take a more systematic approach and show you the full extent of integrations\\ncurrently available for Ray. We do so by discussing this ecosystem as seen from Ray\\nAIR so that we can discuss it in the context of a representative AIR workflow.\\nClearly, we can’t give you concrete code examples for a majority of the libraries in\\nRay’s ecosystem. Instead, we have to be content with giving you another Ray AIR\\nexample showcasing some integrations and discussing what others are available and\\nhow to use them. Where appropriate, we’ll point you to more advanced resources to\\ndeepen your understanding.\\nNow that you know much more about Ray and its libraries, this chapter is also the\\nright place to compare what Ray offers to similar  systems. As you’ve seen, Ray’s eco‐\\nsystem is quite complex, can be seen from different angles, and is used for different\\npurposes. That means many aspects of Ray can be compared to other tools in the\\nmarket.\\nWe’ll also comment on how to integrate Ray into more complex workflows in existing\\nML platforms. To wrap things up, we’ll give you an idea how to continue your\\njourney of learning Ray  after finishing this book.\\nThe notebook for this chapter is available on GitHub .\\n215\\n\\n1Y ou can find the current list of integrations to the AIR ecosystems in the Ray documentation . More generally,\\nyou find a list of integrations for Ray on the Ray Ecosystem page . On the latter, you find many more\\nintegrations than the ones we discuss here, such as ClassyVision, Intel Analytics Zoo, John Snow Labs’ NLU,\\nLudwig AI, PyCaret, and SpaCy.A Growing Ecosystem\\nTo give you a glimpse of Ray’s ecosystem by means of a concrete example,1 we’re\\ngoing to show you how to use Ray AIR with data and models from the PyTorch\\necosystem, how to log your hyperparameter tuning runs to MLflow, and how to\\ndeploy your trained models with Ray’s Gradio integration. Along the way we give\\nyou an overview and discuss usage patterns of other noteworthy integrations at each\\nrespective stage.\\nTo follow the code examples in this chapter, make sure to install the following\\ndependencies in your Python environment:\\npip install \"ray[air, serve]==2.2.0\" \"gradio==3.5.0\" \"requests==2.28.1\"\\npip install \"mlflow==1.30.0\" \"torch==1.12.1\" \"torchvision==0.13.1\"\\nWe’re going to load and transform a dataset using utilities from the PyTorch frame‐\\nwork and then convert this data into a Ray Dataset to work with it in Ray AIR.\\nWe then define a standard PyTorch model and a simple training loop that we can\\nleverage in Ray Train. Next, we wrap this TorchTrainer  in a Tuner and log trial\\nresults to MLflow using the MLflowLogger  that ships with Ray Tune.  Finally, we’re\\ngoing to serve our trained model using Gradio running on Ray Serve.\\nIn other words, the example we’re discussing takes common Python data science\\nlibraries that you might already use and wraps them in an AIR workflow by leverag‐\\ning Ray’s ecosystem integrations. The focus is more on these integrations and how\\nthey interface with AIR and less on the concrete use case.\\nData Loading and Processing\\nIn Chapter 6  you learned about the basics of Ray Datasets, how to create them from\\ncommon Python data structures, how to load Parquet files from storage systems like\\nS3, and how to use Ray’s Dask on Ray integration to interface with Dask.\\nTo give you yet another example of Ray Dataset’s capabilities, let’s see how you can\\nwork with image data loaded through PyTorch’s torchvision  extension. The idea\\nis simple. We’re going to load the common CIFAR-10 dataset available in PyTorch\\nthrough the torchvision.datasets  package and then make it a Ray Dataset. Specifi‐\\ncally, we’re going to define a function called load_cifar  that returns the CIFAR-10\\ndata for training or testing:\\n216 | Chapter 11: Ray’s Ecosystem and Beyond\\n\\nfrom torchvision  import transforms , datasets\\ndef load_cifar (train: bool):\\n    transform  = transforms .Compose([  \\n        transforms .ToTensor (),\\n        transforms .Normalize ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\\n    ])\\n    return datasets .CIFAR10(  \\n        root=\"./data\" ,\\n        download =True,\\n        train=train,  \\n        transform =transform\\n    )\\nUses a PyTorch transform to return normalized Tensor data.\\nLoads the CIFAR-10 dataset using the datasets  module of the torchvision\\npackage.\\nMakes it so that the loader  function returns either training or testing data.\\nNote that so far we haven’t touched any Ray library, and you could have used any\\nother dataset or transform from PyTorch in the same way. To get Ray Datasets for\\nuse with AIR, we’re going to supply our load_cifar  data loader functions to the\\nfrom_torch  utility from Ray AIR:\\nfrom ray.data  import from_torch\\ntrain_dataset  = from_torch (load_cifar (train=True))\\ntest_dataset  = from_torch (load_cifar (train=False))\\nThe CIFAR-10 dataset is used for image classification tasks; it consists of 32-pixel\\nsquare images and comes with labels of a total of 10 categories. So far we’ve loaded\\nthis dataset only in the form provided by PyTorch, but we still need to transform it\\nto use it with an AIR Trainer. Y ou do this by creating an image  and a label  column\\nthat we can then reference in a Trainer. The best way to do so is by mapping batches  of\\ntrain and test data to a dictionary of NumPy arrays with precisely these two columns:\\nimport numpy as np\\ndef to_labeled_image (batch):  \\n    return {\\n        \"image\": np.array([image.numpy() for image, _ in batch]),\\n        \"label\": np.array([label for _, label in batch]),\\n    }\\nA Growing Ecosystem | 217\\n\\n2Y ou can see the continually updated list of supported formats in the Ray Datasets documentation . The\\nsupported output  formats for Datasets largely overlap with the input formats.train_dataset  = train_dataset .map_batches (to_labeled_image )  \\ntest_dataset  = test_dataset .map_batches (to_labeled_image )\\nTransforms each batch  of data by returning an image  and a label  NumPy array.\\nApplies map_batches  to transform our initial datasets.\\nBefore we move on to model training, Table 11-1  shows the supported input formats\\nof the Ray Datasets library.2\\nTable 11-1. The Ray Datasets ecosystem\\nIntegration Type Description\\nText, binary, image\\nfiles,  CSV, JSONBasic data formats Supporting such basic formats should not strictly speaking be considered an\\nintegration , but it’s worth knowing that Ray Datasets can load and store\\nthese formats.\\nNumPy, Pandas, Arrow,\\nParquet, Python objectsAdvanced data\\nformatsRay Datasets supports working with common ML data libraries such as\\nNumPy and Pandas but can also read custom Python objects or read Parquet\\nfiles.\\nSpark, Dask, MARS,\\nModinAdvanced third-\\nparty integrationsRay interoperates with more complex data processing systems by means of\\ncommunity integrations, such as Spark on Ray (RayDP), Dask on Ray, MARS\\non Ray, or Pandas on Ray (Modin).\\nWe’ll talk about the relationship of Ray with systems such as Dask or Spark in more\\ndetail in “Distributed Python Frameworks” on page 227 .\\nModel Training\\nHaving properly shaped our CIFAR-10 to train and test data using Ray Datasets, we\\ncan now define a classifier to train our data on. As this is likely the most natural\\nscenario, we’re going to define a PyTorch model to define an AIR Trainer here. But\\nit’s worth reminding you of what you learned in Chapter 7 : you could just as easily\\nswitch frameworks at this point and work with Keras, Hugging Face, scikit-learn, or\\nany other library supported by AIR.\\nWe will proceed in three steps: define a PyTorch model, specify the training loop\\nthat AIR should run using this model, and define an AIR Trainer that we can fit on\\nour training data. To start with, let’s define a simple convolutional neural network\\nwith max pooling and rectified linear ( relu ) activations with PyTorch that’s built to\\noperate on our CIFAR-10 dataset. If you know PyTorch, the following definition of\\n218 | Chapter 11: Ray’s Ecosystem and Beyond\\n\\n3To convert any PyTorch model to Ray AIR, follow the user guide on this topic .the neural network Net should be straightforward. If you don’t, it suffices to know\\nthat to define a torch.nn.Module  the only thing you need to provide is the definition\\nof a forward  pass of your neural net:\\nimport torch\\nimport torch.nn  as nn\\nimport torch.nn.functional  as F\\nclass Net(nn.Module):\\n    def __init__ (self):\\n        super().__init__ ()\\n        self.conv1 = nn.Conv2d(3, 6, 5)\\n        self.pool = nn.MaxPool2d (2, 2)\\n        self.conv2 = nn.Conv2d(6, 16, 5)\\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\\n        self.fc2 = nn.Linear(120, 84)\\n        self.fc3 = nn.Linear(84, 10)\\n    def forward(self, x):\\n        x = self.pool(F.relu(self.conv1(x)))\\n        x = self.pool(F.relu(self.conv2(x)))\\n        x = torch.flatten(x, 1)\\n        x = F.relu(self.fc1(x))\\n        x = F.relu(self.fc2(x))\\n        x = self.fc3(x)\\n        return x\\nY ou already know that to define an AIR TorchTrainer  for our Net you need a train‐\\ning dataset, a scaling, and an optional run configuration.3 Y ou also need to tell AIR\\nwhat each worker should do when calling fit, by defining an explicit training loop\\nthat grants you maximal flexibility for your training process. The training function\\ntakes a config  dictionary that we can use to specify properties at runtime.\\nThe training loop we’re using here is just what you would expect: we simply load the\\nmodel and the data on each worker and then train on batches of data for a specified\\nnumber of epochs (while reporting the training progress). That’s a fairly standard\\ntraining loop, but there are a couple of crucial spots at which you have to be careful:\\n•To load the model on a worker, use the prepare_model  utility from •\\nray.train.torch  on our Net() .\\n•To access the data shard available to the worker, access get_dataset_shard  on •\\nyour current ray.air.session . For training, we use the \"train\"  key of that\\nshard and transform it to batches of the right size using iter_torch_batches .\\nA Growing Ecosystem | 219\\n\\n•To pass information about the training metrics you’re interested in, use•\\nsession.report  from AIR.\\nHere’s the full definition of our PyTorch training loop for each worker:\\nfrom ray import train\\nfrom ray.air import session, Checkpoint\\ndef train_loop(config):\\n    model = train.torch.prepare_model(Net())  \\n    loss_fct = nn.CrossEntropyLoss()\\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\\n    train_batches = session.get_dataset_shard(\"train\").iter_torch_batches(  \\n        batch_size=config[\"batch_size\"],\\n    )\\n    for epoch in range(config[\"epochs\"]):\\n        running_loss = 0.0\\n        for i, data in enumerate(train_batches):\\n            inputs, labels = data[\"image\"], data[\"label\"]  \\n            optimizer.zero_grad()  \\n            forward_outputs = model(inputs)\\n            loss = loss_fct(forward_outputs, labels)\\n            loss.backward()\\n            optimizer.step()\\n            running_loss += loss.item()  \\n            if i % 1000 == 0:\\n                print(f\"[{epoch + 1}, {i + 1:4d}] loss: \"\\n                      f\"{running_loss / 1000:.3f}\")\\n                running_loss = 0.0\\n        session.report(  \\n            dict(running_loss=running_loss),\\n            checkpoint=Checkpoint.from_dict(\\n                dict(model=model.module.state_dict())\\n            ),\\n        )\\nThe training loop defines the model, loss, and optimizer used first. Note the use\\nof prepare_model  here.\\nLoad the train dataset shard  on this worker and create an iterator containing\\nbatches of data from it.\\n220 | Chapter 11: Ray’s Ecosystem and Beyond\\n\\n4For instance, using the WandbLoggerCallback  you not only can log training results to Weights & Biases but\\nalso automatically upload your checkpoints, as demonstrated in this Ray tutorial .\\nAs per our earlier definition, our data  is a Pandas DataFrame that has an\\n\"image\"  and a \"label\"  column.\\nCompute the forward and backward pass in the training loop as we would with\\nany PyTorch model.\\nKeep track of a running loss term for each 1,000 training batches.\\nLastly, report  this loss using our AIR session  by passing a Checkpoint  with the\\ncurrent model state.\\nThe definition of this function might feel a bit long, given that we’re doing a\\nfairly standard training procedure. While it certainly would be possible to create\\na simple wrapper for PyTorch models in such cases, defining your own training\\nloop gives you full customizability to tackle more complex scenarios. We pass the\\ntraining_loop  to the train_loop_per_worker  argument of our AIR trainer and\\nspecify the configuration for this loop by passing a dictionary with the necessary keys\\nto the train_loop_config .\\nTo make things more interesting and to showcase another Ray integration, we will log\\nthe results of our TorchTrainer  training run to MLflow  by passing a callback to our\\nRunConfig , namely, an MLflowLoggerCallback :\\nfrom ray.train.torch  import TorchTrainer\\nfrom ray.air.config  import ScalingConfig , RunConfig\\nfrom ray.air.callbacks.mlflow  import MLflowLoggerCallback\\ntrainer = TorchTrainer (\\n    train_loop_per_worker =train_loop ,\\n    train_loop_config ={\"batch_size\" : 10, \"epochs\" : 5},\\n    datasets ={\"train\": train_dataset },\\n    scaling_config =ScalingConfig (num_workers =2),\\n    run_config =RunConfig (callbacks =[\\n        MLflowLoggerCallback (experiment_name =\"torch_trainer\" )\\n    ])\\n)\\nresult = trainer.fit()\\nY ou could also use other third-party logging libraries, such as Weights & Biases  or\\nCometML , by passing similar callbacks to your AIR Trainer or Tuner.4\\nA Growing Ecosystem | 221\\n\\nTable 11-2  summarizes all ML training-related integrations of Ray, which span both\\nRay Train and RLlib.\\nTable 11-2. The Ray Train and RLlib ecosystem\\nIntegration Type\\nTensorFlow, PyTorch, XGBoost, LightGBM, Horovod Train integrations maintained by the Ray Team\\nscikit-learn, Hugging Face, Lightning Train integrations maintained by the community\\nTensorFlow, PyTorch, OpenAI gym RLlib integrations maintained by the Ray Team\\nJAX, Unity RLlib integrations maintained by the Ray Team\\nWe distinguish between community-sponsored integrations and ones that are main‐\\ntained by the Ray team itself. Most integrations we’ve talked about in this book were\\nnative integrations, but due to the collaborative nature of open source software, you\\noften feel no difference in maturity between native and third-party integrations.\\nTo wrap up the training-related integrations of AIR, Table 11-3  offers a bird’s-eye\\nview at Tune’s ecosystem.\\nTable 11-3. The Ray Tune ecosystem\\nIntegration Type\\nOptuna, Hyperopt, Ax, BayesOpt, BOHB, Dragonfly,  FLAML, HEBO, Nevergrad,\\nSigOpt, skopt, ZOOptHyperparameter optimization library\\nTensorBoard, MLflow,  Weights & Biases, CometML Logging and experimentation\\nmanagement\\nModel Serving\\nGradio  is a popular way for practitioners to demo their ML models, and it provides\\nmany simple primitives to create graphical user interface elements simply by describ‐\\ning them through the gradio  library in Python.  As you will see, defining and deploy‐\\ning Gradio interfaces is straightforward, but it’s even easier to then wrap them in a\\nso-called GradioServer  from Ray Serve, which allows you to scale any Gradio app on\\na Ray Cluster.\\nTo showcase how to run a Gradio app with Ray Serve on the model we just trained,\\nlet’s first store the result  of our training procedure to disk. We do this by writing the\\nrespective AIR Checkpoint to a local folder of our choice so that we can restore this\\nmodel from checkpoint  in another script:\\nCHECKPOINT_PATH  = \"torch_checkpoint\"\\nresult.checkpoint .to_directory (CHECKPOINT_PATH )\\nNext, let’s create a file called gradio_demo.py  next to the \"torch_checkpoint\"  path\\nfor simplicity. In this script, let’s load our PyTorch model again by first restoring the\\n222 | Chapter 11: Ray’s Ecosystem and Beyond\\n\\n5For simplicity, we duplicated the definition of Net in gradio_demo.py  on GitHub.TorchCheckpoint  and then using this checkpoint and our Net()  definition to create a\\nTorchPredictor  that we can use for inference:\\n# gradio_demo.py\\nfrom ray.train.torch  import TorchCheckpoint , TorchPredictor\\nCHECKPOINT_PATH  = \"torch_checkpoint\"\\ncheckpoint  = TorchCheckpoint .from_directory (CHECKPOINT_PATH )\\npredictor  = TorchPredictor .from_checkpoint (\\n    checkpoint =checkpoint ,\\n    model=Net()\\n)\\nNote that this requires you to import, or otherwise make available, the definition of\\nNet in the gradio_demo.py  script.5\\nNext, we have to define the Gradio Interface , which we define to take images as\\ninput and produce labels as output. Additionally, we have to specify how an input\\nImage  has to be transformed to produce a Label . By default, Gradio represents\\nimages as NumPy arrays, so we can ensure that this array has the right shape\\nand data type and then pass it to our predictor . As this predictor , namely, our\\nTorchPredictor , produces a probability distribution, we take the argmax  of its pre‐\\ndiction to get an integer result that we can use as a label. Put the following code into\\nyour gradio_demo.py  Python script:\\nfrom ray.serve.gradio_integrations  import GradioServer\\nimport gradio as gr\\nimport numpy as np\\ndef predict(payload):  \\n    payload = np.array(payload, dtype=np.float32)\\n    array = payload.reshape((1, 3, 32, 32))\\n    return np.argmax(predictor .predict(array))\\ndemo = gr.Interface (  \\n    fn=predict,\\n    inputs=gr.Image(),\\n    outputs=gr.Label(num_top_classes =10)\\n)\\napp = GradioServer .options(  \\n    num_replicas =2,\\n    ray_actor_options ={\"num_cpus\" : 2}\\n).bind(demo)\\nA Growing Ecosystem | 223\\n\\n6When using the Gradio integration, Ray Serve will automatically run a Gradio app under the hood. The\\nGradio app is instrumented to access the type hints on each Serve deployment. When the user submits a\\nrequest, the Gradio app displays the output of each deployment using a Gradio Block that matches the output\\ntype.\\n7The application expects images of the right size, as we didn’t want to make preprocessing in predict  more\\ninvolved than necessary. Y ou can use the image data provided in the repository of this book  or simply search\\nfor CIFAR-10 images online to test the app yourself.\\nThe predict  function maps Gradio inputs to outputs by leveraging our\\npredictor .\\nThe Gradio interface has one input (an image), one output (a label for 1 of 10\\ncategories of the CIFAR-10 dataset), and a function fn to connect the two.\\nWe bind  the Gradio demo  to a Ray Serve GradioServer  object that gets deployed\\non two replicas with two CPUs each as resources.\\nTo run this application, you can now simply type the following command into a\\nshell:6\\nserve run gradio_demo:app\\nThis command spins up our Serve-backed Gradio demo that you can access on\\nlocalhost:8000 . Y ou can upload or drag and drop images into the respective input\\nfield and request predictions that you see in the output field of the app.7\\nIt’s important to note that this example is really just a thin wrapper around Gradio\\nand would work with any other Gradio app of your choice. In fact, if you called\\ndemo.launch()  instead of defining the Serve app in your script, you could simply\\nlaunch this as a regular Gradio app with python gradio_demo.py .\\nThe other noteworthy detail that’s easy to overlook is that we fed our predictor  a\\nNumPy array. If you check the definition of the data format we used for training,\\nyou’ll recall that predictor  is expected to work on Ray Dataset  instances. The\\npredictor  instance is also smart enough to infer that a single NumPy input must be\\nthe \"image\"  portion of the full input (we don’t need a \"label\"  to run inference).\\nTo wrap up this section, review Table 11-4 , which lists Serve’s current integrations.\\nTable 11-4. The Ray Serve ecosystem\\nLibrary Description\\nServing frameworks and applications FastAPI, Flask, Streamlit, Gradio\\nExplainability and observability Arize, Seldon Alibi, WhyLabs\\n224 | Chapter 11: Ray’s Ecosystem and Beyond\\n\\n8In this chapter we’re going to assume that you know about these different tools from the ecosystem. If you\\ndon’t, for this section it’s enough to understand that Snowflake is a database solution that you might want to\\nintegrate with, JAX is an ML framework, and Neptune can be used for experiment tracking.\\n9To be precise, you have to define a Backend  for JAX, together with a BackendConfig . Y our DataParallel\\nTrainer  then has to be initialized with this backend and your training loop.Building Custom Integrations\\nBefore explaining the relationship of Ray with other complex software frameworks\\nin a bit more detail, let’s talk about how to build your own integrations for Ray AIR.\\nSince AIR is designed for extensibility, you can find suitable interfaces for all the tasks\\nyou want to build custom integrations for.\\nFor example, let’s say you want to read data from Snowflake, train a JAX model  on it,\\nand log your tuning results to Neptune.8 At the time of this writing, there are no such\\nintegrations available, but it’s likely this will change in the future. We didn’t pick these\\nintegrations (Snowflake, JAX, Neptune) to showcase any preference; they just happen\\nto be interesting tools from the ecosystem. In any case, it’s worth knowing how to\\nbuild such integrations.\\nTo load data from Snowflake into a Ray Dataset, you have to create a new Data\\nsource . Y ou define a Datasource  by specifying how to set it up (create_reader ),\\nhow to write to the source ( do_write ), and what happens on successful and failed\\nwrite attempts ( on_write_complete  and on_write_failed ). Given a concrete Snow\\nflakeDatasource  implementation, you could then read your data into a Ray Dataset :\\nfrom ray.data  import read_datasource , datasource\\nclass SnowflakeDatasource (datasource .Datasource ):\\n    pass\\ndataset = read_datasource (SnowflakeDatasource (), ...)\\nNext, let’s say you have an interesting JAX model that you want to scale out using Ray\\nTrain’s capabilities. Specifically, let’s assume you want to run data-parallel training of\\nthe model, that is, to train this single model on several data shards in parallel.  For this\\npurpose, Ray comes with a so-called DataParallelTrainer . To define one, you have\\nto create a train_loop_per_worker  for your training framework and define how JAX\\nshould be handled by Train internally.9 With a JaxTrainer  implementation, you can\\nleverage the same Trainer  interface that we’ve used in all AIR examples:\\nfrom ray.train.data_parallel_trainer  import DataParallelTrainer\\nclass JaxTrainer (DataParallelTrainer ):\\nA Growing Ecosystem | 225\\n\\n    pass\\ntrainer = JaxTrainer (\\n    ...,\\n    scaling_config =ScalingConfig (...),\\n    datasets =dict(train=dataset),\\n)\\nFinally, to use  Neptune for logging and visualizing your Tune Trials, you can define  a\\nLoggerCallback  that gets passed into the run configuration of your Tuner. To define\\none, you need to specify how to create the logger ( setup ), what’s supposed to happen\\nat the beginning and end of a trials ( log_trial_start  and log_trial_end ), and\\nhow to log your results ( log_trial_result ). If you’ve implemented such a class, for\\nexample, NeptuneCallback , you can use it the same way we used the MLflowLogger\\nCallback  in “Model Training” on page 218 :\\nfrom ray.tune  import logger, tuner\\nfrom ray.air.config  import RunConfig\\nclass NeptuneCallback (logger.LoggerCallback ):\\n    pass\\ntuner = tuner.Tuner(\\n    trainer,\\n    run_config =RunConfig (callbacks =[NeptuneCallback ()])\\n)\\nWhile building integrations is not always as difficult as initially perceived, third-party\\nsoftware is a moving target and maintaining integrations can be challenging. Still,\\nyou are now aware of three of the most common integration scenarios for new\\nAIR components, and maybe you feel inclined to work on a community-sponsored\\nintegration of your favorite tool.\\nAn Overview of Ray’s Integrations\\nLet’s summarize all the integrations mentioned in this chapter (and throughout the\\nbook) in one concise diagram. In Figure 11-1  we list all integrations available at the\\ntime of writing.\\n226 | Chapter 11: Ray’s Ecosystem and Beyond\\n\\nFigure 11-1. The Ray AIR ecosystem summarized\\nRay and Other Systems\\nWe’ve not made any direct comparisons with other systems up to this point, for the\\nsimple reason that it makes little sense to compare Ray to something if you don’t\\nhave a good grasp of what Ray is yet. As Ray is quite flexible and comes with a lot\\nof components, it can be compared to different types of tools in the broader ML\\necosystem.\\nLet’s start with a comparison of the more obvious candidates, namely, Python-based\\nframeworks for cluster computing.\\nDistributed Python Frameworks\\nIf you consider frameworks for distributed computing that offer full Python support\\nand don’t lock you into any cloud offering, the current “big three” are Dask, Spark,\\nand Ray. While there are certain technical and context-dependent performance dif‐\\nferences between these frameworks, it’s best to compare them in terms of the work‐\\nloads you want to run on them. Table 11-5  compares the most common workload\\ntypes.\\nRay and Other Systems | 227\\n\\n10This represents a clear trade-off, as framework-specific tools are highly customized and offer many benefits.\\n11Anyscale itself provides a managed service with enterprise features to support building ML applications on\\ntop of Ray.Table 11-5. Comparing workload type support of Ray, Dask, and Spark\\nWorkload type Dask Spark Ray\\nStructured data\\nprocessingFirst-class support First-class support Supported via Ray Datasets and integrations,\\nbut not first  class\\nLow-level parallelism First-class support via tasks None First-class support via tasks and actors\\nDeep learning\\nworkloadsSupported, but not first\\nclassSupported, but not\\nfirst  classFirst-class support via several ML libraries\\nRay AIR and the Broader ML Ecosystem\\nRay AIR focuses primarily on AI compute , for instance by providing any kind of\\ndistributed training via Ray Train, but it’s not built to cover every aspect of an AI\\nworkload.  For instance, AIR chooses to integrate with tracking and monitoring tools\\nfor ML experiments, as well as with data storage solutions, rather than providing\\nnative solutions. Table 11-6  identifies the ecosystem’s complementary components.\\nTable 11-6. Complementary ecosystem components\\nCategory Examples\\nML tracking and observability MLflow,  Weights & Biases, Arize, etc.\\nTraining frameworks PyTorch, TensorFlow, Lightning, JAX, etc.\\nML feature stores Feast, Tecton, etc.\\nOn the other side of the spectrum, you can find categories of tools for which Ray AIR\\ncan be considered an alternative. For instance, there are many framework-specific\\ntoolkits such as TorchX or TFX that tie in tightly with their respective frameworks. In\\ncontrast, AIR is framework-agnostic, thereby preventing vendor lock-in, and offers\\nsimilar tooling.10\\nIt’s also interesting to briefly touch on how Ray AIR compares to specific cloud offer‐\\nings. Some major cloud services offer comprehensive toolkits to tackle ML workloads\\nin Python. To name just one, AWS Sagemaker is a great all-in-one package that\\nallows you to connect well with your AWS ecosystem. AIR does not aim to replace\\ntools like SageMaker. Instead, it aims to provide alternatives for compute-intensive\\ncomponents like training, evaluation, and serving.11\\nAIR also represents a valid alternative to ML workflow frameworks such as KubeFlow\\nor Flyte. In contrast to many container-based solutions, AIR offers an intuitive,\\n228 | Chapter 11: Ray’s Ecosystem and Beyond\\n\\n12We’ll give you a rough sketch for how to do this in the next section.\\n13Ray has a library called Ray Workflows , which is currently in alpha. Compared to tools such as AirFlow,\\nWorkflows is more low-level but allows you to run durable application workflows natively on Ray. Y ou can\\nfind more information about Workflows in the Ray documentation .high-level Python API and offers native support for distributed data. Table 11-7\\nsummarizes these alternatives.\\nTable 11-7. Alternative ecosystem components\\nCategory Examples\\nFramework-specific  toolkits TorchX, TFX, etc.\\nML workflow  frameworks KubeFlow, Flyte, FBLearner FLow\\nSometimes the situation is not as clear-cut, and Ray AIR can be seen or used as both\\nan alternative or a complementary component in the ML ecosystem.\\nFor instance, as open source systems, Ray and AIR in particular can be used within\\nhosted ML platforms such as SageMaker, but you can also build your own ML\\nPlatforms with it.12 Also, as mentioned, AIR can’t always compete with dedicated big\\ndata processing systems like Spark or Dask, but often Ray Datasets can be enough to\\nsuit your processing needs.\\nAs we mentioned in Chapter 10 , it is central to AIR’s design philosophy to have\\nthe ability to express your ML workloads in a single script and execute it on Ray\\nas a single distributed system. Since Ray handles all the task placement and execu‐\\ntion on your cluster for you under the hood, there’s usually no need to explicitly\\norchestrate your workloads (or stitch together many complex distributed systems).\\nOf course, this philosophy should not be taken too literally—sometimes you need\\nmultiple systems or to split up tasks into several stages. On the other hand, dedicated\\nworkflow orchestration tools like Argo or AirFlow can be very useful when used in\\na complementary fashion.13 For instance, you might want to run Ray as a step in the\\nLightning MLOps framework. Table 11-8  provides an overview of components that\\ncan be used alongside AIR or for which AIR can be an alternative.\\nTable 11-8. Ecosystem components that AIR can complement or substitute\\nCategory Examples\\nML platforms SageMaker, Azure ML, Vertex AI, Databricks\\nData processing systems Spark, Dask\\nWorkflow  orchestrators Argo, AirFlow, Metaflow\\nMLOps frameworks ZenML, Lightning\\nRay and Other Systems | 229\\n\\n14We use the term ML platform  in the broadest sense possible, namely, to signify any system that is responsible\\nfor running end-to-end ML workloads.\\n15Orchestration of task graphs can be handled entirely within Ray AIR. External workflow orchestrators will\\nintegrate nicely but are needed only if running non-Ray steps.\\n16The Ray team has built a demo  of a reference architecture that showcases the Feast integration.In case you already have an ML platform, such as Vertex or SageMaker, you can\\nuse any subset of Ray AIR to augment your system.14 In other words, AIR can\\ncomplement existing ML platforms by integrating with existing pipeline or workflow\\norchestrators, storage, and tracking services, without requiring a replacement of your\\nentire ML platform.\\nHow to Integrate AIR into Your ML Platform\\nNow that you have a deeper understanding of the relationship of Ray, and AIR in\\nparticular, to other ecosystem components, let’s summarize what it takes to build your\\nown ML platform and integrate Ray with other ecosystem components.\\nThe core of your ML system build with AIR consists of a set of Ray Clusters, each\\nresponsible for different jobs. For instance, one cluster might run preprocessing, train\\na PyTorch model, and run inference; another one might simply pick up previously\\ntrained models for batch inference and model serving, and so on. Y ou can leverage\\nthe Ray Autoscaler to fulfill your scaling needs and could deploy the whole system\\non Kubernetes with KubeRay. Y ou can then augment this core system with other\\ncomponents as you see fit, for example:\\n•Y ou might want to add other compute steps to your setup, such as running•\\ndata-intensive preprocessing tasks with Spark.\\n•Y ou can use a workflow orchestrator such as AirFlow, Oozie, or SageMaker•\\nPipelines to schedule and create your Ray Clusters and run Ray AIR apps and\\nservices. Each AIR app can be part of a larger orchestrated workflow, for instance\\nby tying into a Spark ETL job from the first bullet point.15\\n•Y ou can also create your Ray AIR clusters for interactive use with Jupyter note‐•\\nbooks, for instance hosted by Google Colab or Databricks Notebooks.\\n•If you need access to a feature store such as Feast or Tecton, Ray Train, Datasets,•\\nand Serve have an integration for such tools.16\\n•For experiment tracking or metric stores, Ray Train and Tune provide integration•\\nwith tools such as MLflow and Weights & Biases.\\n•Y ou can also retrieve and store your data and models from external storage•\\nsolutions like S3, as shown.\\nFigure 11-2  puts all the pieces together in one concise diagram.\\n230 | Chapter 11: Ray’s Ecosystem and Beyond\\n\\nFigure 11-2. Building your own ML platform with Ray’s AI Runtime and other compo‐\\nnents from the ML ecosystem\\nWhere to Go from Here?\\nWe’ve come a long way from an overview of Ray in Chapter 1  to the discussion of\\nits ecosystem in this one. But as this is an introductory book, we’ve just touched the\\nproverbial tip of the iceberg of Ray’s capabilities. While you should now have a good\\ngrasp of the basics of Ray Core and how Ray Clusters work and know when to use\\nAIR and its constituent libraries Datasets, Train, RLlib, Tune and Serve, there’s still so\\nmuch more to learn about each aspect.\\nTo start with, the extensive user guides of Ray Core  will give you a much deeper\\nunderstanding about Ray tasks, actors, and objects; their placement on cluster\\nnodes; and how to handle dependencies for your applications. In particular, you\\nwill find interesting patterns and anti-patterns to design your Ray Core programs\\nwell and avoid common pitfalls. To learn more about the inner workings of Ray, we\\nrecommend reading some of the advanced papers on Ray, such as its architecture\\nwhitepaper .\\nAn interesting topic we skipped entirely is Ray’s tooling around observability . The\\nofficial Ray Observability documentation  is a good starting point for this topic. Y ou\\ncan learn how to debug and profile your Ray applications there, log information\\nfrom your Ray Clusters, monitor their behavior, and export important metrics there.\\nWhere to Go from Here? | 231\\n\\n17At the time of this writing, the dashboard is being overhauled. The look and functionality might change\\ndrastically, so we didn’t include descriptions or screenshots here.Y ou will also find an introduction to the Ray dashboard  there,17 which can help you\\nunderstand your Ray programs.\\nThe focus of this book has been to introduce the core ideas of Ray to ML practi‐\\ntioners and give you practical starting points to tackle your workloads with Ray.\\nHowever, a topic that deserves more attention than the one chapter we could include\\nin this book is how to build, scale, and maintain Ray Clusters. Currently, the best\\nintroduction to advanced Ray Cluster topics is the Ray documentation . There you\\ncan learn how to deploy clusters on all major cloud providers, how to scale clusters\\non Kubernetes, and how to submit Ray jobs to a cluster, in much more detail than we\\ncould cover here.\\nIn this chapter we could mention the majority of Ray integrations only in passing.\\nIf you want to learn more about Ray’s third-party integrations, Ray’s Ecosystem page\\nis a good starting point. Also, it’s important to mention that more Ray libraries  are\\navailable that simply didn’t make the cut for this book, like Ray Workflows and a\\ndistributed, drop-in replacement for Python’s multiprocessing  library.\\nLastly, if you’re interested in becoming part of the Ray community, there are many\\ngood ways of doing so. Y ou can join the Ray Slack  to get in touch with Ray developers\\nand other community members, or join Ray’s discussion forum  to get your questions\\nanswered. If you want to help develop Ray, whether contributing to the documenta‐\\ntion, adding a new use case, or helping the open source community with new features\\nor bug fixes, you should check out the official contributor guide to Ray .\\nSummary\\nIn this chapter you learned more about Ray’s ecosystem, as seen from its AI Runtime.\\nY ou’ve seen the full extent of available integrations of Ray AIR libraries and an exam‐\\nple of training and serving an ML model using three different integrations: PyTorch\\nfor data loading and training, MLflow for logging, and Gradio for serving your\\nmodel. Y ou should now be able to go out there and run your own AIR experiments,\\ntogether with all the tools you’re already using or intend to use in the future. We’ve\\nalso discussed Ray’s limits, how it compares to various related systems, and how you\\ncan use Ray with other tools to augment or build out your own ML platforms.\\n232 | Chapter 11: Ray’s Ecosystem and Beyond\\n\\nThis wraps up this chapter and the book. We hope it piqued your interest in Ray\\nand helped you start your journey with it. The introduction of AIR brought many\\nnew features to the Ray ecosystem, and there’s certainly more on the road map to\\nbe excited about. There’s no doubt that you can now dive into any advanced Ray\\nmaterial—maybe you already have an idea about building your first own Ray app.\\nSummary | 233\\n\\n\\n\\nIndex\\nSymbols\\n@ray.remote decorator, 27, 36\\n@serve.batch decorator, 165\\n@serve.deployment decorator, 19, 161\\ntuning replicas and resource allocation for a\\ndeployment, 164\\nA\\nabstractions\\nlaw of leaky abstractions, 2\\nprovided by Ray, 4\\naction distribution, 78, 81\\naction space (RL), 66, 87\\ndefining for policy server in RLlib environ‐\\nment, 92\\nparametric action spaces in RLlib, 99\\nactions (RL), 66\\ncomputing in Python RLlib API, 78\\npassed to steps in multi-agent environment,\\n88\\nprobabilities of taking each action, 78\\nsimplifying assumptions about, 66\\nactors, 33\\ncontroller actor for Ray Serve deployments,\\n161\\nconverting Simulation class to actor, 62\\nGCS storing location of, 39\\nmapping data with, support by Datasets, 127\\nRay AIR usage combined with tasks for\\nadvanced composite workloads, 208\\nRay Datasets, using in distributed batch\\ninference, 147\\nin Ray Serve deployment, 160\\nRay patterns and anti-patterns for, 47stateful, accessing data from stateless tasks\\nin composite workloads, 212\\nuse for transformations with state, 210\\nadapter function, 204\\nAdvancedEnv, 96\\nagents (RL), 66, 69\\nworking with multiple agents, 85-90\\nmapping agents to policies, 86, 89\\naggregations, support by Ray Datasets, 122, 125\\nAI (see artificial intelligence)\\nAI Runtime (see Ray AIR)\\n“ AI and Compute”, 3\\nAirflow, 137\\nAlgorithm class, 75, 82\\naccessing all Algorithm instances on work‐\\ners, 79\\nproviding with a curriculum in RLlib, 95\\ntraining in multi-agent environment, 89\\nAlgorithmConfig class, 75, 82\\nmethods for categories of common algo‐\\nrithm properties, 82\\nAlphaFold, 3\\nAmazon Web Services (AWS), 192\\nanalyses (Tune), 104, 106, 107\\ngetting in training of Keras model, 118\\nApache Airflow, 137\\nApache Arrow, 10\\ndistributed, use in Ray Datasets, 122\\nPlasma project, 37\\nApache Hadoop, 121\\nApache Spark, 121, 227\\nAPI Reference for RLlib algorithms, 83\\narchitecture (Ray)\\noverview of components, 40\\n235\\n\\nwhitepaper, 38\\nArrow for Python, installing, 10\\nartificial intelligence (AI)\\nRay AIR, 195\\nRay AIR focus on AI compute, 228\\nrecent developments in, 3\\ntypes of AI workloads AIR enables Ray\\nClusters to run, 209\\nasynchronous execution, 28\\nrunning dependent tasks asynchronously\\nand in parallel, 33\\nasyncio capabilities (Python), 165, 166\\nAtari environments (gym), 73\\nautoscaling\\ncluster, 194\\nof Ray AIR workloads, 213\\nof Ray Serve replicas, 165\\nsupport by Ray Clusters, 5\\nawait syntax (Python), 166\\nAWS (Amazon Web Services), 192\\nAzure, 193\\nB\\nbackends, 20\\nBaseEnv class, 85\\nbase_model, 79\\nbatch inference, 208\\ndistributed, using Ray Train, 147\\nexample in Ray documentation, 124\\nusing Datasets, 127\\nbatch normalization layers, 143\\nbatch predictors, 203, 208\\nbatch_timeout_wait, 165\\nBayesian optimization, 108\\nbayesian-optimization library, 108\\nBayesOptSearch, 109\\nbehavior cloning, 98\\nbias, 110, 114\\nbig data processing tools, 207\\nbig data training, 208\\n.bind API, instantiating copy of deployment,\\n162\\nbinding multiple deployments, 167\\nbinpacking algorithm, 194\\nblocking, 30\\nDatasets operations, 127\\nblocks\\nblocks_per_window parameter, ds.window\\nfunction, 129in Datasets, 122\\nand repartitioning, 125\\nbroadcasting, 168\\nin NLP-powered API example, 171\\nC\\ncallbacks\\nconfiguring for Ray Tune, 111\\nLoggerCallback to pass to Tuner run config,\\n226\\nMLFlowLoggerCallback, 221\\nparticularly useful methods on, 113\\nTuneReportCallback as custom Keras call‐\\nback, 117\\nusing to monitor training in Ray Train, 156\\ncartpole-ppo tuned example, 14\\nCartPole-v1 environment, 14\\ncategorical variables, 116\\nchat bots, 157\\ncheckpoints\\nCheckpoint class provided by Ray Train,\\n141\\ncheckpoint property of Ray Train Trainers,\\n148\\ncreating from existing, framework-specific\\nmodel, 202\\ncreating RL algorithm checkpoints, 77\\ncreation by Ray Tune, 113\\ncreation by Ray Tune for RLlib, 74\\ncreation by rllib command, 74\\nevaluating trained algorithm from, 15\\nexporting trained model as Checkpoint in\\nRay Train, 147\\ngenerated by Ray AIR Trainers or Tuners,\\n202\\nRay AIR\\ncreating batch predictor from, 203\\ndeploying PredictorDeployment, 204\\nTuners and checkpoints, 201\\nreporting model checkpoint, 146\\nstateful computations relying on\\ncheckpoint-based fault tolerance, 212\\nCIFAR-10 dataset, 216\\nClarke’s third law, 32\\nclasses\\nconverting Python classes to actors, 34\\ntasks and actors working as distributed ver‐\\nsions of, 24\\nclassifier, training copies in parallel, 130-133\\n236 | Index\\n\\nclassify method, 166\\nclassify_batched method, 165\\nCLI (command-line interface)\\nRLlib CLI, running, 73-74\\nusing Ray Cluster Launcher CLI to deploy\\ncluster, 191\\nclient-side batching, 165\\nclients\\ndefining policy client in RLlib environment,\\n92\\nPolicyClient, 90\\nRay Client, 185\\ncloud computing, 179\\nworking with cloud clusters, 192-194\\nAWS, 192\\nother cloud providers, 193\\nclusters (Ray), 6, 179\\n(see also Ray Clusters)\\nbasic components, 6\\ndefining ScalingConfig for, 152\\nhead node processes for cluster manage‐\\nment, 39\\nstarting a local cluster, 24\\nCodex, 3\\ncolumnar format (Arrow), 125\\ncommunication in Ray Clusters, 39\\ncomposite workloads, 207\\nexecution of, 211\\nfault-tolerance strategies, 212\\nRay AIR usage of actors and tasks for, 208\\nstateful actors accessing data from stateless\\ntasks, 212\\ncompute_actions method, 78\\ncompute_single_action method, 78\\ncomputing over Datasets, 126\\nconcurrent trials (Ray Tune), 111\\nconditional logic, 169\\nconfig argument (tune.run), 115\\ncontainer images, 189\\ncontainers, 186\\nenvironment variables, 189\\nKubeRay operations on, 182\\nspecifying resources for, 189\\ncontinuous action space (RL), 66\\ncontinuous parameters, 102\\ncontroller actor, 161\\ncore layer (Ray), 5\\n(see also Ray Core)\\nCPUsallocating two per replica in Ray Serve, 164\\nspecifying for machines in Ray Cluster, 180\\ncpu_intensive_preprocessing function, 128\\nCSV files\\nreading from S3 bucket into columnar data‐\\nset, 198\\nwriting to/reading back from in Ray Data‐\\nsets, 123\\ncurriculum learning, applying with RLlib, 95-97\\nCurriculumEnv, 96\\nD\\nDask, 20, 227\\nbuilt-in support for Python datetime utilit‐\\nies, 143\\nDask on Ray, 216\\nexample, 135\\nusing to train PyTorch neural network,\\n142\\ndata formats\\nflexibility within Datasets, 125\\nserialization formats supported by Datasets,\\n124\\ndata parallelism, 140\\ndata processing, 9, 121\\n(see also Ray Data; Ray Datasets)\\nexternal library integrations with Ray Data‐\\nsets, 134\\nSpark and Dask engines for, 20\\nusing Ray Datasets library, 10\\ndata science, 8\\nRay AIR and data science workflow, 8\\ndata scientists, 1\\nuses of Ray AIR, 196\\ndata shards, 130\\nget_data_shard utility, 146\\niterating over with iter_torch_batches, 146\\ndata-parallel training, 144\\nDataFrames, 126\\nconverting predictor service payload to, 204\\nDask, 135\\ndf.compute calls, 136\\nDask on Ray, 142\\nexternal processing systems for, 134\\nDataParallel (PyTorch), 144\\nDataParallelTrainer, 225\\nDatasetPipeline, 12\\ncreation using ds.repeat function, 130, 132\\nDatasets conversion to, 129\\nIndex | 237\\n\\nDatasets (Ray), 10, 121\\n(see also Ray Datasets)\\ncontents of a Dataset, 122\\ncreating a Dataset, 123\\ntransforming datasets, 11\\ndatasets argument (XGBoostTrainer in AIR),\\n199\\ndatasets dictionary (Ray Train Trainers), 147\\ndeep learning\\ndefining model for, 143\\ntraining in, 127\\ndeep learning frameworks, RLlib working with,\\n70\\nDeep Q-Learning, 61, 78\\nDeep Q-Networks (DQN), 61\\nusing DQNConfig to define DQN algo‐\\nrithm, 73\\nDeepmind\\'s AlphaFold, 3\\nDense layers, 80, 117\\ndependencies\\nhandling for tasks, 31\\ninstalling for Ray, 7\\nownership versus, 38\\nresolution by Raylet scheduler, 37\\nresolution for tasks, 40\\ntask, dynamic execution dealing with, 4\\ndeployment.options API, 164\\ndeployments\\nRay Clusters, 179\\nRay Clusters on Kubernetes, 182-190\\nin Ray AIR, 204\\nwith PredictorDeployment, 205\\nin Ray Serve, 160\\nbinding multiple deployments, 167\\nconverting Python class to deployment,\\n161\\ntuning replicas and resources allocated\\nto, 164\\ndesign philosophy (Ray AIR), 197\\ndesign principles (Ray), 4\\ndeterministic environment (in RL), 67\\ndictionaries\\nconfig dictionary to pass to trainer as\\ntrain_loop_config, 151\\ndatasets dictionary, Ray Train Trainers, 147\\nPython, creating Dataset with schema from,\\n126\\ndifficulty, 96\\nsetting task difficulty, 96discount_factor, 102\\ndiscrete action space (RL), 66, 72\\nDiscrete class, 51\\ndistributed batch inference (Ray Train), 147\\ndistributed computing\\ndifficulties of, 3\\nRay AI providing universal interface for, 25\\nRay as glue code for distributed workloads,\\n4\\nRay Core capabilities, 23\\nRay framework for, 6\\ndistributed model training, 207\\nbasics, 139-141\\ntraining ML model using Ray Datasets,\\n130-133\\ndistributed object transfer, 39\\ndistributed Python frameworks, 227\\ndistributed scheduler, 39\\ndones, 54\\ngame considered done, 53\\nin RL, 66\\ngym.Env having done condition, 72\\nin multi-agent environment, 88\\nis_done helper to work with multiple\\nagents, 87\\nDQN (see Deep Q-Networks)\\nDQNConfig object, 75\\nmulti_agent method, 89\\ndriver, 6, 39\\ndriver deployment, 167\\ndefining control flow logic for, 174\\nDropout rate, 117, 118\\nds.repeat function, 130, 132\\ndynamic execution, 4\\nE\\necosystem (Ray), 5, 20, 215-233\\nbuilding custom integrations, 225-226\\ndata loading and preprocessing, 216\\ndistributed Python frameworks, 227\\ngrowth of, 216\\nintegrating Ray AIR into ML platforms, 230\\nmodel serving, 222-225\\nmodel training, 218-222\\nRay AIR and broader ML ecosystem, 228\\nRay and other systems, 227\\nwhere to go from here, 231\\nensembling, 168\\nentity recognition model, 173\\n238 | Index\\n\\nEnvironment class, 52\\nmethods implemented for, 53\\nusing to play 2D maze game, 54\\nenvironment method (AlgorithmConfig), 82\\nenvironments (in RL), 66\\nbuilding a gym environment, 71\\ndeterministic environment, 67\\nenvironment configuration for RLlib experi‐\\nments, 84\\nspecifying for DQNConfig using Python\\nRLlib API, 75\\nusing gym environment with RLlib, 73-74\\nworking with RLlib environments, 85-93\\noverview of RLlib environments, 85\\npolicy servers and clients, 90-93\\nusing multiple agents, 86-90\\nepisodes (in RL), 61, 66\\nrandomly rolled out in multi-agent environ‐\\nment, 89\\nepochs\\nnumber of, 146\\ntrain_one_epoch function, 149\\nworkers in, core logic needed to train on\\nbatch of data, 145\\nestimating arrival times, 158\\nevaluate command (rllib), 15, 74\\nevaluating RLlib models in Python API, 77\\nevaluation results for trained RL algorithm, 15\\nexample command (rllib), 15\\nexperiences (in RL), 66\\nExperimentAnalysis object, 107\\nexploiting or exploring environment in RL, 66,\\n78\\nexploration method (AlgorithmConfig), 82\\nExternalEnv, 85\\nF\\nfailures\\nin distributed computing, 5\\nhandling by Ray Datasets, 123\\nRay AIR failure model, 212\\nFastAPI framework, 163\\nparsing input_text query parameter, 163\\nRay Serve\\'s FastAPI for input parsing and\\noutput schema, 171\\nfault tolerance and ownership, 37\\nfeature engineering, 9\\nfeaturizationbuilding features using load-dataset func‐\\ntion in Ray Train, 142\\nin distributed batch inference, 147\\nfetch_wikipedia_page driver logic, 175\\nfilter function, 11\\nfiltering data, 122\\nfilter operations in Ray Datasets, 124\\nfit method (Trainer), 148, 151\\nfitting to training data, AIR preprocessor, 198\\nflat_map function, 11\\nflexibility (Ray), 4, 46\\nflexible distributed Python for machine learn‐\\ning, 2\\nfractional resources, 111, 164\\nframeworks\\nframework specification for RLlib algo‐\\nrithms, 79\\nthird-party training frameworks, 141\\nfunctional programming in Ray Datasets, 11\\nfunctions\\nconverting Python function to Ray task, 27\\ntasks and actions as distributed versions of,\\n24\\nfutures, 28, 31\\nresolved by ray.get in follow_up_task, 32\\nG\\nGCS (Global Control Service), 39\\nGCS server (Ray)\\nfailures of, 212\\nprinting out IP address of, 180\\nUnable to connect to GCS at …, 181\\nget function, 29, 35\\nget_best_config function, 106\\nGitHub repository for this book, xviii\\nGlobal Control Service (GCS), 39\\nGlobal Interpreter Lock (GIL), 27\\nGoogle Cloud, 193\\nGPT-2 model, 18\\nGPUs\\nallocating to Ray Serve deployments, 164\\nexpense of running online inference serv‐\\nices, 159\\nspecifying for machines in Ray Cluster, 180\\nuse_gpu flag, 151\\ngpu_intensive_inference, 128\\ngradient boosted decision tree frameworks, 148\\nGradio, 222-225\\ngreedily choosing an action, 79\\nIndex | 239\\n\\ngrid search, 17, 108\\ngroupby, 122, 125\\ngRPC, 39\\ngym library, 71\\nbuilding a gym environment, 71\\ninstalling, 13\\ngym.Env, 73\\nVectorEnv wrapper for, 85\\nGymEnvironment class\\ndefining multi-agent version of, 86\\nimplementing a gym.Env, 72\\nusing with RLlib, 73\\ngymnasium library, 73\\nH\\nHadoop, 121\\nhead (in deep learning), 80\\nhead node, 6, 39, 180\\naddress of, NAT and, 181\\nconnecting every other node in Ray Cluster\\nto, 180\\nhead pod, 183, 184\\nheterogeneity (Ray), 4\\nhidden units, 118\\nhierarchy of agents, 85\\nhigh availability mode (GCS), 212\\nHPO (see hyperparameter optimization)\\nHPO tools, Ray Tune supporting algorithms\\nfrom, 20\\nHTTP API and driver logic, defining for NLP-\\npowered API, 173\\nHTTP endpoint wrapping an ML model, defin‐\\ning, 161-163\\nHTTP requests, defining logic to handle, 162\\nHTTP , inference service queryable over, 204\\nHugging Face models in Python, 18\\nHugging Face pipeline supporting vectorized\\ninference, 165\\nHugging Face Transformers library, 161, 172\\nHyperopt and Optuna integrations (Ray Tune),\\n108\\nHyperOptSearch algorithm, 118\\nhyperparameter optimization (HPO), 16,\\n101-119\\nintroduction to Ray Tune, 105-115\\nmachine learning with Ray Tune, 115-119\\nRay AIR Trainers integration with Ray Tune\\nfor, 200\\nRay Train integration with Ray Tune, 154tuning hyperparameters, 102-105\\nbuilding random search example with\\nRay, 102-104\\ndifficulty of, 104\\nhyperparameter tuning, 9\\nusing Ray Tune, 16\\nhyperparameters, 102\\ndepending on other hyperparameters, 114\\nspecifying ranges for XGBoostTrainer, 154\\nTrainingWorker, 132\\nI\\nimage processing, 167\\nimitation learning, 98\\ninference, 127\\nmultimodel inference graphs, 166-170\\nvectorized, 128\\nHugging Face pipeline supporting, 165\\ninit function, 7\\ninstalling Ray, 7\\nintegrations, custom, 225-226\\nintegrations, Ray, overview of, 226-231\\nintermediate scores, 109\\nipython interpreter, 7\\nitems, retrieving from database, 25\\niter_torch_batches function, 146\\nJ\\nJAX, 225\\njobs, 6\\nJSON-serializable output (HTTP request), 162\\nJsonLoggerCallback, 156\\nJupyter notebooks, 7\\nK\\nKeras, 20, 79\\ntuning Keras models in Ray Tune, 116\\nkey-value pairs produced in map phase of Map‐\\nReduce, 41\\nkey-value store (GCS), 39\\nkubectl, running Ray programs with, 184\\nKubeRay project, 182\\nKubernetes, 8, 41, 176\\nRay Cluster deployment on, 182-190\\nconfiguring KubeRay, 187-188\\nconfiguring logging for KubeRay, 189\\ninteracting with the KubeRay cluster,\\n184-186\\n240 | Index\\n\\nKubeRay operator, 182\\nsetting up first KubeRay cluster, 183\\nL\\nlabel_column argument (XGBoostTrainer in\\nAIR), 199\\nlambda functions, 126\\nlatency\\nML services online and applications, 158\\nonline inference and, 157\\nlaw of leaky abstractions, 2\\nlayers in Ray, 5\\nleasing worker processes to task owners, 39\\nlibraries (Ray), 5\\ndata science libraries, 8\\ndedicated libraries for machine learning\\nsteps, 9\\nmodel training, 12\\nLightGBM, 148\\nLightGBMTrainer, 148\\nlineage reconstruction, 212\\nlist comprehensions, 26\\nloading data, 216\\nin distributed batch inference, 147\\nloading model in Ray Train, 142\\ntraining and validation data for training\\nworkers, 146\\nload_dataset function (Ray Train), 142, 146\\nlocal clusters, 6\\nLoggerCallback interface, 156\\nlogging\\nconfiguring for KubeRay, 189\\ntrial results to MLFlow, 216\\nusing Neptune for, 226\\nloss, 146\\nM\\nmachine learning (ML)\\nbasic pipelines, Ray Train components used\\nin, 141\\nbroadcasting to multiple models in parallel,\\n168\\nbuilding ML pipeline using Ray Datasets,\\n136-138\\nconditional logic for control flow, 169\\ndata science processes involved in, 9\\ndistributed training of ML models, 130\\nflexible distributed Python for, 2\\nintegrating Ray AIR into ML platforms, 230models are compute intensive, 158\\nmodels not useful in isolation, 159\\nmultiple models in NLP-powered API\\nexample, 170\\nonline inference interacting with ML mod‐\\nels, 157\\nperformance for, Ray Datasets, 121\\nRay AIR and broader ML ecosystem, 228\\nRay AIR as umbrella for all other Ray ML\\nlibraries, 195\\nRay AIR uses by ML engineers, 196\\nrecent developments in, 3\\nreinforcement learning, 50\\ntackling workloads with single script run by\\nsingle system, 197\\ntraining-serving skew in deployments, 153\\nusing Ray Tune, 115-119\\ntuning Keras models, 116\\nusing RLlib with Tune, 115\\nmachine learning frameworks, Ray Train Train‐\\ners integration with, 148\\nmap function, 11\\nperforming custom transformations on\\nDatasets, 126\\nmapping, 122\\nDatasets support for, using Ray actors, 127\\nmapping model across whole dataset, 147\\nmapping batches, 217\\nMapReduce\\nexample using Ray, 41-47\\nmapping and shuffling document data,\\n43\\nreducing word count, 45\\nrunning MapReduce on distributed cor‐\\npus of documents, 42\\nMARL (multi-agent reinforcement learning)\\nproblem\\nRLlib support for, 90\\ntraining, 89\\nmax_batch_size, 165\\nmax_depth parameter (XGBoost model), 201\\nmaze problem, setting up, 50-55\\nmemory\\nAIR memory management, 211\\ndistributed, 39\\nefficient usage by Ray Datasets, 123\\nRaylet object store managing shared mem‐\\nory pool, 37\\nspecifying for trials in Ray Tune, 111\\nIndex | 241\\n\\nmetrics\\nconfiguring for report in Ray Tune, 111\\ngetting best hyperparameters found, 106\\noptimizing in BayesOptSearch, 109\\npassing RLlib metrics to Ray Tune, 115\\npassing to Ray Tune scheduler, 110\\nmigration fatigue, 196\\nmin, 125\\nMinMaxScaler, 154\\nML (see machine learning)\\nML platforms, 230\\nhosted, use of Ray and AIR in, 229\\nintegrating Ray AIR into, 230\\nMLFlow, 156, 221\\nMLFlowLogger, 216\\nMLFLowLoggerCallback, 156, 221\\nMNIST data, 116\\nmode\\ngetting best hyperparameters found, 106\\npassing to Ray Tune scheduler, 110\\nspecifying for trials in Ray Tune, 112\\nmodel parallelism, 140\\nmodel serving, 9, 222-225\\nusing Ray Serve, 18\\nmodel training, 9, 218-222\\ndistributed model training basics, 139-141\\nexample, training copies of a classifier in\\nparallel using Datasets, 130-133\\nparallelizing with Ray, 64\\nRay libraries for, 12\\nRay RLlib, 12\\ntraining reinforcement model, 59-62\\nmodel.state_value_head.summary method, 80\\nmodels\\naccessing state in Python RLlib API, 78\\ncheckpoints as Ray AIR native model\\nexchange, 202\\ncustomizing for RLlib experiments, 80\\nin Deep-Q learning used in DQN, 79\\ndefining deep learning model, 143\\ndistributed training of ML models, 130\\nNLP, 172\\nin Ray Train Checkpoints, 147\\nstate_action_table of policy, 66\\ntuning Keras models in Ray Tune, 116\\nMoore’s law, 3\\nMultiAgentEnv, 85\\ndefining with two agents, 86\\nmultimodel inference graphs, 166-170broadcasting pattern, 168\\nconditional logic pattern, 169\\ncore Ray Serve feature, binding multiple\\ndeployments, 167\\nin NLP-powered API example, 170\\npipelining pattern, 167\\nmultiple agents (RL), 66\\nmulti_agent method (AlgorithmConfig), 82\\nN\\nNAT (Network Address Translation), 181\\nnatural language processing (NLP), 170\\nnc tool, 181\\nNeptune, 225, 226\\nNetwork Address Translation (NAT), 181\\nneural networks, 78\\nin Deep Q-Networks (DQN), 61\\ndefining and training in Ray Train, 141\\nFarePredictor PyTorch network, 143\\nparallelizing computations to speed up\\ntraining, 140\\nNew Y ork City taxi trips, predicting big tips in\\n(example), 141\\nNLP (natural language processing), 170\\nNLP-powered API, building (example),\\n170-176\\narchitecture for NLP pipeline to summarize\\nWikipedia articles, 171\\nfetching content and preprocessing, 172\\nHTTP handling and driver logic, 173\\nNLP models, 172\\nputting it all together, 175-176\\nnmap tool, 181\\nnodes\\nchecking whether each port can be reached\\nfrom, 181\\nconnecting to head node in Ray Cluster, 180\\nstopping Ray processes on, 182\\nunable to access port and IP address speci‐\\nfied, 181\\nnonblocking calls, using Ray wait function for,\\n30\\nnumpy.square optimized implementation, 126\\nnum_gpus, 164\\nnum_replicas, 164, 173\\nO\\nobject references, 32\\nremote Ray tasks returning, 28\\n242 | Index\\n\\nobject store, 24\\ncomponent of Raylets, 36\\nputting database in, 30\\nputting policy into, 63\\nusing with put and get, 29\\nobjective, 103\\nconverting to Ray task, 103\\nstopping Ray Tune objective analysis, 114\\nobjective functions, 16, 103\\ndefining, 105\\ndefining to compute intermediate scores,\\n109\\nKeras objective function in Tune, 117\\nobjects\\ndistributed object transfer, 39\\nequality with tasks and actors in Ray Core,\\n35\\nRay tasks as primary means of creating, 28\\nsharing between driver and workers or\\nbetween workers, 29\\nspilling and recovery in Ray Datasets, 123\\nobservability, 231\\nobservation space (RL), 66, 87\\ndefining for policy server in RLlib environ‐\\nment, 92\\nin gym environment, 72\\nobservations (in RL), 52\\ncomputing actions for given observations,\\n78\\nin multi-agent environment, 88\\ntaken in by DQN model, 80\\ntransforming to form expected by model, 81\\noffline data\\nPython API for in RLlib, 99\\nworking with, 97\\noffline_data method (AlgorithmConfig), 82\\nonline inference, 157-159\\nbuilding services with Ray Serve, 160\\n(see also Ray Serve)\\ndifferences in serving ML models, 158-159\\ncompute intensive ML models, 158\\nML models not useful in isolation, 159\\npipeline in NLP-powered API example, 171\\nuse cases, 157\\nonline serving, 208\\nonline serving execution, 211\\nOpenAI\\nCodex, 3\\n‘ AI and Compute”, 3ownership, 37\\ndependencies versus, 38\\nownership table, 37\\nP\\nPandas DataFrames, 126\\nPandas on Ray, 20\\nparallel execution of dependent tasks, 33\\nparallelization\\ndata-parallel training, 144\\nparallelizing code, Ray Train, 141\\nparametric action spaces, 99\\nParquet data, 124, 126\\nDataset transformations on, 127\\npartitioning, 122\\nblocks and repartitioning in Datasets, 125\\npayload (predictor service), 204\\nperformance, measuring for Ray task, 28\\npipelined execution, 212\\npipelines\\nbuilding ML pipeline using Ray Datasets,\\n136-138\\nDataset, 127-130\\nDatasetPipeline, 12\\nonline inference, 171\\npipelining\\nmultimodel pattern in ML applications, 167\\nusing on Ray Datasets, 210\\nPlasma, 37\\npods\\nhead and worker pods on KubeRay cluster,\\n183\\ninteracting with KubeRay cluster head pod,\\n184\\nRay Cluster, configuring on KubeRay, 188\\nPodTemplate, 182\\npolicies (RL), 66\\naccessing state of in Python RLlib API, 78\\nin multi-agent environment\\nmapping agents to policies, 89\\nin multi-agent RL environments, 86\\nworking with policy servers and clients in\\nRLlib environment, 90-93\\ndefining a client, 92\\ndefining a server, 91\\nPolicy class, 56\\nreplacement in future RLlib release, 79\\nstate_action_table, 57\\nupdating values in state_action_table, 59\\nIndex | 243\\n\\nPolicyClient, 92\\nPolicyServerInput object, 91\\nPredictorDeployment class, 204\\npredictors, 141, 224\\nbatch, 203\\n.predict_pipelined function, 147\\nprepare_model function, 146\\npreprocessing, 216\\ncontent in NLP-powered API example, 172\\nin distributed batch inference, 147\\nusing load_dataset function in Ray Train,\\n142\\nusing Ray Train, 153\\npreprocessors\\nbuilt into Ray Train, 153\\nchoosing between for Ray Train integration\\nwith Ray Tune, 154\\nprovided by Ray Train, 141\\nRay AIR, 198\\ndifferent types available, 199\\nspecifying for XGBoostTrainer, 199\\nin Ray Train Checkpoints, 147\\nprobabilities of taking actions, 78\\nPrometheus, 182\\nProximal Policy Optimization (PPO) algo‐\\nrithm, 14\\nput function, 28, 35\\nplacing data into distributed object store, 29\\nPydantic, 173\\nPyGame, installing, 14\\nPython\\ndatetime utilities, Dask support for, 143\\ndistributed computing frameworks, 227\\nGlobal Interpreter Lock, 27\\nRay for data science community, 2\\nRLlib API (see RLlib Python API)\\nuse for data science, 1\\nversions and support for Ray, 7\\nZen of, 43\\nPython-first frameworks, 2\\nPyTorch, 70, 79\\nbacking Ray RLlib and Train libraries, 20\\nDataParallel, 144\\nloading and transforming dataset using, 216\\nloss_function and batch_loss, 146\\nmigrating existing model to Ray Train, 148,\\n150\\ntorchvision extension, 216training PyTorch neural network using\\nDask on Ray, 142\\nQ\\nQ-Learning algorithm, 59-61, 66\\nQ-values, 66, 78\\n(see also state-action values)\\ngetting for DQN models, 81\\nR\\nrandom number generator (RNG), 169\\nrandom.uniform sampler (numpy), 114\\nrandomly sampling hyperparameters, 102\\nRay\\nabout, 2\\narchitecture whitepaper, 28\\ndesign principles, 4\\necosystem, 20, 215\\n(see also ecosystem)\\ninstalling, 7\\norigins of, 2\\nrelating to other systems, 41\\nRay AIR (AI Runtime), 5, 144, 195-213\\nas umbrella for current Ray data science\\nlibraries, 10\\nand data science workflow, 8\\nDatasets and preprocessors, 198\\nextensibility, 196\\nkey AIR concepts by example, 197-207\\nbatch predictors, 203\\nfrom data loading to inference with AIR,\\n197\\nTrainers, 199\\nTuners and checkpoints, 201\\nRay ecosystem and beyond, 215-233\\nAIR and the broader ML ecosystem, 228\\nbuilding custom integrations, 225-226\\ndata loading and preprocessing, 216\\ndistributed Python frameworks, 227\\necosystem components AIR can comple‐\\nment or substitute, 229\\ngrowing ecosystem, 216\\nintegrating AIR into ML platforms, 230\\nmodel serving, 222-225\\nmodel training, 218-222\\nRay and other systems, 227\\nwhere to go from here, 231\\nthird-party integrations, 195\\nuses of, 195\\n244 | Index\\n\\nworkloads suited for, 207-213\\nAIR failure model, 212\\nAIR memory management, 211\\nautoscaling AIR workloads, 213\\nworkload execution, 209-211\\nRay Client, 7\\nusing to connect to KubeRay cluster, 185\\nRay Clusters, 6, 8, 179-194, 230\\ndeployment on Kubernetes, 182-190\\nconfiguring KubeRay, 187-188\\nconfiguring logging for KubeRay, 189\\ninteracting with the KubeRay cluster,\\n184-186\\nsetting up first KubeRay cluster, 183\\nmanually creating, 180-182\\ntypes of AI workloads AIR enables running\\non, 209\\nusing Ray Cluster Launcher, 190-192\\nconfiguring your Ray Cluster, 190\\ninteracting with a Ray Cluster, 191\\nusing Cluster Launcher CLI, 191\\nworking with cloud clusters, 192-194\\nAWS, 192\\nAzure, 193\\nGoogle Cloud, 193\\nRay Core, 8, 23-47\\nbuilding your first distributed application,\\n49-67\\nbuilding a simulation, 55-58\\nbuilding the app, 62-64\\nintroduction to reinforcement learning,\\n49-50\\nsetting up simple maze problem, 50-55\\nfirst example using Ray API, 25-35\\nfrom classes to actors, 33\\nfunctions and remote Ray tasks, 27-29\\nhandling task dependencies, 31\\nusing object store with put and get, 29\\nusing wait function for nonblocking\\ncalls, 30\\nintroduction to, 24-25\\nmajor API methods of, 35\\nsimple MapReduce example with, 41-47\\nmapping and shuffling document data,\\n43\\nreducing word count, 45\\nunderstanding Ray system components,\\n36-40\\ndistributed scheduling and execution, 39head node, 39\\nscheduling and executing work on a\\nnode, 36-39\\nRay Data, 121\\nallowing sharing of in-memory data across\\nparallel training runs, 130\\necosystem integrations allowing better data\\nprocessing, 134\\nRay Datasets, 9, 121-138, 122-133\\nbasics, 123-126\\nblocks and repartitioning, 125\\nbuilt-in transformations, 124\\ncreating a dataset, 123\\nreading and writing to storage, 123\\nschemas and data formats, 125\\nbenefits of, 121, 123\\nbuilding ML pipeline, 136-138\\ncomputing over Datasets, 126\\ndata processing with, 10\\nDataset pipelines, 127-130\\necosystem, 218\\nexample of capabilities, 216\\nexample, training copies of a classifier in\\nparallel, 130-133\\nexternal library integrations, 134-136\\nloading data into Ray AIR, 198\\nloading Snowflake data into, 225\\nscheduling strategy, 209\\nuse by Ray AIR Trainers, 200\\nuse for stateless computation, 208\\nuse with Ray Train to implement complete\\nML workflow as single application, 148\\nusing in distributed batch inference, 147\\nRay Job Submission server, 185, 191\\nRay RLlib, 10, 69-99\\nadvanced concepts, 93-99\\napplying curriculum learning, 95-97\\nother advanced topics, 98\\nworking with offline data, 97\\nconfiguring experiments, 82-84\\nenvironment configuration, 84\\nresource configuration, 83\\nrollout worker configuration, 83\\necosystem, 222\\ngetting started with, 71-81\\nbuilding a gym environment, 71\\nrunning RLlib CLI, 73-74\\nusing RLlib Python API, 75\\noverview, 70\\nIndex | 245\\n\\nreinforcement learning with, 12\\nusing with Ray Tune, 115\\nworking with environments, 85-93\\noverview of RLlib environments, 85\\npolicy servers and clients, 90-93\\nusing multiple agents, 86-90\\nRay runtime started, 180\\nRay Serve, 10, 158, 160-177\\ndeploying inference service to query over\\nHTTP, 204\\necosystem, 224\\nend-to-end example, building NLP-powered\\nAPI, 170-176\\ndefining HTTP API and driver logic, 173\\nfetching content and preprocessing, 172\\nNLP models, 172\\nputting it all together, 175-176\\nGradioServer, 222\\nintroduction to, 160-170\\narchitectural overview, 160\\ndefining basic HTTP endpoint, 161-163\\nmultimodel inference graphs, 166-170\\npurpose-built features for compute-\\nheavy ML models, 160\\nrequest batching, 165\\nscaling and resource allocation, 163\\nserving model using Gradio on, 216\\nRay Serve library\\nmodel serving with, 18\\nray start --head … command, 191\\nray stop command, 182\\nRay Train, 10, 122, 139-156\\necosystem, 222\\nintroduction to, 141-147\\ndistributed batch inference, 147\\ndistributed training with Train, 144\\nexample, predicting big tips in NYC taxi\\nrides, 141\\nTrain components used in basic ML\\npipelines, 141\\nloading, preprocessing, and featurization,\\n142\\nstandard PyTorch model and training loop\\nto leverage in, 216\\nsupport for gradient boosted decision tree\\nframeworks, 148\\nTrainers in, 148-156\\nintegrating Trainers with Ray Tune, 154migrating to Ray Train with minimal\\ncode changes, 150\\npreprocessing with Ray Train, 153\\nscaling out Trainers, 152\\nusing callbacks to monitor training, 156\\nusing to scale out JAX model, 225\\nRay Tune, 10, 101, 122\\nconfiguring and running, 110-115\\ncallbacks and metrics, 111\\ncheckpoints, stopping, and resuming,\\n113\\ncustom and conditional search spaces,\\n114\\nspecifying resources, 111\\necosystem, 222\\nhow it works, 106-110\\nintegration with other HPO frameworks,\\n108\\noverview of components, 106\\nschedulers, 109\\nsearch algorithms, 108\\nhyperparameter tuning with, 16\\nintegrating Ray Train Trainers with, 154\\nintroduction to, 105-106\\nmachine learning with, 115-119\\ntuning Keras models, 116\\nusing RLlib with Tune, 115\\nMLFlowLogger shipped with, 216\\nRay AIR Trainers integration with, 200\\nsupporting algorithms from notable HPO\\ntools, 20\\nuse by RLlib, 74\\nRay Workflows, 229\\nray.get function, 28, 36\\nusing object store with, 29\\nray.init method, 36, 180\\nray.put method, 36\\nray.remote function, 28, 34, 36\\nray.wait function, 36\\nRayCluster, 182\\nRaylets, 36\\nscheduler, 37\\nreading from and writing to storage (Ray Data‐\\nsets), 123\\nread_csv utility, 198\\nrecommendation systems, 157\\nchallenges around the edges, 159\\nrecovery, 5\\n246 | Index\\n\\nrectified linear unit (ReLU) activation function,\\n118\\nRedis instances, remote, using, 180\\nreduce phase (MapReduce), 42\\nreducing word count, 45\\nreinforcement learning\\nQ-Learning algorithm, 61\\nterminology recap, 66-67\\ntraining a model, 59-62\\nusing Ray RLlib, 12\\nrelational data processing systems, integration\\nwith Ray, 134\\nremote function, 35\\nremote Ray tasks, functions and, 27-29\\nrendering RL environments, 72, 84\\nmodifying in multi-agent environment, 89\\nrepartitioning, 125, 128\\nrepeat function (Datasets), 132\\nreplicas (Ray Serve), 160\\nFastAPI server running in each, 163\\ninstantiating replica of deployment, 162\\nsetting for text summarization model, 173\\ntuning for a deployment, 164\\nreporting, using Ray AIR session, 144\\nrequest batching, 159, 165\\nrequests library, 19\\nrequests package, 160\\nusing to test sentiment classifier, 162\\nresets (RL environments), 72\\nresource management\\nby head node, 39\\nby Raylet scheduler, 37\\nresources\\nallocation in Ray Serve, 163\\nmore expressive policies, 164\\ncompute resources used by a Trainer, 152\\nconfiguring for KubeRay cluster, 189\\nconfiguring for RLlib experiments, 83\\nidle, naive Dataset computation leading to,\\n128\\nspecifying for Ray Tune trials, 111\\nviewing use by Ray Cluster, 24\\nresources method (AlgorithmConfig), 82\\nresponses (HTTP), defining schema for, 173\\nResultGrid (Tuner API), 107\\nresults\\nhuman-readable output of training results,\\n76state and training results of RLlib DQN\\nalgorithm, 76\\ntraining, writing to directory, 74\\nresuming Ray Tune runs, 113\\nrewards (RL), 66\\nin multi-agent environment, 88\\nRISELab (UC Berkeley), 3\\nRL (see Ray RLlib; reinforcement learning)\\nRLlib algorithms page (Ray documentation), 70\\nrllib command-line tool, 13\\nrllib evaluate command, 74\\nRLlib Python API, 75\\naccessing policy and model status, 78\\nsaving, loading, and evaluating RLlib mod‐\\nels, 77\\ntraining RLlib algorithms, 75-76\\nrllib train command, 73\\nRLlib Training API documentation, 84\\nRNG (see random number generator)\\nrobotics, 158\\nrollouts (in RL), 66\\nfinished rollouts used to update policy, 64\\nrollout worker configuration for RLlib\\nexperiments, 83\\nrollouts method of Python RLlib API, 75\\nrollouts method (AlgorithmConfig), 82\\nRunConfig, 155, 201\\nruns (Tune), 108\\nRust compiler, installing, 18\\nS\\nsampling functions (Tune), 106\\nsaving RLlib algorithms in Python API, 77\\nscalability\\nRay, 5\\nRay Datasets, 123\\nScaleStrategy, 182\\nscaling\\nonline inference services of ML models, 159\\nin Ray Serve, 163\\nscaling out Trainers in Ray Train, 152\\nScalingConfig\\nadapting memory in, 212\\ndefining for XGBoostTrainer in Ray AIR,\\n199\\nRay AIR Trainers, 200\\nRay Train Trainers, 147, 151\\nspecifying parameters of cluster nodes, 152\\nschedulers\\nIndex | 247\\n\\nDask scheduler packaged with Ray, 135\\nRay Tune, 107, 109\\ncombining with search algorithms, 110\\nscheduling and executing work on a node,\\n36-37\\ndistributed scheduling and execution, 39\\nschemas, 125\\ndefining for HTTP responses, 173\\nscikit-learn, 218\\ninstalling locally, 131\\nSGDClassifier algorithm, 131\\nscores (HPO), 103\\nformulating flexible stopping condition for,\\n114\\nintermediate scores, 109\\nretuning as dictionary, 105\\nsearch algorithms, 104, 108\\ncustom, HyperOptSearch, 118\\nsupport by Tune, 107\\nsearch example (random), building with Ray,\\n102-104\\nsearch spaces, 104, 104, 106\\ncustom and conditional, 114\\ndefining with random library, 102\\ndefining with tune.uniform, 105\\nsearchers (see search algorithms)\\nself-driving cars, 158\\nsentiment classifier model, 161, 171\\nmodifying to do server-side batching, 165\\nPOSITIVE and NEGATIVE output, 162\\nscaling out to multiple replicas and adjust‐\\ning resource allocation, 164\\ntesting using requests package, 162\\nuse in NLP-powered API example, 172\\nsequential TensorFlow Keras model, 202\\nserialization formats, 124\\nserve run app:scaled_deployment command,\\n164\\nserve run CLI command, 162, 171\\nserve.run, 162\\nserver-side batching, 165\\n“Serving RLlib Models” tutorial, 70\\nsessions\\nRay, 146\\nRay AIR, 144\\nSGDClassifier algorithm, 131\\nTrainingWorker wrapper for, 132\\nsharding data, 130, 220\\nget_data_shard utility, 146shared pool of memory, 37\\nshuffle phase (MapReduce), 42\\nshuffling data, 42, 43\\nsimplicity, 4\\nSimulation class, 56\\nconverting to Ray actor, 62\\nimplementation of, 57\\nSimulationActor instances, 63\\nsimulations\\nimpossibility of faithfully simulating some\\nphysical systems, 67\\nSnowflake, 225\\nsoft state, 207\\nsort operations (Ray Datasets), 124\\nsorting data, 122\\nSpark, 20, 121, 227\\nspeed (Ray), 5\\nStandardScaler, 154, 198\\nstate\\naccessing for model and policy in Python\\nRLlib API, 78\\nin RL environment, 66\\nstate transition probabilities, 67\\nstate-action values (RL), 66, 78, 80, 102\\nand state-action functions, 80\\nstateful computations, 33, 207\\nrelying on checkpoint-based fault tolerance,\\n212\\nstateful execution, 210\\nstateful workloads, autoscaling, 213\\nstateless computations, 207\\nstateless execution, 209\\nstateless tasks of Ray AIR, 208\\nstateless workloads, autoscaling, 213\\nstate_action_table (Policy example), 57\\nupdating values in, 59\\nstep method, 53\\nsteps (in RL), 66, 72\\nactions passed to in multi-agent environ‐\\nment, 88\\nstochastic gradient descent, 131\\nstops\\nstopping Ray processes on a node, 182\\nstopping Ray Tune run, 113\\nsum, 125\\nsynchronous execution\\napproach by distributed training algorithms,\\n130\\nDatasets operations, 127\\n248 | Index\\n\\nsystems related to Ray, 41\\nT\\ntask graphs, 135\\ntask scheduler, 36\\ntasks, 6\\nconverting objective to, 103\\nexecution of, 40\\nhandling task dependencies, 31\\nmodifying existing task to incorporate actor,\\n34\\nRay AIR usage with actors for advanced\\ncomposite workloads, 208\\nRay patterns and anti-patterns for, 47\\nsetting difficulty for, 96\\nstateless data from, accessed by stateful\\nactors in composite workloads, 212\\nTaskSettableEnv, 96\\nTBXLoggerCallback, 156\\nTensorBoard, 156\\nTensorFlow, 70, 79\\nbacking Ray RLlib and Train libraries, 20\\ninstalling, 12\\nTensorFlow Keras model (sequential), 202\\ntext summarization model, 173\\nTorchTrainer, 146\\ninstantiating and working with, 151\\nwrapping in a Tuner, 216\\ntorchvision, 216\\nTPE (Tree-structured Parzen Estimator)\\nsearcher, 108\\ntrain command (rllib), 73\\ntrainables, 106\\nRLlib trainers passed as argument to\\ntune.run, 115\\nTrainers, 141\\nmore on Trainers in Ray Train, 148-156\\nintegrating Trainers with Ray Tune, 154\\nmigrating to Ray Train, 150\\npreprocessing with Ray Train, 153\\nscaling out Trainers, 152\\nTrainer classes in Ray Train sharing\\ncommon interface, 148\\nusing callbacks to monitor training, 156\\nRay AIR\\ncheckpoints generated by, 202\\nTuners working with, 201, 202\\nRay Train\\ndatasets dictionary for, 147ScalingConfig fo, 147\\nspecifying for Ray AIR, 199\\nTorchTrainer, 146\\ntraining algorithms, impressive range in RLlib,\\n99\\ntraining frameworks\\nthird-party, Trainers as wrappers for, 141\\ntraining method (AlgorithmConfig), 82\\ntraining RLlib algorithms, 75-76\\nin multi-agent environment, 89\\ntraining-serving skew, 153\\nTrainingWorker, 131\\ntraining_loop, 149\\ntrain_loop_config, 151\\ntrain_loop_per_worker function, 151\\ntrain_one_epoch helper function, 149\\ntransformations\\ncolumns into format used as features in ML\\nmodel, 143\\ncustom, performing on Datasets, 126\\ndataset, using Ray Datasets, 11\\nRay Datasets, 122\\nbuilt-in transformations, 124\\non Parquet data, 127\\ntransformers package, 160\\ntrial schedulers, 109\\ntrials (HPO), 104, 106\\nspecifying resources for in Ray Tune, 111\\nTune (see Ray Tune)\\ntune function, 17\\nTune\\'s scheduler compatibility matrix, 110\\ntune.report function, 110\\ntune.run function, 17, 106, 108\\npassing RLlib arguments to, 115\\ntune.samle_from function, 114\\nTuneConfig, 201\\ntuned examples, 13\\nTuner API, 107, 154\\nTuneReportCallback, 117\\nTuners, 201\\nAIR Tuners working with AIR Trainers, 202\\ntune_objective function, 105\\ntuples, 32\\nU\\nUC Berkeley, RISELab, 3\\nUnable to connect to GCS at …, 181\\nunions, 124\\nIndex | 249\\n\\nV\\nvalue function head, 80\\nvectorized computations, 126, 159, 165\\nusing Serve\\'s batching API, 172\\nvectorized inference, 128\\nvectorized inference, 165\\nvideo-processing pipelines, 158\\nvolume mounts, 189\\nW\\nwait function, 36\\nusing for nonblocking calls, 30\\nweights, 102, 110\\ngetting for RLlib models in Python API, 79\\nusing random.uniform sampler from\\nnumpy, 114\\nWikipedia articles, summarizing (see NLP-\\npowered API, building)\\nWikipedia package on PyPI, 172\\nwindow function, 12\\nwindows, 129\\nworker nodes, 6\\nscheduling and executing work on, 36\\n(see also Raylets)\\nsystem components comprising, 38\\nworker pods, 183\\nworker processes, 6\\nfault tolerance and ownership, 37\\nleasing to task owners, 39\\nworkersdefining TrainingWorker to train classifier\\ncopy, 131\\nRLlib, getting policy and model weights\\nfrom, 79\\nrollout, 76\\nrollout worker configuration for RLlib\\nexperiments, 83\\ntraining, loading training and validation\\ndata for, 146\\ntraining, specifying number of, 151\\nworkload execution (Ray AIR), 209-211\\ncomposite workload execution, 211\\nonline serving execution, 211\\nstateful execution, 210\\nstateless execution, 209\\nworkload type support, Spark, Dask, and Ray,\\n227\\nX\\nXGBoost, 148\\nXGBoostPredictor, 203, 204\\nXGBoostTrainer, 148\\ncreating and specifying hyperparameter\\nranges, 154\\ndefining for Ray AIR, 199\\nwrapping instance with Tuner, 201\\nZ\\nZen of Python, 43\\n250 | Index\\n\\nAbout the Authors\\nMax Pumperla  is a data science professor and software engineer located in Hamburg,\\nGermany. He’s an active open source contributor, maintainer of several Python pack‐\\nages, author of machine learning books, and speaker at international conferences. He\\ncurrently works as a software engineer at Anyscale. As head of product research at\\nPathmind Inc. he developed reinforcement learning solutions for industrial applica‐\\ntions at scale using Ray RLlib, Serve, and Tune. Max has been a core developer of\\nDL4J at Skymind, helped grow and extend the Keras ecosystem, and is a Hyperopt\\nmaintainer.\\nEdward Oakes  is a software engineer and team lead at Anyscale, where he leads the\\ndevelopment of Ray Serve and is one of the top open source contributors to Ray.\\nPrior to Anyscale, he was a graduate student in the EECS department at UC Berkeley.\\nRichard Liaw  is a software engineer at Anyscale, working on open source tools for\\ndistributed machine learning. He is on leave from the PhD program at the Computer\\nScience Department at UC Berkeley, advised by Joseph Gonzalez, Ion Stoica, and Ken\\nGoldberg.\\nColophon\\nThe animal on the cover of Learning Ray  is a marbled electric ray ( Torpedo marmor‐\\nata), also known as a torpedo ray. Marbled electric rays can be found in the eastern\\nAtlantic Ocean from Africa to Norway, as well as in the Mediterranean Sea. They are\\nbottom dwellers, preferring to live in shallow to moderately deep water in rocky reefs,\\nseagrass beds, and muddy flats.\\nMarbled electric rays are mottled brown and black, camouflaging them in the muddy\\nwaters where they hide during the day. At night, the rays emerge to hunt and forage\\nfor small fish such as gobies, mullet, mackerel, and damselfish. These rays can grow\\nup to 2 feet long, expand their jaws to swallow fish larger than their mouths, and kill\\nwith an electric charge of up to 200 volts.\\nBecause these rays are capable of electrocution, they have few natural predators.\\nMany of the animals on O’Reilly covers are endangered; all of them are important to\\nthe world.\\nThe cover illustration is by Karen Montgomery, based on an antique line engraving\\nfrom Lydekker’s Royal Natural History . The cover fonts are Gilroy Semibold and\\nGuardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad\\nCondensed; and the code font is Dalton Maag’s Ubuntu Mono.\\n\\n', metadata={'ipfs_hash': 'QmbhzXy4bXR5Nz9tKc4LScTMtpxrJBdSbUxFiiE11gTL9n', 'file_type': 'application/pdf', 'source': 'Learning_Ray_-_OReilly.pdf'})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(r[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='gem_id: viggo-train-0\\nmeaning_representation: inform(name[Dirt: Showdown], release_year[2012], esrb[E 10+ (for Everyone 10 and Older)], genres[driving/racing, sport], platforms[PlayStation, Xbox, PC], available_on_steam[no], has_linux_release[no], has_mac_release[no])\\ntarget: Dirt: Showdown from 2012 is a sport racing game for the PlayStation, Xbox, PC rated E 10+ (for Everyone 10 and Older). It\\'s not available on Steam, Linux, or Mac.\\nreferences: [\"Dirt: Showdown from 2012 is a sport racing game for the PlayStation, Xbox, PC rated E 10+ (for Everyone 10 and Older). It\\'s not available on Steam, Linux, or Mac.\"]\\nsplit: train\\n', metadata={'index': 0, 'file_type': 'text/csv', 'ipfs_hash': 'QmSbhdHQ5DN7tYzHTXetUaAQfEp1vqWJzTnxLJAkYFHEdZ', 'source': 'dataset.csv'}),\n",
       " Document(page_content=\"gem_id: viggo-train-1\\nmeaning_representation: inform(name[Dirt: Showdown], release_year[2012], esrb[E 10+ (for Everyone 10 and Older)], genres[driving/racing, sport], platforms[PlayStation, Xbox, PC], available_on_steam[no], has_linux_release[no], has_mac_release[no])\\ntarget: Dirt: Showdown is a sport racing game that was released in 2012. The game is available on PlayStation, Xbox, and PC, and it has an ESRB Rating of E 10+ (for Everyone 10 and Older). However, it is not yet available as a Steam, Linux, or Mac release.\\nreferences: ['Dirt: Showdown is a sport racing game that was released in 2012. The game is available on PlayStation, Xbox, and PC, and it has an ESRB Rating of E 10+ (for Everyone 10 and Older). However, it is not yet available as a Steam, Linux, or Mac release.']\\nsplit: train\\n\", metadata={'index': 1, 'file_type': 'text/csv', 'ipfs_hash': 'QmSbhdHQ5DN7tYzHTXetUaAQfEp1vqWJzTnxLJAkYFHEdZ', 'source': 'dataset.csv'}),\n",
       " Document(page_content=\"gem_id: viggo-train-2\\nmeaning_representation: inform(name[Dirt: Showdown], release_year[2012], esrb[E 10+ (for Everyone 10 and Older)], genres[driving/racing, sport], platforms[PlayStation, Xbox, PC], available_on_steam[no], has_linux_release[no], has_mac_release[no])\\ntarget: Dirt: Showdown is a driving/racing sport game released in 2012. It is rated E 10+, and is available on PlayStation, Xbox and PC, but not on Steam, Mac, or Linux.\\nreferences: ['Dirt: Showdown is a driving/racing sport game released in 2012. It is rated E 10+, and is available on PlayStation, Xbox and PC, but not on Steam, Mac, or Linux.']\\nsplit: train\\n\", metadata={'index': 2, 'file_type': 'text/csv', 'ipfs_hash': 'QmSbhdHQ5DN7tYzHTXetUaAQfEp1vqWJzTnxLJAkYFHEdZ', 'source': 'dataset.csv'}),\n",
       " Document(page_content=\"gem_id: viggo-train-3\\nmeaning_representation: request(release_year[2014], specifier[terrible])\\ntarget: Were there even any terrible games in 2014?\\nreferences: ['Were there even any terrible games in 2014?']\\nsplit: train\\n\", metadata={'index': 3, 'file_type': 'text/csv', 'ipfs_hash': 'QmSbhdHQ5DN7tYzHTXetUaAQfEp1vqWJzTnxLJAkYFHEdZ', 'source': 'dataset.csv'}),\n",
       " Document(page_content='gem_id: viggo-train-4\\nmeaning_representation: request(release_year[2014], specifier[terrible])\\ntarget: What\\'s the most terrible game that you played in the year 2014?\\nreferences: [\"What\\'s the most terrible game that you played in the year 2014?\"]\\nsplit: train\\n', metadata={'index': 4, 'file_type': 'text/csv', 'ipfs_hash': 'QmSbhdHQ5DN7tYzHTXetUaAQfEp1vqWJzTnxLJAkYFHEdZ', 'source': 'dataset.csv'}),\n",
       " Document(page_content='gem_id: viggo-train-5\\nmeaning_representation: request(release_year[2014], specifier[terrible])\\ntarget: What\\'s a truly terrible game released in 2014 that you know of?\\nreferences: [\"What\\'s a truly terrible game released in 2014 that you know of?\"]\\nsplit: train\\n', metadata={'index': 5, 'file_type': 'text/csv', 'ipfs_hash': 'QmSbhdHQ5DN7tYzHTXetUaAQfEp1vqWJzTnxLJAkYFHEdZ', 'source': 'dataset.csv'}),\n",
       " Document(page_content='gem_id: viggo-train-6\\nmeaning_representation: give_opinion(name[Little Nightmares], rating[good], genres[adventure, platformer, puzzle], player_perspective[side view])\\ntarget: Adventure games that combine platforming and puzzles can be frustrating to play, but the side view perspective is perfect for them. That\\'s why I enjoyed playing Little Nightmares.\\nreferences: [\"Adventure games that combine platforming and puzzles can be frustrating to play, but the side view perspective is perfect for them. That\\'s why I enjoyed playing Little Nightmares.\"]\\nsplit: train\\n', metadata={'index': 6, 'file_type': 'text/csv', 'ipfs_hash': 'QmSbhdHQ5DN7tYzHTXetUaAQfEp1vqWJzTnxLJAkYFHEdZ', 'source': 'dataset.csv'}),\n",
       " Document(page_content='gem_id: viggo-train-7\\nmeaning_representation: give_opinion(name[Little Nightmares], rating[good], genres[adventure, platformer, puzzle], player_perspective[side view])\\ntarget: Little Nightmares is a pretty cool game that has kept me entertained. It\\'s an adventure side-scrolling platformer with some puzzle elements to give me a bit of a challenge.\\nreferences: [\"Little Nightmares is a pretty cool game that has kept me entertained. It\\'s an adventure side-scrolling platformer with some puzzle elements to give me a bit of a challenge.\"]\\nsplit: train\\n', metadata={'index': 7, 'file_type': 'text/csv', 'ipfs_hash': 'QmSbhdHQ5DN7tYzHTXetUaAQfEp1vqWJzTnxLJAkYFHEdZ', 'source': 'dataset.csv'}),\n",
       " Document(page_content=\"gem_id: viggo-train-8\\nmeaning_representation: give_opinion(name[Little Nightmares], rating[good], genres[adventure, platformer, puzzle], player_perspective[side view])\\ntarget: Little Nightmares is a good little adventure platforming puzzle game, I particularly enjoy what they did with the side view too.\\nreferences: ['Little Nightmares is a good little adventure platforming puzzle game, I particularly enjoy what they did with the side view too.']\\nsplit: train\\n\", metadata={'index': 8, 'file_type': 'text/csv', 'ipfs_hash': 'QmSbhdHQ5DN7tYzHTXetUaAQfEp1vqWJzTnxLJAkYFHEdZ', 'source': 'dataset.csv'}),\n",
       " Document(page_content='gem_id: viggo-train-9\\nmeaning_representation: inform(name[Super Bomberman], release_year[1993], genres[action, strategy], platforms[Nintendo, PC], available_on_steam[no], has_linux_release[no], has_mac_release[no])\\ntarget: Super Bomberman is an action and strategy game from 1993 for Nintendo consoles and PC only. It\\'s an older console game that is, unfortunately, not available to play on Steam and does not offer support for Mac or Linux.\\nreferences: [\"Super Bomberman is an action and strategy game from 1993 for Nintendo consoles and PC only. It\\'s an older console game that is, unfortunately, not available to play on Steam and does not offer support for Mac or Linux.\"]\\nsplit: train\\n', metadata={'index': 9, 'file_type': 'text/csv', 'ipfs_hash': 'QmSbhdHQ5DN7tYzHTXetUaAQfEp1vqWJzTnxLJAkYFHEdZ', 'source': 'dataset.csv'})]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'index': 0,\n",
       " 'file_type': 'text/csv',\n",
       " 'ipfs_hash': 'QmSbhdHQ5DN7tYzHTXetUaAQfEp1vqWJzTnxLJAkYFHEdZ',\n",
       " 'source': 'dataset.csv'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(r[1][:10])\n",
    "\n",
    "display(r[1][0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infura - Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = IPFSDirectoryDataLoader(use_infura=True, infura_api_key=os.getenv(\"INFURA_API_KEY\"), infura_api_secret=os.getenv(\"INFURA_API_SECRET\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = reader.load(\"QmNUxeu1F9LTFQ7d5fdtQhNT3twNNo4NWRQ11ghchedcAt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='<!-- generate a simple html -->\\n\\n<html>\\n<head>\\n<title>My first PHP page</title>\\n</head>\\n<body>\\n    <h1>My first PHP page</h1>\\n    <p>\\n        <?php\\n            echo \"Hello World!\";\\n        ?>\\n    </p>\\n</body>\\n</html>', metadata={'ipfs_hash': 'QmQV4k5TK66FZtys8D7T71AdkQt5gh4eqkm29647wyGcnk', 'file_type': 'text/html', 'source': 'index.html'})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'ipfs_hash': 'QmQV4k5TK66FZtys8D7T71AdkQt5gh4eqkm29647wyGcnk',\n",
       " 'file_type': 'text/html',\n",
       " 'source': 'index.html'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(r[0])\n",
    "display(r[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='import json\\nimport time\\nimport magic\\nimport PyPDF2\\nimport logging\\nimport requests\\nimport tempfile\\nimport pandas as pd\\nfrom PIL import Image\\nfrom typing import Union, Generator, Callable\\nfrom langchain.document_loaders.base import BaseLoader\\nfrom langchain.docstore.document import Document\\n\\nlogger = logging.getLogger(__name__)\\n\\ndef identify_file_type_from_bytes(byte_data: bytes) -> str:\\n    \"\"\"\\n    Identify the file type of a given byte stream.\\n\\n    Args:\\n        byte_data (bytes): Byte stream to identify the file type of.\\n\\n    Returns:\\n        file_type (str): Identified file type of the byte stream.\\n    \"\"\"\\n    mime = magic.Magic(mime=True)\\n    file_type = mime.from_buffer(byte_data)\\n    return file_type\\n\\n# Reader functions for different file types.\\n# Each function takes a response object and returns\\n# the appropriate data format (text, DataFrame, JSON, image).\\n\\ndef pdf_reader(response : requests.Response) -> str:\\n    \"\"\"\\n    Read and extract text from a PDF file contained in a response object.\\n\\n    Args:\\n        response (requests.Response): Response object containing PDF content.\\n\\n    Returns:\\n        text (str): Extracted text from the PDF.\\n    \"\"\"\\n    with tempfile.TemporaryFile() as temp:\\n        temp.write(response.content)\\n        pdf = PyPDF2.PdfReader(temp)\\n        text = \"\"\\n        for page in pdf.pages:\\n            text += page.extract_text() + \"\\\\n\\\\n\"\\n\\n        return text\\n    \\ndef txt_reader(response : requests.Response) -> str:\\n    \"\"\"\\n    Read text content from a response object.\\n\\n    Args:\\n        response (requests.Response): Response object containing text content.\\n\\n    Returns:\\n        text (str): Decoded text content.\\n    \"\"\"\\n    with tempfile.TemporaryFile() as temp:\\n        temp.write(response.content)\\n        temp.seek(0)\\n        text = temp.read().decode(\"utf-8\")\\n        return text\\n\\ndef lazy_csv_loader(df : pd.DataFrame) -> Generator[Document, None, None]:\\n    for index, row in df.iterrows():\\n        content = \"\"\\n        for k, v in row.items():\\n            # only string values which are not null and string\\n            if isinstance(v, str) and v.strip() != \"\":\\n                content += f\"{k.strip()}: {v.strip()}\\\\n\"\\n            else:\\n                content += f\"{k}: {v}\\\\n\"\\n        yield Document(\\n            page_content=content,\\n            metadata={\\n                \"index\": index,\\n                \"file_type\": \"text/csv\",\\n            },\\n        )\\n\\ndef csv_reader(response : requests.Response) -> list[Document]:\\n    \"\"\"\\n    Read CSV content from a response object and return it as a pandas DataFrame.\\n\\n    Args:\\n        response (requests.Response): Response object containing CSV content.\\n\\n    Returns:\\n        df (pandas.DataFrame): DataFrame containing the CSV data.\\n    \"\"\"\\n    with tempfile.TemporaryFile() as temp:\\n        temp.write(response.content)\\n        temp.seek(0)\\n        df = pd.read_csv(temp)\\n    \\n    df = [doc for doc in lazy_csv_loader(df)]\\n\\n    return df\\n\\n    \\ndef json_reader(response : requests.Response) -> str:\\n    \"\"\"\\n    Read JSON content from a response object and return it as a dictionary.\\n\\n    Args:\\n        response (requests.Response): Response object containing JSON content.\\n\\n    Returns:\\n        data (dict): Dictionary containing the JSON data.\\n    \"\"\"\\n    with tempfile.TemporaryFile() as temp:\\n        temp.write(response.content)\\n        temp.seek(0)\\n        data = json.load(temp)\\n        # JSON to string\\n        data = json.dumps(data)\\n        return data\\n    \\ndef image_reader(response : requests.Response) -> Image.Image:\\n    \"\"\"\\n    Read image content from a response object and return it as a PIL image.\\n\\n    Args:\\n        response (requests.Response): Response object containing image content.\\n\\n    Returns:\\n        img (PIL.Image.Image): Image object.\\n    \"\"\"\\n    with tempfile.NamedTemporaryFile() as temp:\\n        temp.write(response.content)\\n        temp.seek(0)\\n        img = Image.open(temp.name)\\n        return img\\n\\ndef select_reader(file_type : str) -> Callable:\\n    \"\"\"\\n    Select the appropriate reader function based on the file type.\\n\\n    Args:\\n        file_type (str): File type to select the reader for.\\n\\n    Returns:\\n        reader (function): Appropriate reader function for the file type.\\n\\n    Raises:\\n        Exception: If the file type is unsupported.\\n    \"\"\"\\n    if \"pdf\" in file_type:\\n        return pdf_reader\\n    elif \"csv\" in file_type:\\n        return csv_reader\\n    elif \"json\" in file_type:\\n        return json_reader\\n    # elif \"image\" in file_type:\\n        # return image_reader\\n    elif \"text\" in file_type:\\n        return txt_reader\\n    else:\\n        raise Exception(f\"Unsupported file type: {file_type}\")\\n\\n\\nclass IPFSFileloader(BaseLoader):\\n    base_url: str = \"http://127.0.0.1:5001\"\\n    infura_base_url: str = \"https://ipfs.infura.io:5001\"\\n    base_api_path: str = \"api/v0\"\\n    read_suffix: str = \"cat\"\\n    version_suffix: str = \"version\"\\n    max_retries: int = 5\\n    retry_delay: int = 1\\n    def __init__(self, \\n        use_infura: bool =False, \\n        infura_api_key: Union[str, None]=None, \\n        infura_api_secret: Union[str, None]=None, \\n        debug: bool=False\\n    ) -> None:\\n        \"\"\"\\n        Constructor for IPFSFileloader.\\n\\n        Args:\\n            use_infura (bool, optional): Flag to use Infura instead of local IPFS. Defaults to False.\\n            infura_api_key (str, optional): Infura API key. Required if use_infura is True.\\n            infura_api_secret (str, optional): Infura API secret. Required if use_infura is True.\\n            debug (bool, optional): Enable or disable debug logging. Defaults to False.\\n        \"\"\"\\n        self.use_infura = use_infura\\n        self.infura_api_key = infura_api_key\\n        self.infura_api_secret = infura_api_secret\\n        self.debug = debug\\n\\n        if self.use_infura and (not self.infura_api_key or not self.infura_api_secret):\\n            raise ValueError(\"Infura API key and secret are required when using Infura\")\\n\\n        self.check_daemon_running()\\n\\n    def check_daemon_running(self) -> None:\\n        \"\"\"\\n        Checks if the IPFS daemon is running and accessible, or if the Infura API is reachable.\\n        \"\"\"\\n        if self.use_infura:\\n            full_url = f\"{self.infura_base_url}/{self.base_api_path}/{self.version_suffix}\"\\n            auth = requests.auth.HTTPBasicAuth(self.infura_api_key, self.infura_api_secret)\\n        else:\\n            full_url = f\"{self.base_url}/{self.base_api_path}/{self.version_suffix}\"\\n\\n        for i in range(self.max_retries):\\n            try:\\n                response = requests.post(full_url, auth=auth if self.use_infura else None)\\n                if response.ok:\\n                    if self.debug:\\n                        logger.debug(f\"IPFS daemon is running at {full_url}\")\\n                    return\\n                else:\\n                    raise Exception(\"IPFS daemon not running or not accessible\")\\n            except requests.exceptions.ConnectionError:\\n                time.sleep(self.retry_delay)\\n\\n        raise Exception(\"IPFS daemon not running or not accessible\")\\n\\n    def load(\\n        self, \\n        ipfs_hash : str\\n        ) -> Union[Document, list[Document]]:\\n        \"\"\"\\n        Loads a file from IPFS using its hash, either from local IPFS or Infura.\\n        \"\"\"\\n        if self.use_infura:\\n            full_url = f\"{self.infura_base_url}/{self.base_api_path}/{self.read_suffix}\"\\n            auth = requests.auth.HTTPBasicAuth(self.infura_api_key, self.infura_api_secret)\\n        else:\\n            full_url = f\"{self.base_url}/{self.base_api_path}/{self.read_suffix}\"\\n\\n        full_url += f\"?arg={ipfs_hash}\"\\n\\n        for i in range(self.max_retries):\\n            try:\\n                response = requests.post(full_url, auth=auth if self.use_infura else None)\\n                if response.ok:\\n                    file_type = identify_file_type_from_bytes(response.content)\\n                    reader = select_reader(file_type)\\n                    text = reader(response)\\n                    # if isinstance() is string\\n                    if isinstance(text, str):\\n                        return Document(page_content=text, metadata={\"ipfs_hash\": ipfs_hash, \"file_type\": file_type})\\n                    if isinstance(text, list):\\n                        # add ipfs_hash to metadata\\n                        for doc in text:\\n                            doc.metadata[\"ipfs_hash\"] = ipfs_hash\\n                        return text\\n                else:\\n                    logger.error(f\"Failed to load file from IPFS with hash {ipfs_hash}\")\\n            except requests.exceptions.ConnectionError:\\n                time.sleep(self.retry_delay)\\n\\n        raise Exception(f\"Failed to connect to IPFS daemon for hash {ipfs_hash}\")\\n', metadata={'ipfs_hash': 'QmfGVVAcwgjQjJTVmUgthhjomCg3AYKNywAcsh8U6KvME6', 'file_type': 'text/x-script.python', 'source': 'ipfs_filereader.py'})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'ipfs_hash': 'QmfGVVAcwgjQjJTVmUgthhjomCg3AYKNywAcsh8U6KvME6',\n",
       " 'file_type': 'text/x-script.python',\n",
       " 'source': 'ipfs_filereader.py'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(r[1])\n",
    "display(r[1].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='1CS3244 : Machine Learning \\nXavier Bresson1Department of Computer ScienceNational University of Singapore (NUS)Lecture 3 : kNN, k-d Tree, Decision Tree, Random Forest, Gradient BoostingSemester 1 2023/24\\nXavier Bressonhttps://twitter.com/xbresson  \\n\\n\\n2Material used for preparation\\nXavier Bresson2\\nProf Kilian Weinberger, CS4780 Cornell, Machine Learning, 2018\\nhttps://www.cs.cornell.edu/courses/cs4780/2018fa \\nProf Min-Yen Kan, CS3244 NUS, Machine Learning, 2022\\nhttps://knmnyn.github.io/cs3244-2210 \\n\\n\\n3Outline\\nXavier Bresson3\\nkNN\\nk-d tree\\nDecision tree\\nBagging\\nBoosting\\nConclusion\\n\\n\\n4Outline\\nXavier Bresson4\\nkNN\\nk-d tree\\nDecision tree\\nBagging\\nBoosting\\nConclusion\\n\\n\\n5\\n<latexit sha1_base64=\"3XUwxeo15JJl0/kJnb65md5BdzQ=\">AAADdXicbVJbaxNBFN4kXmq8tNUXQYRjq20W2jRbipdCoCCCL0JFe4FOsszOnk2GzMyuO7O66XZ/gf/ON/+GL7462cTYph4Y+OZ85/pxgkRwbTqdn7V648bNW7eX7jTv3rv/YHll9eGxjrOU4RGLRZyeBlSj4AqPDDcCT5MUqQwEngSjtxP+5Cummsfqsxkn2JN0oHjEGTXW5a/Wvkf+qJW7sNEFYjA3qSxkHGJJAj5okWK838q3xi7hCj71R35OKsIlpLkxj4dv3AyhnAZAF0bbfxmFNEVtQCEfDIM4hTiCMp/nhRjZsUOgGspJRTLUCWVYdNpvmCxn5UiRb+4TSXPfgmoOP++Pynn3sLQT5psuEfhl0bnpbgEBEsUpFQLsv8oHotFIrjI9rQWkvLoPVSEsNDh37WJEZ9LnlTIXuc+3z31+0U+gUqRfeDtJOWmXdL1/pVofqBpSY6hyZ+TuJfJdxgQP0ZJQNptNf2W90+5UBteBNwPrzswO/ZUfJIxZJlEZJqjWZ14nMb2CpoYzgWWTZBqtoiM6wDMLFZWoe0V1NSW8sJ4QrDb2KQOV93JGQaXWYxnYSEnNUC9yE+f/uLPMRK97BVdJZlCxaaMoE2BimJwghDxFZsTYAspSbmcFNqQpZcYe6kQEb3Hl6+B4t+29bO993Fs/2J3JseQ8cdacluM5r5wD571z6Bw5rPar/rj+rL5W/9142nje2JiG1muznEfOFWvs/AEckhVG</latexit>fk(x)=m o d e\\x00{y:(x, y)2Skx}\\x00withSkx=k\\x00nearest neighbor ofxdeﬁned asSkx={x0: maxx02Skxd(x, x0)\\uf8ffd(x, x00),8x002S\\\\Skx}and d(x, z)=Xi\\x00|xi\\x00zi|p\\x001/p,p= 1 (Manhattan),p= 2 (Euclidean)kNN\\nXavier Bresson5\\nkNN algorithm\\nAssumption : Close data points have similar labels, i.e. class or regression value.\\nAlgorithm : For a test data point x, assign the most common labels in its k nearest neighbors in the training set S.\\nFormalization\\nkNNclassification : \\nkNNregression : \\n<latexit sha1_base64=\"pLcZQhPXPg2YEB4N/bT/tBseJIM=\">AAACIXicbVBNa9tAEF2laeO6H3GaYy9LTIsNxUghpCZQMOTSo0PiJOBVxWo9chbvrsTuqFgI/5Ve8ldyySGl5FbyZyIrPqRJHww83pthZl6cKenQ9/96ay/WX77aaLxuvnn77v1ma+vDqUtzK2AkUpXa85g7UNLACCUqOM8scB0rOItnh0v/7CdYJ1NzgkUGoeZTIxMpOFZS1Oon0awz79LP3yhDmKPVpQZuFiyW0w4ri4PO/EvRZdLQ4x+zaM5qo8tYM2q1/Z5fgz4nwYq0yQrDqHXLJqnINRgUijs3DvwMw5JblELBoslyBxkXMz6FcUUN1+DCsv5wQT9VyoQmqa3KIK3VxxMl184VOq46NccL99Rbiv/zxjkm/bCUJssRjHhYlOSKYkqXcdGJtCBQFRXhwsrqViouuOUCq1CXIQRPX35OTnd7wX5v72ivPdhdxdEgH8kO6ZCAfCUD8p0MyYgI8otckRvy27v0rr0/3u1D65q3mtkm/8C7uwdEbaJP</latexit>fk(x) = mean\\x00{y:(x, y)2Skx}\\x00\\nkNN binary classification with k=3x belongs to blue class\\n\\n6Curse of dimensionality\\nXavier Bresson6\\nEuclidean/Lp distances between points : \\nk-NN assumption “close data points have similar labels” works if we can define a meaningful distance between two data points.\\nUnfortunately, in high-dim spaces, data points sampled from a random probability distribution, are far from each other with (almost) the same distance value. \\nLet us sample points uniformly at random within the unit cube and let us the compute the distance between all pair of points when the dimensionality increases.\\n\\n\\n7Blessing of structure\\nXavier Bresson7\\nReal-world data does not follow a random probability distribution!\\nData has structure, s.a. edges, textures for natural images.\\nThis means that data lie in a much lower dimensional sub-space than Rd. \\nFor example, images of human face can be accurately described with e.g. 50 features s.a. male/female, blond/dark hair, etc, although the original image lie in a space of 1M dimensions (1,000 x 1,000 pixels).\\nRd\\n\\n8kNN summary\\nXavier Bresson8\\nkNN algorithm is the simplest machine learning technique for classification (binary and multiple classes) and regression.\\nIt is expressive as it can produce non-linear boundary decision.\\nAs n→∞, kNNbecomes provably very accurate, but also intractable.\\nReal-world applications have a limited n number of training data.\\nAs d→∞, curse of dimensionality kicks in and kNNbreaks for Euclidean distances.\\nkNN works if distances are semantically meaningful.\\nCombining kNNand deep learning representation is today a strong baseline.\\n\\n\\n9Outline\\nXavier Bresson9\\nkNN\\nk-d tree\\nDecision tree\\nBagging\\nBoosting\\nConclusion\\n\\n\\n10kNN complexity\\nXavier Bresson10\\nkNN time complexity is O(n.d.k), where n the number of training data, d the number of data dimensions and k is the number of k nearest neighbors.\\nThis complexity means that kNN becomes very slow and memory consuming when n is large but we want to have n as large as possible to get the best possible accuracy.\\nCan we improve the speed? Yes, by leveraging data structure. \\n\\n\\n11k-d tree\\nXavier Bresson11\\nGeneral idea : When we search for the closest point(s), most data points are actually far away and hence there is no need to compute the distances for these far away points.\\n How to achieve this goal?\\nSolution is to partition the d-dim feature space with a binary tree structure.\\n\\n\\n12\\nk-d tree speeds up kNN\\nXavier Bresson12\\nFor example, let us consider the full dataset and one partition as follows:\\nMake a cut along one feature dimension that divides the data into two sets, i.e. Left and Right, with approximatively the same number of data in each half.\\nConsider a new data point x, or which we want to find the closest neighbor.\\nIdentify which set the data x lies, here the right set R.\\nFind the nearest neighborxNNRin R, it requires O(n/2). \\nCompute the distance d(x,C) between x and the cut C.\\nIf d(x,C) > d(x, xNNR) then all x in L can be discarded (by triangular inequality) and kNNgets a 2x speed-up! \\nCut/split the space S into 2 sets R and L with ≈ the same number of points\\nAll points in L cannot be NN, we can discard/prune the space by a factor 2.\\n<latexit sha1_base64=\"w1Oy9Bvc2SFpjHPIifHqw4QEoZ8=\">AAACOnicbVBNSwMxEM36WetX1aOXYLFUkLIroh48FOrBU6liVejWkk1TG8wmSzJrW5b9XV78Fd48ePGgiFd/gNvagrY+CLx5b4bJPC8Q3IBtP1tT0zOzc/OphfTi0vLKamZt/dKoUFNWpUoofe0RwwSXrAocBLsONCO+J9iVd1fq+1f3TBuu5AX0Alb3ya3kLU4JJFIjc+YC64L2o2ac7+52G6OyXI5vRvw83skd/+kbFaV4B7suTufcE9WRRGvVSTcyWbtgD4AniTMkWTREpZF5cpuKhj6TQAUxpubYAdQjooFTweK0GxoWEHpHblktoZL4zNSjwekx3k6UJm4pnTwJeKD+noiIb0zP95JOn0DbjHt98T+vFkLrqB5xGYTAJP1Z1AoFBoX7OeIm14yC6CWEUM2Tv2LaJppQSNLuh+CMnzxJLvcKzkFh/2w/W7SHcaTQJtpCeeSgQ1REp6iCqoiiB/SC3tC79Wi9Wh/W50/rlDWc2UB/YH19A73urm8=</latexit>d(x, xRNN)<d(x,C)+\\n\\n13\\nk-d tree speeds up kNN (on average)\\nXavier Bresson13\\nQ: What happens if d(x,C) < d(x, xNNR)? \\nIt is possible that the NN lies in L –so we need to compute all distances d(x,xL).\\nSpeed complexity is then O(n), same as kNN.\\nWorst case complexity of k-d tree is kNNcomplexity, but it is actually much better in practice (average complexity).\\nWe need to compute the distance to all points in L.<latexit sha1_base64=\"G9ICP1SDtT4GnrRnBuMujiCiWuQ=\">AAACO3icbVBNSwMxEM36WetX1aOXYLFUkLIrop6kUA+eShWrQreWbJraYDZZklnbsuz/8uKf8ObFiwdFvHp3W1vQ1geBN+/NMJnnBYIbsO1na2p6ZnZuPrWQXlxaXlnNrK1fGhVqyqpUCaWvPWKY4JJVgYNg14FmxPcEu/LuSn3/6p5pw5W8gF7A6j65lbzFKYFEamTOXWBd0H7UjPPd3W5jVJbL8c2In8c7OXz8p3FUlOId7Lo4nXNPVEcSrVUn3chk7YI9AJ4kzpBk0RCVRubJbSoa+kwCFcSYmmMHUI+IBk4Fi9NuaFhA6B25ZbWESuIzU48Gt8d4O1GauKV08iTggfp7IiK+MT3fSzp9Am0z7vXF/7xaCK2jesRlEAKT9GdRKxQYFO4HiZtcMwqilxBCNU/+immbaEIhibsfgjN+8iS53Cs4B4X9s/1s0R7GkUKbaAvlkYMOURGdogqqIooe0At6Q+/Wo/VqfVifP61T1nBmA/2B9fUNLESumw==</latexit>d(x, xRNN)>d(x,C)+\\n<latexit sha1_base64=\"ylSVsS16ZqJFC7+IjNC5bfFEU80=\">AAACqnicjVFNT9tAEF27LR9poWk5clkRoYIURXaFgEORkLhwoAgiklDFIVqvx2TFem3tjiGR5R/HX+DGv2ETkqqBSjDSSm/evDe7OxNmUhj0vEfH/fDx08Li0nLl85eV1a/Vb9/bJs01hxZPZaovQ2ZACgUtFCjhMtPAklBCJ7w5Gtc7t6CNSNUFjjLoJexaiVhwhpbqV+8DhCHqpBAxLeksicqtYX3Yn6Wnp+XVDDfL7V/v0Z2U2zNIcQDKdp8THszbmnUaBJW/jtRa9J0w8IbtpFHpV2tew5sEfQ38KaiRaZz1qw9BlPI8AYVcMmO6vpdhr2AaBZdQVoLcQMb4DbuGroWKJWB6xWTUJd20TETjVNujkE7Yfx0FS4wZJaFVJgwH5mVtTP6v1s0x3u8VQmU5guLPF8W5pJjS8d5oJDRwlCMLGNfCvpXyAdOMo93ueAj+yy+/Bu2fDX+3sXO+Uzv0puNYIutkg2wRn+yRQ3JMzkiLcOeH89tpOx237jbdP273Weo6U88amQs3egIuOdZZ</latexit>if d(x, xRNN)<d(x, xLNN)t h e nxNN=xRNN,otherwisexNN=xLNN.Complexity is O(n).\\n\\n14k-d tree structure\\nXavier Bresson14\\nTree construction / training stage \\nSplit recursively in half along each feature dimension.\\nIterate over all feature dimensions.\\nTree depth is quite small depth =O(log2n), e.g. log2103≈10, log2106≈20, log2109≈30\\nHow to select which next feature dimension?\\nA good heuristic is to select the feature dimension that captures the largest variation of data (similar to PCA). \\n\\n\\n15k-d tree search\\nXavier Bresson15\\nExample of NN search process in 2D \\nWhich order of search?\\nWhich partitions to prune?\\n\\n\\n16\\nk-d tree search #1\\nXavier Bresson16\\nExample of NN search process in 2D \\nWhich order of search?\\nWhich partitions to prune?\\nBest case scenario O(n/4)\\n\\n\\n17\\nk-d tree search #2\\nXavier Bresson17\\nExample of NN search process in 2D \\nWhich order of search?\\nWhich partitions to prune?\\nWorst case scenario O(n)\\n\\n\\n18\\nk-d tree search\\nXavier Bresson18\\nExample of NN search process in 2D \\nWhich order of search?\\nWhich partitions to prune?\\nWorst case scenario O(n)\\n\\n\\n19\\nk-d tree search\\nXavier Bresson19\\nExample of NN search process in 2D \\nWhich order of search?\\nWhich partitions to prune?\\nWorst case scenario O(n)\\n\\n\\n20\\n<latexit sha1_base64=\"YvsD3pUQn2gS/BeuVAM3k9iTV30=\">AAACP3icbVBBSxtBGJ21tabR1liPXj4ahEgh7AZRL4LgpbcoNFHIxjA7mU0GZ2eGmW/VsOw/8+Jf8Narlx5aSq+9dTYG2qoPBt689z5mvpcYKRyG4ddg6dXr5Tcrtbf11bV379cbGx/6TueW8R7TUtvzhDouheI9FCj5ubGcZonkZ8nlceWfXXHrhFZfcGb4MKMTJVLBKHpp1Oh3WwY+wThOLWWFKovOhSl3IIbYiskUqbX62t+g24qlnow6oKq0DyC/QZsVcC1wCiWYw7+JnXp91GiG7XAOeE6iBWmSBU5Gjft4rFmecYVMUucGUWhwWFCLgkle1uPccUPZJZ3wgaeKZtwNi/n+JWx7ZQyptv4ohLn670RBM+dmWeKTGcWpe+pV4kveIMf0YFgIZXLkij0+lOYSUENVJoyF5QzlzBPKrPB/BTalvkn0lVclRE9Xfk76nXa019493W0edRZ11MgW+UhaJCL75Ih8JiekRxi5JQ/kO/kR3AXfgp/Br8foUrCY2ST/Ifj9B8BFq7A=</latexit>O(p+dn2p)!O(log2n+d)w i t hp=O(log2n)\\n<latexit sha1_base64=\"Q6+CjJBvO6ywHSv3aOpml5SSwyA=\">AAACPnicbVAxTxsxGPVR2tJAS2jHLp8aVQpLdBch6IKExNINkAggxenJ5/guFj7bsr9rFZ3yy1j4DWwdWRiKECtjfSFDCzzJ0tN775O/72VWSY9x/DtaerX8+s3blXet1bX3H9bbGx9PvKkcFwNulHFnGfNCSS0GKFGJM+sEKzMlTrPz/cY//Smcl0Yf49SKUckKLXPJGQYpbQ+or8rUAoUDmsmiCzR3jNd6Vvd/2BlQlKXwEDg09ibswkHX6s2Qp04WE2TOmV/NdFdTZYq0D8Fspe1O3IvngOckWZAOWeAwbV/RseFVKTRyxbwfJrHFUc0cSq7ErEUrLyzj56wQw0A1C1uN6vn5M/galDHkxoWnEebqvxM1K72flllIlgwn/qnXiC95wwrzb6Naaluh0Pzxo7xSgAaaLmEsneCopoEw7mTYFfiEhf4wNN6UkDw9+Tk56feS7d7W0VZnr7+oY4V8Jl9IlyRkh+yR7+SQDAgnF+Sa/CG30WV0E91F94/RpWgx84n8h+jhLxaDq+8=</latexit>XpO\\x00n2p⇥2p\\x00=O(pn)!O(nlog2n)\\n<latexit sha1_base64=\"38ZF7Ude2tU40KRTGQ17XFicKII=\">AAACb3icbVFNb9QwEHXCV1m+lnLgUIRGrJCyl1USVcClUiUu3LZIbFtps0SO42St2o5lTyirKFd+IDf+Axf+Ac52haBlJEtP782zZ54LI4XDOP4RhLdu37l7b+/+6MHDR4+fjJ/un7qmtYwvWCMbe15Qx6XQfIECJT83llNVSH5WXLwf9LMv3DrR6E+4MXylaK1FJRhFT+Xjb5lrVW4ggzlkhagjKLPKUtbpvks/mx4yFIo78HirT+EI5lFp9NRbMivqNVJrm8vhgqjM1s5Qxrt4FqdM9aAz2dR5CkM38q9oVQeXAtfQgzmaR3/UUT6eeM+24CZIdmBCdnWSj79nZcNaxTUySZ1bJrHBVUctCiZ5P8pax/0oF7TmSw819Uusum1ePbz2TAlVY/3RCFv2b0dHlXMbVfhORXHtrmsD+T9t2WL1btUJbVrkml09VLUSsIEhfCiF5QzlxgPKrPCzAltTHzf6LxpCSK6vfBOcprPkzezw4+HkON3FsUcOyCsSkYS8JcfkAzkhC8LIz2A/OAheBL/C5+HLEK5aw2DneUb+qXD6Gwqit8I=</latexit>XpO\\x00dn2p⇥2p\\x00=O(dpn)!O(dnlog2n)w i t hp=O(log2n)k-d tree complexity\\nXavier Bresson20\\nSuppose k=1 (i.e. nearest neighbor)\\nTraining / building k-d tree\\nSpace/memory complexity (worst case) : \\nTime/speed complexity (worst case) :\\nInference / NN search\\nTime/speed complexity (best case) : \\nTime/speed complexity (worst case/NN) : \\nTime/speed complexity (average case/tricky) : \\n<latexit sha1_base64=\"4edVQzyRIUhBDHzXYf1R7aPpsqk=\">AAAB7XicbVBNSwMxEJ2tX7V+VT16CRahXsquFPVY8OLNCvYD2qVks9k2NpssSVYopf/BiwdFvPp/vPlvzLZ70NYHA4/3ZpiZFyScaeO6305hbX1jc6u4XdrZ3ds/KB8etbVMFaEtIrlU3QBrypmgLcMMp91EURwHnHaC8U3md56o0kyKBzNJqB/joWARI9hYqX1XDcV5aVCuuDV3DrRKvJxUIEdzUP7qh5KkMRWGcKx1z3MT40+xMoxwOiv1U00TTMZ4SHuWChxT7U/n187QmVVCFEllSxg0V39PTHGs9SQObGeMzUgve5n4n9dLTXTtT5lIUkMFWSyKUo6MRNnrKGSKEsMnlmCimL0VkRFWmBgbUBaCt/zyKmlf1LzLWv2+Xmlc5HEU4QROoQoeXEEDbqEJLSDwCM/wCm+OdF6cd+dj0Vpw8plj+APn8wcreI4r</latexit>O(dn)\\n<latexit sha1_base64=\"9SUMYVBzvad+UTXcvcPBcKhk+vg=\">AAAB+HicbVDLSsNAFJ3UV62PRl26GSxC3ZSkFHVZcOPOCvYBbQiTyaQdOpkJMxOhhn6JGxeKuPVT3Pk3TtostPXAhcM593LvPUHCqNKO822VNja3tnfKu5W9/YPDqn103FMilZh0sWBCDgKkCKOcdDXVjAwSSVAcMNIPpje5338kUlHBH/QsIV6MxpxGFCNtJN+u3tXDERNjvwn5BaxUfLvmNJwF4DpxC1IDBTq+/TUKBU5jwjVmSKmh6yTay5DUFDMyr4xSRRKEp2hMhoZyFBPlZYvD5/DcKCGMhDTFNVyovycyFCs1iwPTGSM9UateLv7nDVMdXXsZ5UmqCcfLRVHKoBYwTwGGVBKs2cwQhCU1t0I8QRJhbbLKQ3BXX14nvWbDvWy07lu1drOIowxOwRmoAxdcgTa4BR3QBRik4Bm8gjfryXqx3q2PZWvJKmZOwB9Ynz/MXZEv</latexit>O(dlog2n)#hierarchy levels#data pts in each region#regions\\nsplitting time per feature dimension\\n\\n\\n21k-d tree with k nearest neighbors\\nXavier Bresson21\\nNote that the “k” in the name “k”-d tree means the number of data features.\\nNote that the “d” in the name k-”d” tree means “dimension”.\\nIn our lecture, we call the number of data features d and the number of nearest neighborsk.\\n\\n\\n22Summary\\nXavier Bresson22\\nkNN is slow because it does a lot of unnecessary pairwise distance computations.\\nk-d tree partitions the feature space so we can discard space partitions that are further away than our closest k neighbors.\\nPros : \\nExact kNN, but approximation can be used e.g. no backtracking in parent nodes.\\nEasy to implement.\\nAverage inference complexity is O(d.log2n), compared to O(d.n) with kNN.\\nCons : \\nCuts are axis-aligned which does not generalize well to higher dimensions.\\n[Not included] Ball tree partitions the manifold of data points (assumption),                      as opposed to the whole space. This performs much better in higher dimensions.\\n\\n\\n23Outline\\nXavier Bresson23\\nkNN\\nk-d tree\\nDecision tree\\nBagging\\nBoosting\\nConclusion\\n\\n\\n24Motivation\\nXavier Bresson24\\nkNN requires to store the full training set to make a prediction.\\nWhen n becomes large, it becomes intractable.  \\nReal-world assumption : Most data are not random and usually concentrate in regions with the same predicted target e.g. class or regression value. \\nThis enables faster nearest neighbor search with k-d tree data structure.\\nHowever, the ultimate goal is not to find the closest data points, but to solve a classification or a regression problem.\\nThe data identity, i.e. data features, is irrelevant for the classification/regression task. \\nWhat is critical is to identify areas where all points have the same class label.\\nFor example, if a test point falls into a cluster of 1,000 points with all positive class label, then we know that its kNN will all be positive before computing the distances to the 1,000 points. \\n\\n\\n25Decision tree\\nXavier Bresson25\\nDecision tree leverages the idea that a data point has the same class label or same regression value when it falls into a cluster of same label or same regression value.\\nMajor advantage : There is no need to load the full training set for inference. \\nInstead, we can build and load a tree structure that recursively splits the feature space into regions with similar label/value. \\n\\n\\n26Construction\\nXavier Bresson26\\nDecision tree construction / training stage\\nStart from the root node of the tree that represents the entire dataset.\\nSplit this set into two halves with approximatively the same size by cutting along a feature dimension e.g. x1 with a threshold value t1. This produces two sets of data points R and L.\\nThreshold t1 and dimension x1 are chosen such that the resulting children nodes R and L are purer than their parent node S w.r.t. class label or regression value.\\nIf all points in R have the same e.g. positive class label and all points in L have also the same negative class label, then the decision tree is done.\\nIf not, the current leaf nodes are split again until all leaves are pure, i.e. all data points in the node have the same label. \\n\\n\\n27Inference\\nXavier Bresson27\\nInference with decision tree\\nOnce the tree is constructed, there is no need to keep the training set in memory.\\nWhat we need to store\\nThe tree structure which has a depth of log2n ≤ 30.\\nClass probability/regression value in the final leaf nodes.\\nIf pure classes, only class label is stored.\\nDecision tree does not require any distance computation.\\nThe cut is based on feature value.\\nHence, inference is very fast with O(log2n), independent of feature dimension d.\\n\\n\\n28Inference\\nXavier Bresson28\\nInference with decision tree\\nclass probability class label \\n\\n\\n29Optimal decision tree\\nXavier Bresson29\\nQ: Can we build a decision tree that is \\nMaximally compact, i.e. small depth.\\nOnly has pure leaves.\\nYes in theory, if no two data points have same features but different labels.\\nNo in practice, as finding a minimum size tree is NP-hard.\\nBut there exists a greedy algorithm that can approximate effectively small decision trees.\\nWe split the data recursively by minimizing a function that measures label purity in the children’s nodes.\\n\\n\\n30Purity function\\nXavier Bresson30\\n<latexit sha1_base64=\"Iy2UUiiTjzVOtspCLYbZGCc++Co=\">AAAC5nicbVLLbhMxFPUMrxIeDbBkc0VC1UrRKFNVgCpFqgQLloXQhxSnI49zp7HG85DtgY4m8wFsWIAQW76JHR+DhJ1ECFquZPn43MfxvXZcSqHNcPjT869dv3Hz1sbtzp279+5vdh88PNZFpTge8UIW6jRmGqXI8cgII/G0VMiyWOJJnL50/pP3qLQo8nemLnGasfNcJIIzY6mo+2uLGrwwKmteYWJrtJR2tsb7tNm+iMSgjsRO1IhR2J7ltB0ABctSkdOMmXkcN2/bs5ljbdyINuEgCIIBXwaui0Kf90FoMHOEvMpiVFAkwCXTGjWsxKJ05OQG9c5+PUpXMjpKqa5ijQbG7jwejaOQ8qq0Cm6DccRdcumSE8V4s7B1Fq3dFu0fcedwbTrNGTMMPggzB8lilNBP+20n6vaGwXBpcBWEa9AjazuMuj/orOBVhrlZNjEJh6WZNkwZwSW2HVppLBlP2TlOLMxZhnraLJ+phaeWmUFSKLtyA0v274yGZVrXWWwj3Xz1ZZ8j/+ebVCZ5MW1EXlYGc74SSioJpgD35jATCrmRtQWMK2HvCnzO3HDsz3BDCC+3fBUc7wbhs2DvzV7vYHc9jg3ymDwh2yQkz8kBeU0OyRHhHvc+ep+9L/7c/+R/9b+tQn1vnfOI/GP+99/e9OTO</latexit>DeﬁneS:{(xi,yi)ni=1},xi2Rd,yi={1,. . . ,c},cis the number of classesSk={(x, y):y=k},sk⇢S, S=S1[...[Scpk=|Sk||S|fraction of data with labelkWe want pure leaf nodes, i.e. The worst case is when all leaves are random prediction, i.e. To avoid the worst case, we will maximize the KL distance between the random prediction and the best candidate p obtained by splitting\\n<latexit sha1_base64=\"nO9lPCmT/4Lrb+q2k35jfqxhvsw=\">AAACLXicbVBNaxRBEO2JH4nr16pHL4UbiQdZZkJQLwsBPeSYQDYJ7CxDTW9N0kxPd9NdE1yG/UO55K+I4CEiufo3nNnsQRPf6fHeK6rq5U6rwHF8Fa3du//g4frGo97jJ0+fPe+/eHkUbO0ljaXV1p/kGEgrQ2NWrOnEecIq13Scl587//icfFDWHPLc0bTCU6MKJZFbKet/cVk5SiBl+sq+aqCwHhCCI9mFYLPcBDQzWIDLmnJrMYrfQ9pmUGsot1JjeQRlL+sP4mG8BNwlyYoMxAr7Wf97OrOyrsiw1BjCJIkdTxv0rKSmRS+tAzmUJZ7SpKUGKwrTZvntAt62ymx5aGENw1L9e6LBKoR5lbfJCvks3PY68X/epObi07RRxtVMRt4sKmoNbKGrDmbKk2Q9bwlKr9pbQZ6hR8ltwV0Jye2X75Kj7WHyYbhzsDPY3V7VsSFeizfinUjER7Er9sS+GAspLsQ3cSV+RpfRj+hXdH0TXYtWM6/EP4h+/wGqoKXf</latexit>pk= 1 for a speciﬁckandpk0=0,8k06=k\\n<latexit sha1_base64=\"ZK3/o9VEi3EthsbfwWD9XQSTvLw=\">AAACBHicbVDLSsNAFJ3UV62vqMtuBovgQkpSiroRCm5cVrAPaEKYTCftkMkkzEyEErJw46+4caGIWz/CnX/jpM1CWw9cOJxzL/fe4yeMSmVZ30ZlbX1jc6u6XdvZ3ds/MA+P+jJOBSY9HLNYDH0kCaOc9BRVjAwTQVDkMzLww5vCHzwQIWnM79UsIW6EJpwGFCOlJc+sJ1547QQC4czOM5yfQyeIBWIMhjXPbFhNaw64SuySNECJrmd+OeMYpxHhCjMk5ci2EuVmSCiKGclrTipJgnCIJmSkKUcRkW42fyKHp1oZQ71cF1dwrv6eyFAk5SzydWeE1FQue4X4nzdKVXDlZpQnqSIcLxYFKYMqhkUicEwFwYrNNEFYUH0rxFOkE1E6tyIEe/nlVdJvNe2LZvuu3ei0yjiqoA5OwBmwwSXogFvQBT2AwSN4Bq/gzXgyXox342PRWjHKmWPwB8bnD52kl2I=</latexit>pk=1c,8k\\n<latexit sha1_base64=\"HkJ71qJoYXKgJ7fFmfgWc4hUym0=\">AAACVXicbVFdSyMxFM3Mun7U3bW7PvoSLIKClBkR9aUg+CLog4JVoVOHTJppwyQzMbmzWsL8SV/Ef+KLYKYt4teFhJNzz+EmJ4kS3EAQPHn+j7mf8wuLS43lX7//rDT//rs0Rakp69JCFPo6IYYJnrMucBDsWmlGZCLYVZId1f2r/0wbXuQXMFasL8kw5ymnBBwVN0UkyX2scIQjYPegpT05rTbV9u1WJzKljG3WCasbilWc4UgUwyjVhFp3quyt27bfbPiOwwhX2LGdqSisLK0FaaGJEDhrNOJmK2gHk8JfQTgDLTSrs7j5EA0KWkqWAxXEmF4YKOhbooFTwapGVBqmCM3IkPUczIlkpm8nqVR4wzED7Ka7lQOesO8dlkhjxjJxSklgZD73avK7Xq+E9KBvea5KYDmdDkpLgaHAdcR4wDWjIMYOEKq5uyumI+IiAfcRdQjh5yd/BZc77XCvvXu+2zrcmcWxiNbQOtpEIdpHh+gYnaEuougBPXue53uP3os/589Ppb4386yiD+WvvALKCLNK</latexit>maxpKL(p, q)=cXk=1pklogpkqk,withqk=1c,8k\\n\\n31Entropy\\nXavier Bresson31\\nMaximizing the KL distance reduces to minimizing the entropy :\\n<latexit sha1_base64=\"HuIgqeP1PoIpM/bEbneQGQ2UqYI=\">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lKUY8FLx6r2A9oQ9lsN+3SzSbsToRS+g+8eFDEq//Im//GTZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNMbjO/88S1EbF6xGnC/YiOlAgFo2ilh6Q0KFfcqrsAWSdeTiqQozkof/WHMUsjrpBJakzPcxP0Z1SjYJLPS/3U8ISyCR3xnqWKRtz4s8Wlc3JhlSEJY21LIVmovydmNDJmGgW2M6I4NqteJv7n9VIMb/yZUEmKXLHlojCVBGOSvU2GQnOGcmoJZVrYWwkbU00Z2nCyELzVl9dJu1b1rqr1+3qlUcvjKMIZnMMleHANDbiDJrSAQQjP8ApvzsR5cd6dj2VrwclnTuEPnM8fDMKNAQ==</latexit>p\\n<latexit sha1_base64=\"gH9x9gJvmPQ8SJmO4b5DE2vnjLc=\">AAAB7HicbVBNS8NAEJ3Ur1q/oh69LBahXkpSinoseOmxgmkLbSib7aZdutmE3Y1QQn+DFw+KePUHefPfuGlz0NYHA4/3ZpiZFyScKe0431Zpa3tnd6+8Xzk4PDo+sU/PuipOJaEeiXks+wFWlDNBPc00p/1EUhwFnPaC2X3u956oVCwWj3qeUD/CE8FCRrA2kteuJdeVkV116s4SaJO4BalCgc7I/hqOY5JGVGjCsVID10m0n2GpGeF0URmmiiaYzPCEDgwVOKLKz5bHLtCVUcYojKUpodFS/T2R4UipeRSYzgjrqVr3cvE/b5Dq8M7PmEhSTQVZLQpTjnSM8s/RmElKNJ8bgolk5lZEplhiok0+eQju+subpNuouzf15kOz2moUcZThAi6hBi7cQgva0AEPCDB4hld4s4T1Yr1bH6vWklXMnMMfWJ8/ZZ6NuA==</latexit>H(p)\\n<latexit sha1_base64=\"8ZkcBi5RhgC6lhK3nA4F1kTFZ/g=\">AAADn3ichVJdb9MwFHUTPkb4WDce4cFQgTqxVck0MV4qTSDEpE2oSHQbqkvkuE5r1U4824FVUX4Wf4Q3/g1O0m5sXYUlW8f3nHt8c3MjyZk2vv+n4bh37t67v/bAe/jo8ZP15sbmiU4zRWifpDxVZxHWlLOE9g0znJ5JRbGIOD2Nph9K/vQHVZqlyVczk3Qo8DhhMSPY2FC40fiFDL0wSuRHx0Vbbp9vve4inYkwn3aD4juBMpxCxNMxihUmub0V+bk9tuEiD/5kZgILaKPdWhQUOSkFcaow59DmI692vXSrwE55Vjeb+1/DFSZvLk3ISkVNwwW1/NSC6Qarar0yQR4S+CKUCN7SuQWzMr+2FywpZTvLumv8YVtuXdX6MTEqlbOyEV7YbPkdv1pwGQRz0ALz1Qubv9EoJZmgiSEcaz0IfGmGOVaGEU4LD2WaSkymeEwHFiZYUD3Mq/kq4CsbGUH7O+1ODKyi/2bkWGg9E5FVCmwm+iZXBm/jBpmJ3w1zlsjM0ITUD8UZhyaF5bDCEVOUGD6zABPFbK2QTLAdCWNHumxCcPOTl8HJbid429n7stc62J23Yw08Ay9BGwRgHxyAQ9ADfUCc585758g5dl+4n9zPbq+WOo15zlNwbbnf/gKQIiDP</latexit>KL(p, q)=cXk=1pklogpkqk,withqk=1c,8k=Xpklogpk\\x00pklogqk,withqk=1c=Xpklogpk+pklogc=Xpklogpk+ logcXpk,withXpk=1=Xpklogpk+ logcmaxpKL(p, q) = maxpXpklogpk+ logc=m i np\\x00Xpklogpk=m i npH(p) Entropy\\n\\n\\n32Entropy of binary tree\\nXavier Bresson32\\nEntropy of binary tree :\\n<latexit sha1_base64=\"GbeaAL52oOz3HTk15G1sS1QdhO8=\">AAACMXicbZDLSsNAFIYn9VbrrerSTbBYEpSSlKJuCgU3XXRRq71AU8pkOmmHTi7MTISS9pXc+CbipgtF3PoSTtMgWj0w8PGf/3Dm/HZACReGMVdSa+sbm1vp7czO7t7+QfbwqMX9kCHcRD71WceGHFPi4aYgguJOwDB0bYrb9vhm0W8/YMaJ792LSYB7Lhx6xCEICin1s9WqVrto6PlyoNV0yfp5oDUkNHTLyuTLlsMgiqa16Sya3k1nsSHRGt9a7O1nc0bBiEv9C2YCOZBUvZ99tgY+Cl3sCUQh513TCEQvgkwQRPEsY4UcBxCN4RB3JXrQxbwXxRfP1DOpDFTHZ/J5Qo3VnxMRdDmfuLZ0ulCM+GpvIf7X64bCue5FxAtCgT20XOSEVBW+uohPHRCGkaATCRAxIv+qohGUgQgZ8iIEc/Xkv9AqFszLQum2lKsUkzjS4AScAg2Y4ApUQBXUQRMg8AhewCt4U56UufKufCytKSWZOQa/Svn8Akxupis=</latexit>H(L, R)=p(L)H(L)+p(R)H(R)=|L||S|H(L)+|R||S|H(R)\\n<latexit sha1_base64=\"Q8aDKqH64RSKW/Cad6UVEFPcRsI=\">AAAB7HicbVBNT8JAEJ3iF+IX6tHLRmKCF9ISgh5JvHDEaIEEGrJdtrBhu212tyak4Td48aAxXv1B3vw3bqEHBV8yyct7M5mZ58ecKW3b31Zha3tnd6+4Xzo4PDo+KZ+edVWUSEJdEvFI9n2sKGeCupppTvuxpDj0Oe35s7vM7z1RqVgkHvU8pl6IJ4IFjGBtJLddfbgujcoVu2YvgTaJk5MK5OiMyl/DcUSSkApNOFZq4Nix9lIsNSOcLkrDRNEYkxme0IGhAodUeeny2AW6MsoYBZE0JTRaqr8nUhwqNQ990xliPVXrXib+5w0SHdx6KRNxoqkgq0VBwpGOUPY5GjNJieZzQzCRzNyKyBRLTLTJJwvBWX95k3TrNadZa9w3Kq16HkcRLuASquDADbSgDR1wgQCDZ3iFN0tYL9a79bFqLVj5zDn8gfX5AzlwjZs=</latexit>H(S)\\n\\n33Information gain\\nXavier Bresson33\\nDefinition : The information that is gained by splitting a set of data points.\\nIn the case of decision tree, the splitting is controlled by a specific feature value, i.e. xi< ti.\\nThe entropy of the subsets S1,…,Sc is defined as\\nFinally, the information gain (IG) is the difference between the entropy of the original set S and the weighted sum of the entropy of the subset Sk. \\n<latexit sha1_base64=\"tSwnXzWuirttquI/+B+xXNtB6c8=\">AAACP3icbVDLSgMxFM3UV62vqks3waK0UIaZUtRNoeCmy0rtAzp1yKSZNjTzIMkIZTp/5sZfcOfWjQtF3LozfSDaeiG5h3POJbnHCRkV0jCetdTa+sbmVno7s7O7t3+QPTxqiSDimDRxwALecZAgjPqkKalkpBNygjyHkbYzup7q7XvCBQ38WzkOSc9DA5+6FCOpKDvbquUbtlnUdb3YsHHhvGKJyLPjUcVM7jAMlTgq1Ga3ZWWWVMvlCMcTJU4S1SbJjzFjZ3OGbswKrgJzAXJgUXU7+2T1Axx5xJeYISG6phHKXoy4pJiRJGNFgoQIj9CAdBX0kUdEL57tn8AzxfShG3B1fAln7O+JGHlCjD1HOT0kh2JZm5L/ad1Iule9mPphJImP5w+5EYMygNMwYZ9ygiUbK4Awp+qvEA+RSkWqyKchmMsrr4JWSTcv9PJNOVctLeJIgxNwCvLABJegCmqgDpoAgwfwAt7Au/aovWof2ufcmtIWM8fgT2lf37PVq90=</latexit>H(S1,. . . ,Sc)=cXk=1p(Sk)H(Sk)=cXk=1|Sk||S|H(Sk)\\n<latexit sha1_base64=\"TrY1pvjhvch4gRTfDBIug9dSGa8=\">AAACPnicbVBNSwMxFMz6bf2qevQSLEoLddkVUS+C4MF6U2ptoVuXbJpqaLK7JG/FsvSXefE3ePPoxYMiXj2a1gW1OpCXYd48kjdBLLgGx3m0xsYnJqemZ2Zzc/MLi0v55ZULHSWKshqNRKQaAdFM8JDVgINgjVgxIgPB6kH3aNCv3zCleRSeQy9mLUmuQt7hlICR/HzNA3YLSqYnx/1itVz13bJt2+amuLR5UClWS1umfKslz8tluqcT6afdA7d/SXFsTN1SZVg9z88XHNsZAv8lbkYKKMOpn3/w2hFNJAuBCqJ103ViaKVEAaeC9XNeollMaJdcsaahIZFMt9Lh+n28YZQ27kTKnBDwUP05kRKpdU8GxikJXOvR3kD8r9dMoLPfSnkYJ8BC+vVQJxEYIjzIEre5YhREzxBCFTd/xfSaKELBJJ4zIbijK/8lF9u2u2vvnO0UDrezOGbQGlpHReSiPXSIKugU1RBFd+gJvaBX6956tt6s9y/rmJXNrKJfsD4+ASbNqIM=</latexit>IG(S, S1,. . . ,Sc)=H(S)\\x00H(S1,. . . ,Sc)=H(S)\\x00cXk=1p(Sk)H(Sk)\\n\\n34Feature and threshold selection\\nXavier Bresson34\\nGoal is to find subsets that maximizes the information gain, achieving the purest possible subsets. \\nIdentifying the purest subsets is to find a feature xi  and a threshold value ti. \\nDecision tree construction (pseudo-code)\\nWhile leaf nodes are not pure (or ≥ threshold)\\nLoop over (remaining) feature dimensions, i.e. x1,x2,…,xd\\nLoop over n thresholds (e.g. middle points between two consecutive points,            such as ti=(xi+1-xi)/2)\\nCompute information gain for R and L\\nSave (dimension, threshold value) with maximum information gain.\\nSplit space with best (dimension, threshold value) and remove dimension xi from loop.\\nExact complexity is O(n.d). Approximations are used in practice for speed-up.\\n\\n\\n35Regression tree\\nXavier Bresson35\\nIt is straightforward to extend decision tree to other task s.a. regression as long as a purity function can be defined for the new task :\\n<latexit sha1_base64=\"rGtW0UTPSe1NHJ/bvYDx+QOZ1Zs=\">AAADI3icbVLLbhMxFPUMrxIeTWHJ5oqIKpFoNBNVPBZIlbrpgkUhpK0Uh5HHcRIrY89ge9qOpvMvbPgVNixAFRsW/Aue6TSEhCtZOj73nnvsa4dJxLXxvF+Oe+Pmrdt3Nu427t1/8HCzufXoSMepomxA4yhWJyHRLOKSDQw3ETtJFCMijNhxON8v88enTGkeyw8mS9hIkKnkE06JsVSw5bzGhp0bJfKDot3vvNnBOhWQBHPAUTytwDb+lJIxYMWnM0OUis+gZmrl20oJeKIIzf0iv+hfFGWbIG+fP886mEvoF9DOdnBIVJ4VQb/zsbdQQ/uUKE4kZZ0C48Y2numEUJb7VBSLojNuZmC31w1gxQ7W/bIlB8GIrLovqP2IaL2YAxii57a/vStc+/eW/eE9myqm9d/aRiNotryuVwWsA78GLVTHYdC8xOOYpoJJQ0v7oe8lZpQTZTiNWNHAqWbWek6mbGihJILpUV69cQHPLDOGSazskgYqdlmRE6F1JkJbKYiZ6dVcSf4vN0zN5NUo5zJJDZP0ymiSRmBiKD8MjLli1ESZBYQqbs8KdEbs7I39VuUQ/NUrr4OjXtd/0d19t9va69Xj2EBP0FPURj56ifbQATpEA0Sdz85X57vzw/3ifnMv3Z9Xpa5Tax6jf8L9/Qf0xwCb</latexit>H(S)=\\x00Xpklogpk!L(S)=1|S|X(x,y)2S(y\\x00¯yS)2(variance)with ¯yS=1|S|X(x,y)2Sy(mean)Classiﬁcation task Regression task\\nRegression function (blue) learned with regression treeTrue regression function (black)\\n\\n\\n36\\n<latexit sha1_base64=\"Q6+CjJBvO6ywHSv3aOpml5SSwyA=\">AAACPnicbVAxTxsxGPVR2tJAS2jHLp8aVQpLdBch6IKExNINkAggxenJ5/guFj7bsr9rFZ3yy1j4DWwdWRiKECtjfSFDCzzJ0tN775O/72VWSY9x/DtaerX8+s3blXet1bX3H9bbGx9PvKkcFwNulHFnGfNCSS0GKFGJM+sEKzMlTrPz/cY//Smcl0Yf49SKUckKLXPJGQYpbQ+or8rUAoUDmsmiCzR3jNd6Vvd/2BlQlKXwEDg09ibswkHX6s2Qp04WE2TOmV/NdFdTZYq0D8Fspe1O3IvngOckWZAOWeAwbV/RseFVKTRyxbwfJrHFUc0cSq7ErEUrLyzj56wQw0A1C1uN6vn5M/galDHkxoWnEebqvxM1K72flllIlgwn/qnXiC95wwrzb6Naaluh0Pzxo7xSgAaaLmEsneCopoEw7mTYFfiEhf4wNN6UkDw9+Tk56feS7d7W0VZnr7+oY4V8Jl9IlyRkh+yR7+SQDAgnF+Sa/CG30WV0E91F94/RpWgx84n8h+jhLxaDq+8=</latexit>XpO\\x00n2p⇥2p\\x00=O(pn)!O(nlog2n)\\n<latexit sha1_base64=\"TcbnvN/dWjvqNBt8HNUeigQ8yyQ=\">AAACJXicbVDLSgMxFM34rONr1KWbYBHaTZkpRV0oCG7ctYKthU4tmTRtg5lkSO6oZejPuPFX3LiwiODKXzGtXfg6cOHknHvJvSdKBDfg++/O3PzC4tJybsVdXVvf2PS2thtGpZqyOlVC6WZEDBNcsjpwEKyZaEbiSLCr6OZs4l/dMm24kpcwTFg7Jn3Je5wSsFLHO64WytdJEYc41Lw/AKK1urOvakFaEdg96DjDdxwGeISTk2ohFKrfKWNZdF234+X9kj8F/kuCGcmjGWodbxx2FU1jJoEKYkwr8BNoZ0QDp4KN3DA1LCH0hvRZy1JJYmba2fTKEd63Shf3lLYlAU/V7xMZiY0ZxpHtjAkMzG9vIv7ntVLoHbUzLpMUmKRfH/VSgUHhSWS4yzWjIIaWEKq53RXTAdGEgg12EkLw++S/pFEuBQelykUlf1qexZFDu2gPFVCADtEpOkc1VEcUPaAn9ILGzqPz7Lw6b1+tc85sZgf9gPPxCSV3oeM=</latexit>O(2p)!O(n)w i t hp=O(log2n)Complexity\\nXavier Bresson36\\nTraining / building decision tree\\nSpace/memory complexity (worst case) : \\nTime/speed complexity (worst case) :\\nInference / NN search\\nTime/speed complexity :\\n#nodes in the tree\\npurity time\\n<latexit sha1_base64=\"wuYBHcq5+53z+6H78PGk0mkQ1Qc=\">AAACCXicbVC7TsMwFHV4lvIKMLJYVEhlqZKqAsZKLGwUiT6kJooc12mtOnZkO6Aq6srCr7AwgBArf8DG3+C0GaDlSFc6Pude+d4TJowq7Tjf1srq2vrGZmmrvL2zu7dvHxx2lEglJm0smJC9ECnCKCdtTTUjvUQSFIeMdMPxVe5374lUVPA7PUmIH6MhpxHFSBspsGECPehJOhxpJKV4MC94U/WYGAZ1yM9gObArTs2ZAS4TtyAVUKAV2F/eQOA0JlxjhpTqu06i/QxJTTEj07KXKpIgPEZD0jeUo5goP5tdMoWnRhnASEhTXMOZ+nsiQ7FSkzg0nTHSI7Xo5eJ/Xj/V0aWfUZ6kmnA8/yhKGdQC5rHAAZUEazYxBGFJza4Qj5BEWJvw8hDcxZOXSadec89rjdtGpVkv4iiBY3ACqsAFF6AJrkELtAEGj+AZvII368l6sd6tj3nrilXMHIE/sD5/AOKHl9g=</latexit>p!O(log2n)\\n\\n37Outline\\nXavier Bresson37\\nkNN\\nk-d tree\\nDecision tree\\nBagging\\nBoosting\\nConclusion\\n\\n\\n38Bagging\\nXavier Bresson38\\nDecision trees have great advantages at inference\\nSpeed complexity is very fast, i.e. O(log2n)≤ 30\\nMemory complexity is low, i.e. O(n) independent of d\\nHowever, these techniques have high variance performance.\\nThis means that the quality of the classification/regression solutions vary significantly (see next slide).\\nThey are known as weak learners (classifiers or regressors).\\n\\n\\n39Bias and variance (week 5)\\nXavier Bresson39\\nQuality of predictive models are evaluated by their bias-variance properties.\\nFor example, let assume that the task of the model is to predict the red center of the target below \\nLow variance and low biasThe perfect model!High variance and low biasThe model is able to find the correct solution is average.Low variance and high biasThe model favors more some solutions, far from the true ones.Low variance and low biasThe worst model!The model has not only bad bias but also large variance results.\\n\\n40\\n<latexit sha1_base64=\"ZjmUFJx1tSXIA4YEAQqx6rWGEjM=\">AAADG3icnVJNa9RAGJ7Ej9b4ta1HL4OLNmvrkiyleikURPBYWbct7KRhZnayOzRfzLypG0L+h5f+lV48KOJJ8OC/cZKuWtuK4AsDD8/zPvN+zLA8lho877tlX7t+4+bS8i3n9p279+53Vlb3dFYoLkY8izN1wKgWsUzFCCTE4iBXgiYsFvvs6GWj7x8LpWWWvoUyF0FCp6mMJKdgqHDF8p0nJKEwY6x6VYeVO98oezUmTE7H2I3CoTvvPSt7h4OWCkgbzjYm+N82wqiqotrA9d/w3GUbmICYg0oq/E7CDDf+n2nbRKYQVsM1ogumBeDh4dMaRw3TyLk7XOtNjPp//fxqYf3vpitbNrWcsNP1+l4b+DLwF6CLFrEbdr6SScaLRKTAY6r12PdyCCqqQPJY1A4ptMgpP6JTMTYwpYnQQdW+bY0fG2aCo0yZkwJu2fOOiiZalwkzmc0o+qLWkFdp4wKiF0El07wAkfKzQlERY8hw81HwRCrBIS4NoFxJ0yvmM6ooB/OdmiX4F0e+DPYGfX+rv/lms7szWKxjGT1Ej5CLfPQc7aDXaBeNELfeW6fWR+uTfWJ/sD/bX85SbWvheYD+CPvbD1+C85w=</latexit>E(x,y)⇥(fS(x)\\x00y)2⇤=E(x,y)⇥(fS(x)\\x00¯f(x)+¯f(x)\\x00y)2⇤,with¯f(x)=ZS0⇢S⇤fS0(x)p(S0)dS0=E(x,y)⇥(fS(x)\\x00¯f(x))2⇤+E(x,y)⇥(¯f(x)\\x00y)2⇤Bias and variance\\nXavier Bresson40\\nLong history of analysis of the bias-variance trade-off (but recently questioned by deep learning).\\nIt is highly challenging to design the perfect model (i.e. low bias and low variance).\\nFormalization\\ndatatargetAssumption: no noise\\nPredictive model\\nAverage predictor\\nError between prediction model and average over all predictorsVarianceError between average predictor and targetBias\\nTrue data distribution\\nMean over data points\\n\\n\\n41Reducing variance\\nXavier Bresson41\\nDecision trees have low bias but high variance, i.e. solutions vary around the true solution.\\nGoal : Design a technique that reduces variance, i.e. \\nMost common idea is to take the average of multiple solutions, a.k.a. ensemble technique :\\n<latexit sha1_base64=\"FfQKEmQqwMCZD6ucfpbGumeefJw=\">AAACJHicbVDLSgMxFM3Ud32NunQTLEILWmZKUcGNIIJLRWuFzjgkaaaGZjJDkhHLMB/jxl9x48IHLtz4LWZqF9p6IHA4515uzsEJZ0o7zqdVmpqemZ2bXygvLi2vrNpr61cqTiWhLRLzWF5jpChngrY005xeJ5KiCHPaxv3jwm/fUalYLC71IKF+hHqChYwgbaTAPvQiJjzoRUjfYpyd5EFWvd8Z1HLoYdbrwGoYXFTva7seRjILc0NrN42h55cDu+LUnSHgJHFHpAJGOAvsN68bkzSiQhOOlOq4TqL9DEnNCKd52UsVTRDpox7tGCpQRJWfDUPmcNsoXRjG0jyh4VD9vZGhSKlBhM1kEUaNe4X4n9dJdXjgZ0wkqaaC/BwKUw51DIvGYJdJSjQfGIKIZOavkNwiiYg2vRYluOORJ8lVo+7u1ZvnzcpRY1THPNgEW6AKXLAPjsApOAMtQMADeAIv4NV6tJ6td+vjZ7RkjXY2wB9YX99zZKK/</latexit>minE(x,y)⇥(fS(x)\\x00¯f(x))2⇤\\n<latexit sha1_base64=\"Y+a+L55tvrlRX6AmuOTC7IITubQ=\">AAACunicbVFNa9tAEF2pX6n75bbHXoaaBqcEYYXQ5NBAoJceU1wnAa8jRuuVvbFWErujxEboR7a3/puuFJU2SQcWHm/ezOy8iYtUWRqNfnn+g4ePHj/Zetp79vzFy1f9129ObV4aISciT3NzHqOVqcrkhBSl8rwwEnWcyrN49aXJn11JY1WefadNIWcaF5lKlEByVNT/kUTj4XoHOBaFydfAl0hVUjfU9hHwxKCowrrSNXBb6qi6PArrCw1JVI2jy1bGjVosCY3Jr4HHaLpqDpzkmoyuAC24ctC3lCpLaMN5b/uP7HopjWyErnO4GwTB7jjSvJ0bW0kwvvj4t+WQTClhjoQwdyYZFZfNPjtQ96L+YBSM2oD7IOzAgHVxEvV/8nkuSi0zEilaOw1HBc0qNKREKuseL60sUKxwIacOZqilnVWt9TV8cMwckty4lxG07L8VFWprNzp2So20tHdzDfm/3LSk5HBWqawoSWbiZlBSpkA5NHd0axspKN04gMIo91cQS3TnInftxoTw7sr3weleEH4K9r/tD473Oju22Dv2ng1ZyA7YMfvKTtiECe/Am3mJt/A/+7Gv/NWN1Pe6mrfsVvj0Gzkj1kY=</latexit>fS(x)⇡ˆf(x)=1mmXj=1fSj(x)!¯f(x) asm!1where{S1,. . . ,Sm}⇢S⇤(true data distribution)\\n\\n42Reducing variance\\nXavier Bresson42\\nWhy averaging classifiers reduces variance ? \\nBecause of the law of large numbers :The law of large numbers states for i.i.d.(independent and identically distributed)       random variable xiwith mean    :\\nApply to learners : Assume we have m training datasets S1,…,Smsampled from S*, the true data distribution.Train a learner on each training set and average the result:\\n<latexit sha1_base64=\"nOdTILUR2vDYGAohPMuHCzPoYs0=\">AAAB73icbVBNS8NAEJ34WetX1aOXxSJ4Kkkp6rHgxWMF+wFtKJvttl262cTdiVhC/4QXD4p49e9489+4aXPQ1gcDj/dmmJkXxFIYdN1vZ219Y3Nru7BT3N3bPzgsHR23TJRoxpsskpHuBNRwKRRvokDJO7HmNAwkbweTm8xvP3JtRKTucRpzP6QjJYaCUbRSpxdQnT7Niv1S2a24c5BV4uWkDDka/dJXbxCxJOQKmaTGdD03Rj+lGgWTfFbsJYbHlE3oiHctVTTkxk/n987IuVUGZBhpWwrJXP09kdLQmGkY2M6Q4tgse5n4n9dNcHjtp0LFCXLFFouGiSQYkex5MhCaM5RTSyjTwt5K2JhqytBGlIXgLb+8SlrVindZqd3VyvVqHkcBTuEMLsCDK6jDLTSgCQwkPMMrvDkPzovz7nwsWtecfOYE/sD5/AHZL4/O</latexit>¯x\\n<latexit sha1_base64=\"k7Ce2MqYdmFNS435bINH7vYM5JE=\">AAACPHicbVA9TyMxFPQCx0GOgwAljUV0ElW0ixDQnIREQwmCAFI2rN463sTC9q7st3eJrP1hNPwIOioaChCipcYJKfgaydJoZp6e36SFFBbD8DaYmp75Mftzbr72a+H34lJ9eeXU5qVhvMVymZvzFCyXQvMWCpT8vDAcVCr5WXq5P/LP/nFjRa5PcFjwjoKeFplggF5K6sdxZoC5qHKqorEtVeLE36i6UHSQCBob0esjGJP/p3EKxg18CPkAjXIULK2o+hAROsNhLak3wmY4Bv1KoglpkAkOk/pN3M1ZqbhGJsHadhQW2HFgUDDJq1pcWl4Au4Qeb3uqQXHbcePjK/rHK12a5cY/jXSsvp9woKwdqtQnFWDffvZG4ndeu8Rst+OELkrkmr0tykpJMaejJmlXGM5QDj0BZoT/K2V98G2i73tUQvT55K/kdLMZbTe3jrYae5uTOubIGlknGyQiO2SPHJBD0iKMXJE78kAeg+vgPngKnt+iU8FkZpV8QPDyCqpxr4Y=</latexit>1mmXi=1xi!¯xasm!1\\n<latexit sha1_base64=\"GJpOyExZki/dqvaa+U/yOLuToS8=\">AAACSnicbVBNSyMxGM5UXbXqbnWPewkWwVOZEdndiyB48ajsVoVOHd5JM200yQzJO2oJ8/u8ePLmj/DiQVn2Yqb24NcDgYfngyRPWkhhMQzvgsbM7NyX+YXF5tLyytdvrdW1I5uXhvEuy2VuTlKwXArNuyhQ8pPCcFCp5Mfp+V7tH19wY0Wu/+K44H0FQy0ywQC9lLQgHgG6rKI7NM4MMBdVTlU0tqVK3NlOVJ0qmiXuT3LmRSOGIwRj8ksap2DqWoz8Co1yFCytqHoTETrDcTNptcNOOAH9SKIpaZMpDpLWbTzIWam4RibB2l4UFth3YFAwyatmXFpeADuHIe95qkFx23eTKSq64ZUBzXLjj0Y6UV83HChrxyr1SQU4su+9WvzM65WY/e47oYsSuWYvF2WlpJjTelc6EIYzlGNPgBnh30rZCPyi6NevR4jef/kjOdrqRD8724fb7d2t6RwL5AdZJ5skIr/ILtknB6RLGLkm9+SRPAU3wUPwL/j/Em0E08538gaN2WcIgbQP</latexit>ˆf=1mmXj=1fSj!¯fasm!1\\n\\n\\n43Bagging\\nXavier Bresson43\\nAveraging several classifiers/regressors will decrease the variance and make the ensemble classifier/regressor more accurate.\\nBut we do not have access to more training sets {S1,..,Sm} than the original set S.\\nBecause we do not know the true distribution S*.\\nHow do we create new training sets? \\nSolution is bagging.\\nBagging algorithm\\nSample m datasets S1,..,Sm from original S with replacement.\\nFor each training set Sj, train a classifier fSj.\\nFinal/ensemble classifier is \\n<latexit sha1_base64=\"FpXhZZpXdyZE54fgmsoijfNj8LE=\">AAACGXicbVDLSgMxFM3UV62vqks3wSLUTZkpRd0UCm5cVrQP6NQhk2batMnMkGTEEuY33Pgrblwo4lJX/o3pY6GtBwKHc87l5h4/ZlQq2/62Miura+sb2c3c1vbO7l5+/6Apo0Rg0sARi0TbR5IwGpKGooqRdiwI4j4jLX90OfFb90RIGoW3ahyTLkf9kAYUI2UkL2+7A6R0kBYfTmEVuoFAWDup5il0ZcI9Paw66R2HgadvvOEk5eULdsmeAi4TZ04KYI66l/90exFOOAkVZkjKjmPHqquRUBQzkubcRJIY4RHqk46hIeJEdvX0shSeGKUHg0iYFyo4VX9PaMSlHHPfJDlSA7noTcT/vE6igouupmGcKBLi2aIgYVBFcFIT7FFBsGJjQxAW1PwV4gEy7ShTZs6U4CyevEya5ZJzVqpcVwq18ryOLDgCx6AIHHAOauAK1EEDYPAInsEreLOerBfr3fqYRTPWfOYQ/IH19QNPup/U</latexit>ˆf(x)=1mmXj=1fSj(x)\\n\\n44Sampling with replacement\\nXavier Bresson44\\nWhat is sampling with replacement?\\nWhen a data is selected, it continues to be part of the set and can be sampled again (unlike sampling without replacement, once a data is selected then it is removed from the set and cannot be sampled again). \\nSampling with replacementSampling without replacement\\n\\n\\n45Bagging\\nXavier Bresson45\\nSampling with replacement breaks the assumption of the law of large numbers as training sets Sj have data not i.i.d. and therefore, there is no theoretical guarantee that the following convergence is true anymore :\\nHowever, in practice, bagging reduces the variance quite effectively.\\nBut after a large number m, there will be a diminishing return.\\nImportantly, bagging can reduce the variance without increasing the error of an unbiased classifier.\\nUnbiased classifier produces the correct solution is average.\\n<latexit sha1_base64=\"mPxIKr052/HW4uVSE6SXhe2f0J0=\">AAACKXicbVBNa9tAFFw5beO6H3GSYy9LTaEnIxnT5mIw9JJjSurYYLniab2y196VxO5Tiln0d3LJX8klhZak1/6RrmwfWrsDC8PMG96+iXMpDPr+o1c7ePL02WH9eePFy1evj5rHJ1cmKzTjA5bJTI9iMFyKlA9QoOSjXHNQseTDePmp8ofXXBuRpV9wlfOJglkqEsEAnRQ1++Ec0CYl7dEw0cBsUFpV0tAUKrKLXlB+VTSJ7GW0cKIWszmC1tk3Gsagq1gjarb8tr8G3SfBlrTIFhdR83s4zViheIpMgjHjwM9xYkGjYJKXjbAwPAe2hBkfO5qC4mZi15eW9J1TpjTJtHsp0rX6d8KCMmalYjepAOdm16vE/3njApOziRVpXiBP2WZRUkiKGa1qo1OhOUO5cgSYFu6vlM3BFYau3KqEYPfkfXLVaQcf2t3P3Va/s62jTt6Qt+Q9CchH0ifn5IIMCCM35I78ID+9W+/ee/B+bUZr3jZzSv6B9/sPvQim4A==</latexit>ˆf=1mmXj=1fSj!¯f\\n<latexit sha1_base64=\"mPxIKr052/HW4uVSE6SXhe2f0J0=\">AAACKXicbVBNa9tAFFw5beO6H3GSYy9LTaEnIxnT5mIw9JJjSurYYLniab2y196VxO5Tiln0d3LJX8klhZak1/6RrmwfWrsDC8PMG96+iXMpDPr+o1c7ePL02WH9eePFy1evj5rHJ1cmKzTjA5bJTI9iMFyKlA9QoOSjXHNQseTDePmp8ofXXBuRpV9wlfOJglkqEsEAnRQ1++Ec0CYl7dEw0cBsUFpV0tAUKrKLXlB+VTSJ7GW0cKIWszmC1tk3Gsagq1gjarb8tr8G3SfBlrTIFhdR83s4zViheIpMgjHjwM9xYkGjYJKXjbAwPAe2hBkfO5qC4mZi15eW9J1TpjTJtHsp0rX6d8KCMmalYjepAOdm16vE/3njApOziRVpXiBP2WZRUkiKGa1qo1OhOUO5cgSYFu6vlM3BFYau3KqEYPfkfXLVaQcf2t3P3Va/s62jTt6Qt+Q9CchH0ifn5IIMCCM35I78ID+9W+/ee/B+bUZr3jZzSv6B9/sPvQim4A==</latexit>ˆf=1mmXj=1fSj!¯f?\\n\\n46Bagging\\nXavier Bresson46\\nAdvantages\\nEasy to implement\\nEasy to reduces variance for high variance classifiers/regressors.\\nBagging also provides an error estimate of the test error (for free).\\nDuring sampling Sj, some training data xk will not be selected and hence can act as a test data for the ensemble of classifiers. \\n\\n\\n47Random forest\\nXavier Bresson47\\nOne of the most popular and useful bagging algorithms is random forest.\\nRandom forest is an ensemble of decision trees.\\nAlgorithm\\nSample m training datasets S1,…,Sm from S with replacement.\\nFor each Sj, train a decision tree fSj with one important modification :\\nOnly consider a randomly small number of k of splits with k≤ d features. \\nGoal is to make sure that all classifiers fSj are all very different. As such, they will make different errors at test time, but averaging will correct most of the errors.\\nFinal classifier is \\nHyper-parameters \\n           is a good heuristic\\nm is as large as computational resource permits\\n<latexit sha1_base64=\"FpXhZZpXdyZE54fgmsoijfNj8LE=\">AAACGXicbVDLSgMxFM3UV62vqks3wSLUTZkpRd0UCm5cVrQP6NQhk2batMnMkGTEEuY33Pgrblwo4lJX/o3pY6GtBwKHc87l5h4/ZlQq2/62Miura+sb2c3c1vbO7l5+/6Apo0Rg0sARi0TbR5IwGpKGooqRdiwI4j4jLX90OfFb90RIGoW3ahyTLkf9kAYUI2UkL2+7A6R0kBYfTmEVuoFAWDup5il0ZcI9Paw66R2HgadvvOEk5eULdsmeAi4TZ04KYI66l/90exFOOAkVZkjKjmPHqquRUBQzkubcRJIY4RHqk46hIeJEdvX0shSeGKUHg0iYFyo4VX9PaMSlHHPfJDlSA7noTcT/vE6igouupmGcKBLi2aIgYVBFcFIT7FFBsGJjQxAW1PwV4gEy7ShTZs6U4CyevEya5ZJzVqpcVwq18ryOLDgCx6AIHHAOauAK1EEDYPAInsEreLOerBfr3fqYRTPWfOYQ/IH19QNPup/U</latexit>ˆf(x)=1mmXj=1fSj(x)\\n<latexit sha1_base64=\"5P5QHk8gm3/RnTz6j8rrw7cRcKQ=\">AAAB8nicbVBNS8NAEN3Ur1q/qh69LBbBU0lKUS9CwYvHCrYV2lA2m027dLMbdydCCf0ZXjwo4tVf481/46bNQVsfDDzem2FmXpAIbsB1v53S2vrG5lZ5u7Kzu7d/UD086hqVaso6VAmlHwJimOCSdYCDYA+JZiQOBOsFk5vc7z0xbbiS9zBNmB+TkeQRpwSs1J9cD8yjhiycVYbVmlt358CrxCtIDRVoD6tfg1DRNGYSqCDG9D03AT8jGjgVbFYZpIYlhE7IiPUtlSRmxs/mJ8/wmVVCHCltSwKeq78nMhIbM40D2xkTGJtlLxf/8/opRFd+xmWSApN0sShKBQaF8/9xyDWjIKaWEKq5vRXTMdGEgk0pD8FbfnmVdBt176LevGvWWo0ijjI6QafoHHnoErXQLWqjDqJIoWf0it4ccF6cd+dj0Vpyiplj9AfO5w8Uv5EV</latexit>k=pd\\n\\n48Random forest\\nXavier Bresson48\\nExample with two-moon binary classification \\n\\n\\n49Outline\\nXavier Bresson49\\nkNN\\nk-d tree\\nDecision tree\\nBagging\\nBoosting\\nConclusion\\n\\n\\n50Boosting\\nXavier Bresson50\\nLet us consider the case where classifiers/regressors have high bias, i.e. these prediction models have large errors on the training set. \\nAn example of such high-bias models are decision trees with limited depth, e.g. value 4.\\nQ: Can we design an ensemble method that combines a large number of weak learners to generate a strong learner with low bias? \\nYes, this class of algorithms is called boosting.\\nBoosting reduces bias.\\n\\n\\n51Boosting\\nXavier Bresson51\\nVanilla boosting algorithm\\nAssume we have an ensemble classifier at step t= T, i.e.\\nPrediction error is defined with a loss function as L(f) as\\nWe would like to add a new (weak) learner f to FTto decrease the prediction error as much as possible. The weak learner f is selected by minimizing the following loss :\\nAfter we found the new learner ft+1, we simply add it to FT :\\n<latexit sha1_base64=\"zVjkc7MfIsRScBg16DlpXMV2KAQ=\">AAACG3icbVDLSsNAFJ3UV42vqks3g0VoQUpSiropFNy4cFHBPqCJYTKdtEMnkzAzEUvIf7jxV9y4UMSV4MK/cfpYaOuBezmccy8z9/gxo1JZ1reRW1ldW9/Ib5pb2zu7e4X9g7aMEoFJC0csEl0fScIoJy1FFSPdWBAU+ox0/NHlxO/cEyFpxG/VOCZuiAacBhQjpSWvUL0uBWVYh04gEE7tLOUZdGQSeimt29kdhw5hrBSUHjxaPh3rZkKvULQq1hRwmdhzUgRzNL3Cp9OPcBISrjBDUvZsK1ZuioSimJHMdBJJYoRHaEB6mnIUEumm09syeKKVPgwioYsrOFV/b6QolHIc+noyRGooF72J+J/XS1Rw4aaUx4kiHM8eChIGVQQnQcE+FQQrNtYEYUH1XyEeIp2S0nGaOgR78eRl0q5W7LNK7aZWbFTnceTBETgGJWCDc9AAV6AJWgCDR/AMXsGb8WS8GO/Gx2w0Z8x3DsEfGF8/P2WfAg==</latexit>L(f)=1nnXi=1`(f(xi),yi)\\n<latexit sha1_base64=\"SQDm+1fzBD5uL+imfQYr0toqYZE=\">AAACPXicbVBNSxxBEO1Z86GTr9UcvTQugQ2GZUYkyUURAsGDB4VdXdgZhprent3G7p6huya4DPPHvPgfvHnLJYeIePVq70cgWfOg4fWrelTVSwspLAbBjddYefb8xcvVNf/V6zdv3zXXN05tXhrGeyyXuemnYLkUmvdQoOT9wnBQqeRn6fm3af3sBzdW5LqLk4LHCkZaZIIBOilpdrOkwu2w3ouQX6BRFZiRErpOqiwSmh7WNKJH7e9JdzsCWYyBZh8/OWn+2Q/oHxsFPaRWgZS1T5NmK+gEM9CnJFyQFlngOGleR8OclYprZBKsHYRBgbHbBQWTvPaj0vIC2DmM+MBRDYrbuJpdX9MPThnSLDfuaaQz9W9HBcraiUpdpwIc2+XaVPxfbVBi9jWuhC5K5JrNB2WlpJjTaZR0KAxnKCeOADPC7UrZGAwwdIH7LoRw+eSn5HSnE37u7J7stg52FnGskk2yRdokJF/IATkkx6RHGLkkP8lvcutdeb+8O+9+3trwFp735B94D48Baa0v</latexit>ft+1= argminf2HL(FT+↵f),↵>0 and small\\n<latexit sha1_base64=\"n4pEdr0iJ3ql8g3Zg6Brvw68434=\">AAACDHicbVDLSsNAFL2prxpfVZduBosgFEpSiroRCoK4rNAXtCFMppN26OTBzEQooR/gxl9x40IRt36AO//GSZuFth4YOJxzLnfu8WLOpLKsb6Owtr6xuVXcNnd29/YPSodHHRklgtA2iXgkeh6WlLOQthVTnPZiQXHgcdr1JjeZ332gQrIobKlpTJ0Aj0LmM4KVltxS+dZNWxV7hq5RxmaoggaYx2OMfDdV2jBNpFNW1ZoDrRI7J2XI0XRLX4NhRJKAhopwLGXftmLlpFgoRjidmYNE0hiTCR7RvqYhDqh00vkxM3SmlSHyI6FfqNBc/T2R4kDKaeDpZIDVWC57mfif10+Uf+WkLIwTRUOyWOQnHKkIZc2gIROUKD7VBBPB9F8RGWOBidL9mboEe/nkVdKpVe2Lav2+Xm7U8jqKcAKncA42XEID7qAJbSDwCM/wCm/Gk/FivBsfi2jByGeO4Q+Mzx8Suphr</latexit>FT+1=FT+↵ft+1H: hypothesis space<latexit sha1_base64=\"gp0VPBY6JWbRehzv/z3hwpuYjHM=\">AAACHnicbVDJSgNBEO1xjXGLevTSGIQIEmZCXC6RgCAeI2SDTBx6Oj1Jk56F7hoxDPkSL/6KFw+KCJ70b+wsoCY+KHi8V0VVPTcSXIFpfhkLi0vLK6uptfT6xubWdmZnt67CWFJWo6EIZdMligkesBpwEKwZSUZ8V7CG278c+Y07JhUPgyoMItb2STfgHqcEtORkTq6cau7+CJewrWLfSaBkDW+r2CYi6hEHsOeAto/tH+XCTKedTNbMm2PgeWJNSRZNUXEyH3YnpLHPAqCCKNWyzAjaCZHAqWDDtB0rFhHaJ13W0jQgPlPtZPzeEB9qpYO9UOoKAI/V3xMJ8ZUa+K7u9An01Kw3Ev/zWjF45+2EB1EMLKCTRV4sMIR4lBXucMkoiIEmhEqub8W0RyShoBMdhWDNvjxP6oW8dZov3hSz5cI0jhTaRwcohyx0hsroGlVQDVH0gJ7QC3o1Ho1n4814n7QuGNOZPfQHxuc3dJWfhw==</latexit>FT(x)=TXt=1↵tft(x),↵t>0\\n\\n52Gradient boosting\\nXavier Bresson52\\nHow do we solve the optimization problem?\\nWe use a first-order Taylor approximation of L : \\n<latexit sha1_base64=\"IxDnNzEZ+dVlhVaCefiNl+5X91w=\">AAACG3icbVBNSwMxEM36bf2qevQSLIIilN1S1IsgCNKDhwptFbplyabZNphkl2RWLMv+Dy/+FS8eFPEkePDfmH4ctPpg4PHeDDPzwkRwA6775czMzs0vLC4tF1ZW19Y3iptbLROnmrImjUWsb0JimOCKNYGDYDeJZkSGgl2Ht+dD//qOacNj1YBBwjqS9BSPOCVgpaBYiYIMDr381Ad2D1pmRPckV3mQRT5XuJZjH1/uXwSNQ5+IpE9wdBAUS27ZHQH/Jd6ElNAE9aD44XdjmkqmgApiTNtzE+jYTcCpYHnBTw1LCL0lPda2VBHJTCcb/ZbjPat0cRRrWwrwSP05kRFpzECGtlMS6Jtpbyj+57VTiE46GVdJCkzR8aIoFRhiPAwKd7lmFMTAEkI1t7di2ieaULBxFmwI3vTLf0mrUvaOytWraumsMoljCe2gXbSPPHSMzlAN1VETUfSAntALenUenWfnzXkft844k5lt9AvO5zenK6B1</latexit>ft+1= argminf2HL(FT+↵f)\\n<latexit sha1_base64=\"W7m6Bcmnu45n2/7+pR0fDJ157uc=\">AAACfXicdVFda9RAFJ3Erxq/Vn305eJS2bUlJKVYX4SCUHzoQwW3Leys4WZ2sjt0MhlmJtIl5F/4y3zzr/iikzSgtnphmMM598zHubmWwrok+R6Et27fuXtv63704OGjx09GT5+d2qo2jM9YJStznqPlUig+c8JJfq4NxzKX/Cy/eN/pZ1+4saJSn9xG80WJKyUKwdB5Kht9PZ4c7VCUeo1QTOEVRa1NdQmensIODAqVqFaSA1WYS+zF3QKo6VlKo//ZbF1mjXiXtp8V0MIga6hG4wRKOG5/46PJZSambVz0exRlo3ESJ33BTZAOYEyGOslG3+iyYnXJlWMSrZ2niXaLpjueSd5GtLZcI7vAFZ97qLDkdtH06bWw7ZklFJXxSzno2T8dDZbWbsrcd5bo1va61pH/0ua1K94uGqF07bhiVxcVtQRXQTcKWArDmZMbD5AZ4d8KbI0+JecH1oWQXv/yTXC6F6dv4v2P++PDvSGOLfKCvCQTkpIDckg+kBMyI4z8CCCYBq+Dn+F2uBvGV61hMHiek78qPPgFuxG8Fw==</latexit>L(F+↵f)⇡L(F)+↵hrL(F),fi⇡L(F)+↵nXi=1@L@F(xi).f(xi)gradient\\n<latexit sha1_base64=\"NWIP2zsXOKvrdLfnLIlKzCYcZgw=\">AAACtHicbVFba9swFJa9W+ddmm6PezksrDgPC3YoXRkECqNjDx10bGkKUebJipyISrKR5C3B+BfubW/7N5PjQLukBwQf3+Uc6SgtBDc2iv56/r37Dx4+2nscPHn67Pl+5+DFpclLTdmI5iLXVykxTHDFRpZbwa4KzYhMBRun1x8affyTacNz9c2uCjaVZK54ximxjko6v7FlS6tldbYkshAMfnG7gM9fz+B9DRgOz8OP4TLhvd4QZ5rQKq6rQY1TPg+hFd6uEg4N0fs+AIyDQ2iNuCDaciLgvL7BbaSGIdzdor+VDW8Mvd02w9v5IAiSTjfqR+uCXRBvQBdt6iLp/MGznJaSKUsFMWYSR4WdVs0UKlgd4NKwgtBrMmcTBxWRzEyr9dJreOOYGWS5dkdZWLO3ExWRxqxk6pyS2IXZ1hryLm1S2uxkWnFVlJYp2g7KSgE2h+YHYcY1o1asHCBUc3dXoAvi9mbdPzdLiLefvAsuB/34uH/05ah7OtisYw+9Qq9RiGL0Dp2iT+gCjRD1Ym/s/fCIf+xjn/qstfreJvMS/Ve++geKas6D</latexit>Example with MSE :L(F(xi)) =12\\x00F(xi)\\x00yi\\x002@L@F(xi)=\\x00F(xi)\\x00yi\\x00.@(F(xi)\\x00yi)@F(xi)=F(xi)\\x00yi\\n\\n53Gradient boosting\\nXavier Bresson53\\nOptimization problem :\\nAt last, we need an algorithm that computes f ∈H, H is known as the hypothesis space, i.e. a space of solutions for the task at hand.\\nAs the goal of boosting is to reduce the bias of predictors, the space H is supposed to contain high-bias models s.a. decision trees with limited depth.\\nAnother major advantage of gradient boosting is that solution f does not have to be exact, i.e. any approximation f can used as long as the dot product is negative (see next slides for justification) :\\n<latexit sha1_base64=\"mtJwSQ6AwLTmYB8nAEe8BFp0zqA=\">AAACyXicjVFdaxQxFM2MX3X96KqPvlxcLLtUlplS1BehICwF+1Ch2xY263Anm9kNzWRikim7DvPkP/TNN3+KmemA2op4IeRwzj33JvemWgrrouh7EN66fefuva37vQcPHz3e7j95emqL0jA+ZYUszHmKlkuh+NQJJ/m5NhzzVPKz9OJ9o59dcmNFoU7cRvN5jkslMsHQeSrp/6COr53JKzTLXKg6qTIqFBzWQOFoOElOdilKvULIRrBDUWtTrOHfnhHsQmeiEtVScqAKU4mt+CoDajqW9nbgP2pSW+ZJJd7F9ScFNDPIKqrROIESjupfeDJcJ2JUj7P27vnyPpL+IBpHbcBNEHdgQLo4Tvrf6KJgZc6VYxKtncWRdvOq6cEkr3u0tFwju8Aln3moMOd2XrWbqOGlZxaQFcYf5aBlf3dUmFu7yVOfmaNb2etaQ/5Nm5UuezuvhNKl44pdNcpKCa6AZq2wEIYzJzceIDPCvxXYCv2onF9+M4T4+pdvgtO9cfx6vP9xf3Cw141jizwnL8iQxOQNOSCH5JhMCQsmgQzK4DL8EH4O1+GXq9Qw6DzPyB8Rfv0JBs/aSA==</latexit>argminf2HL(FT+↵f)⇡argminf2HL(F)+↵hrL(F),fi⇡argminf2HnXi=1@L@F(xi).f(xi)\\n<latexit sha1_base64=\"zwx3GGYPYPMDvJxrEepnV3qH33o=\">AAACKXicbVDLSsNAFJ3UV42vqks3g0Wom5KUoi4UCoK4cFHBPqCpYTKdtEMnkzAzEUvI77jxV9woKOrWH3HSFtTWA8MczrmXe+/xIkalsqwPI7ewuLS8kl8119Y3NrcK2ztNGcYCkwYOWSjaHpKEUU4aiipG2pEgKPAYaXnD88xv3REhachv1Cgi3QD1OfUpRkpLbqHmyDhwE3pmp7ccOr5AOHEiJBRFDF6lP/yidO/Sw7Tsj394apmOY2q4haJVtsaA88SekiKYou4WXpxeiOOAcIUZkrJjW5HqJtkYzEhqOrEkEcJD1CcdTTkKiOwm40tTeKCVHvRDoR9XcKz+7khQIOUo8HRlgNRAznqZ+J/XiZV/0k0oj2JFOJ4M8mMGVQiz2GCPCoIVG2mCsKB6V4gHSKeldLhZCPbsyfOkWSnbR+XqdbVYq0zjyIM9sA9KwAbHoAYuQR00AAYP4Am8gjfj0Xg23o3PSWnOmPbsgj8wvr4B+Z+kZw==</latexit>nXi=1@L@F(xi).f(xi)<0\\n\\n\\n54Gradient boosting\\nXavier Bresson54\\nStep-by-step optimization :\\n<latexit sha1_base64=\"dDXEvQPOYWct8HCJSZlNh98Icic=\">AAADCHicjVLLahsxFNVMX6n7iNMuC0XUtDiQGI8JTTeBQMFk4UVK4yRguUYj3xmLaDSDdCeNGWbZTX+lmy5aSrf9hO76N9XY00celF4QHJ17z7nSlcJMSYvd7g/Pv3b9xs1bK7cbd+7eu7/aXHtwaNPcCBiKVKXmOOQWlNQwRIkKjjMDPAkVHIUnL6v80SkYK1N9gPMMxgmPtYyk4OioyZr3+BlDOEOTFK/zLEst0JIO2v31HRYZLoqgLHolC2Xcpv3NOa3Q+pveBv0lojgD7STLapZxg5IrOij/4H6546SMNX53Oqg19ZabOJG6nBQRk5ruuYTiOlbwb9ONiDJT1w0gQiPjGXJj0rfnjPnZFcab/+vcmDRb3U53EfQyCGrQInXsT5rf2TQVeQIaheLWjoJuhuOiMhcKygbLLWRcnPAYRg5qnoAdF4uHLOlTx0xplBq3NNIF+7ei4Im18yR0lQnHmb2Yq8ircqMcoxfjQuosR9Bi2SjKFcWUVr+CTqUBgWruABdGurNSMeNuRuj+TjWE4OKVL4PDXid43tl6tdXa7dXjWCGPyBPSJgHZJrtkj+yTIRHeO++D98n77L/3P/pf/K/LUt+rNQ/JufC//QSCNfdl</latexit>SupposeL(F)=12\\x00F\\x00y\\x002,then@L@F=F\\x00yThen argminf2Hh@L@F,fi,argmaxf2Hh\\x00@L@F,fiFind candidate f that best aligns to –gradient, which always points to the solution y.\\n\\n\\n55Gradient boosting\\nXavier Bresson55\\nStep-by-step optimization :\\n\\n\\n56Gradient boosting (AnyBoost)\\nXavier Bresson56\\nPseudo-code\\n<latexit sha1_base64=\"Mxr8fHj040ae0BH435xmbuOz348=\">AAAD4HicbVNNb9NAEN3YfJTw0RSOXEZEVIlaRXZVAaqIVIQUFamHIjVtpThYm806XnW9NrtraGT5xIUDCHHlZ3Hjj3Bm7Thy2jAHezwz773Z2fEk4Uxpx/nTsOxbt+/c3bjXvP/g4aPN1tbjMxWnktAhiXksLyZYUc4EHWqmOb1IJMXRhNPzyeXbIn/+iUrFYnGq5wkdR3gmWMAI1ibkbzX+bnuaXmkZZe9EkuoDyOF4FzzMkxCbd9a58tnu3GddLzefEdYhwTx7k8MSBh0jOGVEx7Kbe15ze+A7fadwlhWDWBpW3XcOTsvwxxRPYeYz6IMXSEwyL8FSM8zhuDPwdaHYLSXzOjMoo6uqM4mnjArdhbxmDfxM77h5f1mG5SxiIvezwGMCjgwePJVGfsb6bv5BFF30woVgTf2Z6dA0HBT91QfuXJtFt9Zc4lhgQOvsi45KjddOLaJDKlY6rx8AgwXEyJtpwE51GVAxwboy5YrCSmKVbFkjqU6lUSw5m02/1XZ6Tmmw7riV00aVnfit3940JmlkJk44VmrkOokeZ8X1EE7zppcqmmByiWd0ZFyBI6rGWbmgOTw3EXM5Zg+CWGgoo6uIDEdKzaOJqSwGrm7miuD/cqNUB6/GGSs2lwqyEApSDjqGYtthyiQlms+Ng4lkplcgITY7p80/UQzBvXnkdedsr+e+6O2/328f7lXj2EBP0TPUQS56iQ7RETpBQ0SssfXF+mZ9tyf2V/uH/XNRajUqzBN0zexf/wDkzzwD</latexit>Input:L,↵,{(xi,yi)},A(predictor)F0=0Fort=0:Tgi=@L(Ft(xi),yi)@F(xi)(gradient)ft+1= argminf2HnXi=1gi.h(xi),withf=A({(xi,yi)})ifnXi=1gi.ht+1(xi)<0 thenFt+1=Ft+↵ht+1elsereturnFt\\n\\n57Summary\\nXavier Bresson57\\nBoosting is a powerful technique to turn weak learners into a strong learner.\\nClass of boosting algorithms \\nGradient boosting \\nClassification and regression tasks \\nWeak learners are regression trees of depth 4\\nAdaptative boosting (AdaBoost)\\nSpecific case of binary classification (cannot be applied directly to multi-class and regression)\\nSpecific case of exponential loss, i.e. \\nStep size α can be computed optimally (closed-form solution)\\nTraining error decreases exponentially, O(log  n) convergence (can be proved)\\nA hybrid algorithm that combines advantages of bagging and boosting can be designed :\\nStochastic gradient boosting : sub-sample with replacement + low-depth trees\\n<latexit sha1_base64=\"E7zHR0k3ty4XcO6i2+uYwhOu1Sc=\">AAACDHicbVDLSgMxFM34rOOr6tJNsAjtwmGmFHVTKAjFhYsK9gF9DJk004ZmMkOSEcswH+DGX3HjQhG3foA7/8b0sdDWA4HDOedyc48XMSqVbX8bK6tr6xubmS1ze2d3bz97cNiQYSwwqeOQhaLlIUkY5aSuqGKkFQmCAo+Rpje6mvjNeyIkDfmdGkekG6ABpz7FSGnJzeZu8tVCuSPjwE1o2Ul7HJJecjZ2qVXNP7i0kJqmTtmWPQVcJs6c5MAcNTf71emHOA4IV5ghKduOHalugoSimJHU7MSSRAiP0IC0NeUoILKbTI9J4alW+tAPhX5cwan6eyJBgZTjwNPJAKmhXPQm4n9eO1b+ZTehPIoV4Xi2yI8ZVCGcNAP7VBCs2FgThAXVf4V4iATCSvc3KcFZPHmZNIqWc26Vbku5SnFeRwYcgxOQBw64ABVwDWqgDjB4BM/gFbwZT8aL8W58zKIrxnzmCPyB8fkDSNSZLw==</latexit>L(F)=nXi=1e\\x00yi.F(xi)\\n\\n58Outline\\nXavier Bresson58\\nkNN\\nk-d tree\\nDecision tree\\nBagging\\nBoosting\\nConclusion\\n\\n\\n59Conclusion\\nXavier Bresson59\\nkNN is a simple and expressive learning technique but time and memory consuming.  \\nk-d tree speeds up kNN by discarding far away data points.\\nDecision tree improves the memory complexity (no loading of data points required) and speeds up inference with tree structure.\\nBagging is an ensemble method that combines a large number of weak learners with high variance to generate a strong learner with low variance. \\nBoosting is an ensemble method that combines a large number of weak learners with high bias to generate a strong learner with low bias. \\nBagging/boosting are universal, i.e. agnostic of the algorithm used.\\nUse these ensemble techniques to boost your algorithm accuracy by a few percentage,         e.g. to win Kaggle competitions J.\\n\\n\\n60\\nQuestions?\\nXavier Bresson60\\n\\n', metadata={'ipfs_hash': 'QmeYwLHRRkBFCDF6jvRJbvvvSJPqy4zFPM5BE6JYMdjygc', 'file_type': 'application/pdf', 'source': 'lecture3_kNN_techniques.pdf'})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'ipfs_hash': 'QmeYwLHRRkBFCDF6jvRJbvvvSJPqy4zFPM5BE6JYMdjygc',\n",
       " 'file_type': 'application/pdf',\n",
       " 'source': 'lecture3_kNN_techniques.pdf'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(r[2])\n",
    "display(r[2].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='PassengerId: 1\\nSurvived: 0\\nPclass: 3\\nName: Braund, Mr. Owen Harris\\nSex: male\\nAge: 22.0\\nSibSp: 1\\nParch: 0\\nTicket: A/5 21171\\nFare: 7.25\\nCabin: nan\\nEmbarked: S\\n', metadata={'index': 0, 'file_type': 'text/csv', 'ipfs_hash': 'QmQyktKF3TWrAuZAnXWGJfm3b6CdV4oNJfidcV7yrwjsjG', 'source': 'train.csv'}),\n",
       " Document(page_content='PassengerId: 2\\nSurvived: 1\\nPclass: 1\\nName: Cumings, Mrs. John Bradley (Florence Briggs Thayer)\\nSex: female\\nAge: 38.0\\nSibSp: 1\\nParch: 0\\nTicket: PC 17599\\nFare: 71.2833\\nCabin: C85\\nEmbarked: C\\n', metadata={'index': 1, 'file_type': 'text/csv', 'ipfs_hash': 'QmQyktKF3TWrAuZAnXWGJfm3b6CdV4oNJfidcV7yrwjsjG', 'source': 'train.csv'}),\n",
       " Document(page_content='PassengerId: 3\\nSurvived: 1\\nPclass: 3\\nName: Heikkinen, Miss. Laina\\nSex: female\\nAge: 26.0\\nSibSp: 0\\nParch: 0\\nTicket: STON/O2. 3101282\\nFare: 7.925\\nCabin: nan\\nEmbarked: S\\n', metadata={'index': 2, 'file_type': 'text/csv', 'ipfs_hash': 'QmQyktKF3TWrAuZAnXWGJfm3b6CdV4oNJfidcV7yrwjsjG', 'source': 'train.csv'}),\n",
       " Document(page_content='PassengerId: 4\\nSurvived: 1\\nPclass: 1\\nName: Futrelle, Mrs. Jacques Heath (Lily May Peel)\\nSex: female\\nAge: 35.0\\nSibSp: 1\\nParch: 0\\nTicket: 113803\\nFare: 53.1\\nCabin: C123\\nEmbarked: S\\n', metadata={'index': 3, 'file_type': 'text/csv', 'ipfs_hash': 'QmQyktKF3TWrAuZAnXWGJfm3b6CdV4oNJfidcV7yrwjsjG', 'source': 'train.csv'}),\n",
       " Document(page_content='PassengerId: 5\\nSurvived: 0\\nPclass: 3\\nName: Allen, Mr. William Henry\\nSex: male\\nAge: 35.0\\nSibSp: 0\\nParch: 0\\nTicket: 373450\\nFare: 8.05\\nCabin: nan\\nEmbarked: S\\n', metadata={'index': 4, 'file_type': 'text/csv', 'ipfs_hash': 'QmQyktKF3TWrAuZAnXWGJfm3b6CdV4oNJfidcV7yrwjsjG', 'source': 'train.csv'}),\n",
       " Document(page_content='PassengerId: 6\\nSurvived: 0\\nPclass: 3\\nName: Moran, Mr. James\\nSex: male\\nAge: nan\\nSibSp: 0\\nParch: 0\\nTicket: 330877\\nFare: 8.4583\\nCabin: nan\\nEmbarked: Q\\n', metadata={'index': 5, 'file_type': 'text/csv', 'ipfs_hash': 'QmQyktKF3TWrAuZAnXWGJfm3b6CdV4oNJfidcV7yrwjsjG', 'source': 'train.csv'}),\n",
       " Document(page_content='PassengerId: 7\\nSurvived: 0\\nPclass: 1\\nName: McCarthy, Mr. Timothy J\\nSex: male\\nAge: 54.0\\nSibSp: 0\\nParch: 0\\nTicket: 17463\\nFare: 51.8625\\nCabin: E46\\nEmbarked: S\\n', metadata={'index': 6, 'file_type': 'text/csv', 'ipfs_hash': 'QmQyktKF3TWrAuZAnXWGJfm3b6CdV4oNJfidcV7yrwjsjG', 'source': 'train.csv'}),\n",
       " Document(page_content='PassengerId: 8\\nSurvived: 0\\nPclass: 3\\nName: Palsson, Master. Gosta Leonard\\nSex: male\\nAge: 2.0\\nSibSp: 3\\nParch: 1\\nTicket: 349909\\nFare: 21.075\\nCabin: nan\\nEmbarked: S\\n', metadata={'index': 7, 'file_type': 'text/csv', 'ipfs_hash': 'QmQyktKF3TWrAuZAnXWGJfm3b6CdV4oNJfidcV7yrwjsjG', 'source': 'train.csv'}),\n",
       " Document(page_content='PassengerId: 9\\nSurvived: 1\\nPclass: 3\\nName: Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\\nSex: female\\nAge: 27.0\\nSibSp: 0\\nParch: 2\\nTicket: 347742\\nFare: 11.1333\\nCabin: nan\\nEmbarked: S\\n', metadata={'index': 8, 'file_type': 'text/csv', 'ipfs_hash': 'QmQyktKF3TWrAuZAnXWGJfm3b6CdV4oNJfidcV7yrwjsjG', 'source': 'train.csv'}),\n",
       " Document(page_content='PassengerId: 10\\nSurvived: 1\\nPclass: 2\\nName: Nasser, Mrs. Nicholas (Adele Achem)\\nSex: female\\nAge: 14.0\\nSibSp: 1\\nParch: 0\\nTicket: 237736\\nFare: 30.0708\\nCabin: nan\\nEmbarked: C\\n', metadata={'index': 9, 'file_type': 'text/csv', 'ipfs_hash': 'QmQyktKF3TWrAuZAnXWGJfm3b6CdV4oNJfidcV7yrwjsjG', 'source': 'train.csv'})]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'index': 0,\n",
       " 'file_type': 'text/csv',\n",
       " 'ipfs_hash': 'QmQyktKF3TWrAuZAnXWGJfm3b6CdV4oNJfidcV7yrwjsjG',\n",
       " 'source': 'train.csv'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(r[3][:10])\n",
    "display(r[3][0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chainlink-ipfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
